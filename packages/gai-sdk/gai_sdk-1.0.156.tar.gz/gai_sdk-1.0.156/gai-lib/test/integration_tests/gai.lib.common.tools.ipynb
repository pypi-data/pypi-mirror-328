{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Googler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ul style='list-style-type: none; padding: 0;'>\n",
       "    <li style='margin-bottom: 20px;'>\n",
       "        <h3 style='margin: 0; color: blue;'>Time in Singapore - Time and Date</h3>\n",
       "        <a href=\"https://www.timeanddate.com/worldclock/singapore\" style='color: magenta; font-size: 0.9em;'>https://www.timeanddate.com/worldclock/singapore</a>\n",
       "        <p style='margin: 5px 0; color: gray;'>None</p>\n",
       "    </li>\n",
       "    \n",
       "    <li style='margin-bottom: 20px;'>\n",
       "        <h3 style='margin: 0; color: blue;'>Cheap Flights to Singapore from ₹ 4370 - Skyscanner</h3>\n",
       "        <a href=\"https://www.skyscanner.co.in/flights-to/sg/cheap-flights-to-singapore.html#:~:text=How%20long%20is%20the%20flight,5%20hours%20and%2049%20minutes.\" style='color: magenta; font-size: 0.9em;'>https://www.skyscanner.co.in/flights-to/sg/cheap-flights-to-singapore.html#:~:text=How%20long%20is%20the%20flight,5%20hours%20and%2049%20minutes.</a>\n",
       "        <p style='margin: 5px 0; color: gray;'>None</p>\n",
       "    </li>\n",
       "    \n",
       "    <li style='margin-bottom: 20px;'>\n",
       "        <h3 style='margin: 0; color: blue;'>Time Zones in Singapore - Time and Date</h3>\n",
       "        <a href=\"https://www.timeanddate.com/time/zone/singapore\" style='color: magenta; font-size: 0.9em;'>https://www.timeanddate.com/time/zone/singapore</a>\n",
       "        <p style='margin: 5px 0; color: gray;'>None</p>\n",
       "    </li>\n",
       "    \n",
       "    <li style='margin-bottom: 20px;'>\n",
       "        <h3 style='margin: 0; color: blue;'>Philippine Standard Time - Wikipedia</h3>\n",
       "        <a href=\"https://en.wikipedia.org/wiki/Philippine_Standard_Time#:~:text=The%20Philippines%20shares%20the%20same,Indonesia%2C%20and%20most%20of%20Mongolia.\" style='color: magenta; font-size: 0.9em;'>https://en.wikipedia.org/wiki/Philippine_Standard_Time#:~:text=The%20Philippines%20shares%20the%20same,Indonesia%2C%20and%20most%20of%20Mongolia.</a>\n",
       "        <p style='margin: 5px 0; color: gray;'>None</p>\n",
       "    </li>\n",
       "    \n",
       "    <li style='margin-bottom: 20px;'>\n",
       "        <h3 style='margin: 0; color: blue;'>Current Local Time in Singapore, Singapore</h3>\n",
       "        <a href=\"https://www.timeanddate.com/worldclock/singapore/singapore\" style='color: magenta; font-size: 0.9em;'>https://www.timeanddate.com/worldclock/singapore/singapore</a>\n",
       "        <p style='margin: 5px 0; color: gray;'>Current Local Time in Singapore, Singapore· Weather · Time Zone · No DST · Difference · Sunrise · Sunset · Day length · Moon 50.6%. Rise – 12:49 am. Set ...</p>\n",
       "    </li>\n",
       "    \n",
       "    <li style='margin-bottom: 20px;'>\n",
       "        <h3 style='margin: 0; color: blue;'>Current Local Time in Singapore</h3>\n",
       "        <a href=\"https://www.timeanddate.com/worldclock/singapore\" style='color: magenta; font-size: 0.9em;'>https://www.timeanddate.com/worldclock/singapore</a>\n",
       "        <p style='margin: 5px 0; color: gray;'>Current Local Time in Singapore; Singapore, Mon 3:55 pm ...</p>\n",
       "    </li>\n",
       "    \n",
       "    <li style='margin-bottom: 20px;'>\n",
       "        <h3 style='margin: 0; color: blue;'>Time in Singapore now</h3>\n",
       "        <a href=\"https://time.is/Singapore\" style='color: magenta; font-size: 0.9em;'>https://time.is/Singapore</a>\n",
       "        <p style='margin: 5px 0; color: gray;'>Time zone info for Singapore.UTC +8. Singapore Time (SGT) now 13 hours ahead of New York.</p>\n",
       "    </li>\n",
       "    \n",
       "    <li style='margin-bottom: 20px;'>\n",
       "        <h3 style='margin: 0; color: blue;'>Current local time in Singapore</h3>\n",
       "        <a href=\"https://www.worldtimeserver.com/current_time_in_SG.aspx\" style='color: magenta; font-size: 0.9em;'>https://www.worldtimeserver.com/current_time_in_SG.aspx</a>\n",
       "        <p style='margin: 5px 0; color: gray;'>Current local time and date in Singaporefrom a trusted independent resource.</p>\n",
       "    </li>\n",
       "    \n",
       "    <li style='margin-bottom: 20px;'>\n",
       "        <h3 style='margin: 0; color: blue;'>Time in Singapore now</h3>\n",
       "        <a href=\"https://24timezones.com/Singapore/time\" style='color: magenta; font-size: 0.9em;'>https://24timezones.com/Singapore/time</a>\n",
       "        <p style='margin: 5px 0; color: gray;'>Currentlocal time in Singaporeand DST dates in 2025. Local time. 11:58:08 AM ... Correctlocal time in Singapore, Singapore timezone, official time change dates ...</p>\n",
       "    </li>\n",
       "    \n",
       "    <li style='margin-bottom: 20px;'>\n",
       "        <h3 style='margin: 0; color: blue;'>Current Local Time in Singapore, Singapore - Time Difference</h3>\n",
       "        <a href=\"https://www.zeitverschiebung.net/en/city/1880252\" style='color: magenta; font-size: 0.9em;'>https://www.zeitverschiebung.net/en/city/1880252</a>\n",
       "        <p style='margin: 5px 0; color: gray;'>None</p>\n",
       "    </li>\n",
       "    \n",
       "    <li style='margin-bottom: 20px;'>\n",
       "        <h3 style='margin: 0; color: blue;'>Current Time In Singapore</h3>\n",
       "        <a href=\"https://www.tickcounter.com/timezone/singapore\" style='color: magenta; font-size: 0.9em;'>https://www.tickcounter.com/timezone/singapore</a>\n",
       "        <p style='margin: 5px 0; color: gray;'>12 Jan 2025—</p>\n",
       "    </li>\n",
       "    \n",
       "    <li style='margin-bottom: 20px;'>\n",
       "        <h3 style='margin: 0; color: blue;'>Singapore Time zone • Current Time in</h3>\n",
       "        <a href=\"https://www.prokerala.com/travel/timezones/Singapore\" style='color: magenta; font-size: 0.9em;'>https://www.prokerala.com/travel/timezones/Singapore</a>\n",
       "        <p style='margin: 5px 0; color: gray;'>Current time & date in Singapore is:12:24:24 AM on Wednesday, Jan 22, 2025. Singapore Timezone in detail. Current Time and date in Singapore Timezone.</p>\n",
       "    </li>\n",
       "    \n",
       "    <li style='margin-bottom: 20px;'>\n",
       "        <h3 style='margin: 0; color: blue;'>Time.SG : Singapore Local Time, Singapore Current Time ...</h3>\n",
       "        <a href=\"https://time.sg/\" style='color: magenta; font-size: 0.9em;'>https://time.sg/</a>\n",
       "        <p style='margin: 5px 0; color: gray;'>The current local time in Singapore is now 11:44:34 pm. Today's date in Singapore is Tuesday, January 21, 2025. Singapore time is 8 hours ahead of UTC, ...</p>\n",
       "    </li>\n",
       "    \n",
       "    <li style='margin-bottom: 20px;'>\n",
       "        <h3 style='margin: 0; color: blue;'>Singapore, Asia: Current Local Time & Date, Time Zone and ...</h3>\n",
       "        <a href=\"https://www.zeitverschiebung.net/en/country/sg\" style='color: magenta; font-size: 0.9em;'>https://www.zeitverschiebung.net/en/country/sg</a>\n",
       "        <p style='margin: 5px 0; color: gray;'>Time Difference​​ The time in Singapore iscurrently 16 hours ahead of the time in America/Los_Angeles.</p>\n",
       "    </li>\n",
       "    </ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gai.selenium.client import SeleniumClient\n",
    "response = await SeleniumClient().google(\"What is the current time in Singapore?\")\n",
    "results = response.json()\n",
    "\n",
    "html_content = \"<ul style='list-style-type: none; padding: 0;'>\"\n",
    "\n",
    "for result in results:\n",
    "    html_content += f\"\"\"\n",
    "    <li style='margin-bottom: 20px;'>\n",
    "        <h3 style='margin: 0; color: blue;'>{result[\"title\"]}</h3>\n",
    "        <a href=\"{result[\"link\"]}\" style='color: magenta; font-size: 0.9em;'>{result[\"link\"]}</a>\n",
    "        <p style='margin: 5px 0; color: gray;'>{result[\"snippet\"]}</p>\n",
    "    </li>\n",
    "    \"\"\"\n",
    "\n",
    "html_content += \"</ul>\"\n",
    "\n",
    "# Display the formatted HTML\n",
    "display(HTML(html_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('PMO | National Day Rally 2023 Skip to main content Search Home About Us Newsroom Photos Topics National Awards National Awards (COVID-19) Toggle search bar Toggle navigation Search You are here Home Newsroom National Day Rally 2023 National Day Rally 2023 Share Tweet Email Whatsapp Print Text Size SM Lee Hsien Loong |  20 August 2023 PM Lee Hsien Loong delivered his National Day Rally 2023 speech on 20 August 2023 at the Institute of Technical Education (ITE) Headquarters. PM spoke in Malay and Mandarin , followed by English. “A BETTER HOME, A BRIGHTER FUTURE” SECTION 1: INTRODUCTION My fellow Singaporeans, good evening. Return to Normalcy We are all relieved that COVID is behind us. Life as we knew it has resumed. COVID-19 was the most challenging ordeal for our nation since Independence. We can all be proud of how we pulled through together. Unlike many other countries, Singapore has emerged from the pandemic stronger, more resilient, and more united. This is a tribute to the indomitable spirit of our nation. Global situation and economic outlook Having come through COVID, we are once again being tested. The international environment is fraught with geopolitical tension and economic uncertainty. We feel acutely the pressures mounting all around us. The rivalry between the US and China affects every country and region in the world. Their mutual wariness and distrust has deepened. The rest of the world has to tread a careful path, to avoid being caught in the cross-fire. The war in Europe rages on. Russia’s invasion of Ukraine is a continuing human tragedy, and an assault on international norms and values. It is a cautionary warning to the rest of the world never to take peace for granted, and a reminder to us of the vital importance of a strong SAF. Meanwhile, the global economic order is fraying. Globalisation is weakening. Supply chains are splitting up. Countries are layering on multiple protectionist measures. This hurts all countries, but especially small, open trading nations like ourselves. With global warming, the world is also experiencing more extreme weather. From China to Japan, to Europe and the US, no region is spared from floods and droughts, heatwaves and wildfires. This will affect food production and prices worldwide. We have not fully felt it in Singapore yet, but it is coming. Economically, Singapore is keeping up. We expect positive economic growth this year. Hopefully, we will avoid a recession. Inflation is at last coming down, but it will probably stay higher than what we were used to. The cost of living is still on everyone’s minds. In my Chinese speech earlier, I explained how the Government will continue to support you. We will weather this storm together. Forward Singapore Our nation must navigate carefully in this increasingly troubled landscape. There is no ready playbook, there is no model answer. I am glad that DPM Lawrence Wong and the 4G team have taken it upon themselves to chart out where Singapore will go from here. They launched Forward Singapore in June last year, to refresh our social compact. They have held dialogues with thousands of Singaporeans who shared their ideas to tackle various issues. For example: how to equip our people with the skills to succeed; how to improve care for the vulnerable; and how to strengthen our solidarity as one people. The Forward Singapore report will be published later this year. It will be a compass to help Singapore navigate through the stormy seas. Despite the dark clouds, the world still offers many opportunities for those who dare to seize them. In my Malay speech just now, I spoke about promising new sectors such as the digital economy. I also shared that the Government will give more support to workers – to help you adapt and upskill, to stay ahead of the game. Our Forward SG plans include financial support for workers who lose your jobs, while you upgrade your skills. It will be a temporary safety net to help you meet immediate needs, to free you to upskill and train, as you prepare yourself for a good long-term job. So long as you are willing to make the effort, the Government will go the extra mile to help you. SECTION 2: RETIREMENT Financial concerns are top-of-mind not just for younger workers, but also the older ones. These concerns become more urgent as we approach retirement. Especially for those in their 50s and early 60s. Let us call them the “Young Seniors”. \"Young”, because you are younger than the Pioneer Generation and the Merdeka Generation; “Seniors”, because you will soon retire, or maybe you have recently retired. Young Seniors are in a unique position today. Compared to the Pioneer and Merdeka Generations, you have benefited more from Singapore’s growth, and generally done better in life. But compared to workers younger than you, in their 30s and 40s today, you have generally earned less over your lifetimes. You have also had less time to benefit from improvements to the CPF system, and so have built up less retirement savings. Young Seniors are also in a particularly sandwiched phase of your lives. You have to shoulder the responsibility of caring for both the young and old in your families. Your kids may be young adults, but often are not yet fully independent and still live in the same household. Many of you Young Seniors also have elderly parents at home, who may be beset with the infirmities of old age. You have to shuttle them to medical appointments and hospital visits, plus attend to their everyday needs. All this, while watching your own health, because you are not so youthful yourselves. Responsibilities on all these fronts multiply your burdens. So, as Young Seniors, you are understandably anxious about your retirement needs. Beyond the daily cost of living pressures, you know that retirement is creeping up on you. You wonder: Will I have enough to get by? Can I cope? But do not worry – the Government will help you. You will not be left behind. Majulah Package We will introduce a package to help Young Seniors meet your retirement needs – let us name it the Majulah Package. This will be for Singaporeans who are 50 and above this year – born in 1973 or earlier. It will benefit those with lower incomes and less wealth. The support will be tiered, depending on your income and your CPF savings. The Majulah Package will comprise three components. First, an “Earn and Save” Bonus, to help you build up your CPF savings while you work. Most Young Seniors are still working, and have some years to go before retirement. We encourage you to continue working as long as you can. Lower- and middle-income workers will get a CPF bonus of up to $1,000 a year, depending on your income. The Government will credit this into your CPF account, on top of the usual employer and employee contributions. You will receive this Earn and Save bonus yearly, as long as you are working, whether full-time or part-time. Take for example a lower-income 55 year-old who plans to retire at 65. Over 10 years, her Earn and Save bonus adds up to $12,000 in extra CPF savings if you include the CPF interest, which is not bad. Second, a Retirement Savings Bonus (RSB). If your CPF balances have not reached the CPF Basic Retirement Sum, you will receive a one-time CPF bonus of up to $1,500. Those who are not working will get this bonus too. This includes homemakers, who have given up their careers and laboured to raise their families, and thus have very low CPF balances. Third, a MediSave Bonus. Most Young Seniors have enough MediSave balances. Nevertheless, many still worry about healthcare costs, because you will soon be Not-So-Young Seniors. So the Majulah Package will include a modest one-time MediSave Bonus of up to $1,000. It will give you some extra buffer, to help pay your medical expenses and insurance premiums. The Majulah Package will help Young Seniors, in their 50s and early 60s, to meet your retirement needs. But besides Young Seniors, we also want to encourage older seniors to continue working for as long as you can. Hence the Majulah Package also covers the Pioneer and Merdeka Generations. I think quite a few here belong to the Pioneer and Merdeka Generation. Let me say to you, if you are PG and MG: If you are still working and meet the income criteria, you too will receive the Earn and Save Bonus as long as you are working. If you are not working, you can still get something from the one-time Retirement Savings Bonus and MediSave Bonus. This will be in addition to the PG or MG benefits that you are already receiving and will continue to receive. Think of this as a 58th National Day present! The Majulah Package will cost the Government about $7 billion. MOF will create a new Fund to meet the full lifetime costs of the Package, using resources from this term of Government. We will honour this commitment without burdening future generations. The Majulah Package will benefit some 1.4 million older people across several generations. We will also be enhancing existing schemes, like Silver Support, Workfare, and the Matched Retirement Savings Scheme and the details will be announced next year. Taken together, these improvements will help seniors to meet basic retirement needs, especially for lower- and middle-income Singaporeans so that you can have greater peace of mind in your golden years. SECTION 3: AGEING I want to talk about two other important issues tonight – caring for our ageing population, and housing our people. Singapore is one of the fastest-ageing nations in the world. Today, about one in five Singaporeans is a senior, aged 65 and above. By 2030, nearly one in four Singaporeans will be a senior. I first talked about ageing in the National Day Rally back in 2007. At that time, we had 500 centenarians – people aged 100 and older. We thought that was a lot. By 2013, which is 6 years later, this had doubled to around 1,000. And by 2030, we are likely to double again to at least 2,000 centenarians! It shows vividly how our society is getting older and older, faster and faster. Today, if you ask – it is not in the chart – but we have about 1,500 already. In 2030, I will not be making this speech, but whoever is doing so will have to take care of all of us seniors, and some of us very senior. Today, we are an aged society; soon, we will be a ‘super-aged’ society. This has massive social and economic implications. We have much to do to help our seniors age well. Today, I will speak on two aspects of our preparations: active ageing; and making homes and precincts more senior-friendly. Active Ageing First, on active ageing. We have invested significantly in healthcare for seniors. The Government plays a big role in providing care, but seniors must also play our part to look after ourselves. MOH launched Healthier SG last month. Healthier SG aims to get each of us to take more responsibility for our own health, supported by family doctors and community partners. When you enrol, you will get a personalised health plan, including when to go for screenings and vaccinations. Besides medical prescriptions, your doctor will also give you what we call social prescriptions, like advising you to lead a more active lifestyle, to maintain a healthier diet, to lose a little bit of weight, or to quit smoking. But while your GP can suggest or even arrange these for you, only you can decide to follow through and do them. But please do so. Prevention is always better than cure. My wife and I have signed up. I encourage all seniors to sign up for Healthier SG when you get the invitation from MOH. Beyond Healthier SG, how else can we help Singaporeans to age well? It is not just about fending off sickness – it is also about staying well, staying well both physically and mentally. Because mental wellness is just as important as physical wellness. One of the biggest threats to the well-being of seniors is loneliness. Today, many seniors are living alone. They risk becoming socially isolated. Their spouses may have passed on; their children are probably living elsewhere with their own families. So it is important that someone keeps regularly in touch with them. It is also important that they keep active and have a social network for mutual support. During COVID, many old folks were stuck at home, unable to go out for their usual activities. We were very worried that isolated and inactive, they would deteriorate quickly. So we restarted community activities for seniors as soon as we could do so safely – stand further apart, do it outdoors, but gather and do it, and keep that human contact, and that activity. We will do more to help seniors stay engaged and socially active. That is why we have set up Active Ageing Centres (AACs) all over Singapore. I recently visited this Active Ageing Centre, the Bedok Radiance AAC. It is run by Thye Hua Kwan Moral Charities. I was very happy to see a vibrant community of seniors joining the activities and using the services there. Many of them told me that they were living alone, and always looked forward to coming down to the activities. It gave them something to do with their friends, something to look forward to, a sense of purpose. The AAC ran a varied weekly programme: art jamming, karaoke and dance classes. When I was there, a HAPPY exercise class was in full swing – HAPPY means Healthy Ageing Promotion Programmes for You, H-A-P-P-Y. The seniors looked happy, but some of them were not so well. A few were wheelchair-bound, but they still joined in the activities. This cheerful lady told me she hoped to joget again! Why not, even in a wheelchair? Other seniors were using the health services at the AAC. Some were getting their vital signs checked so that doctors could follow up if something was amiss. One was having a teleconsultation – with nurses physically there to help him, and a doctor calling in on Zoom from the polyclinic. It was good that relatives, or neighbours, were making the effort to bring the seniors down to the AAC, for them to socialise and cheer up their lives. What I found most encouraging was that many seniors were not just taking part in activities, but also helping to organise and run them – seniors for seniors. I met Mdm Goh, the one in the pink blouse, Mdm Karen, and Mdm Farida. They were preparing to deliver hot meals to frail seniors living in upstairs in the rental flats who could not come down to the AAC. They do this every day. It is a great way to build neighbourly bonds while staying active. Actually, besides meal deliveries, there are also communal meals downstairs at the AAC. The staff told me that these meals were a big draw and an important activity because they encourage the old folks to come down. When they gathered at the AACs for meals, they would socialise and make friends, and the staff could keep an eye on them, made sure they were alright. I also discovered that these meals are actually cooked by the seniors themselves, at least some of the seniors themselves! These three makciks – Mdm Aminah on the left, Mdm Rosnah, and Mdm Fatimah – they used their SkillsFuture credits to attend a food hygiene and preparation course so that they could cook meals for their fellow seniors. Mdm Fatimah told me they use less salt and sugar in their cooking to make it a Healthier Choice. Then I tasted her chicken curry, and I said, \"You must have used santan – coconut milk.” She said no – she uses normal milk. But it tasted just as good – sedap sekali! I had a second helping. If you are observant, you would have noticed that the volunteers I have mentioned are all ladies. But we are starting to see more male seniors coming down to the AACs. The centres are introducing more activities that interest men, such as jamming. Or even something simple like a café corner, where the uncles can catch up over a coffee. There will be something for everybody at the AACs. I was very heartened by my visit to Bedok Radiance AAC. I believe that the network of AACs across Singapore will be a valuable resource for seniors to stay active and healthy. We will invest to expand the network, and work with community partners to enhance the AACs’ services and reach. If you have elderly family members, especially if they are living on their own, please bring them to join an AAC nearby. And if you are a senior yourself, I strongly encourage you to join a centre near you, because many seniors are having a good time there. Take the first step towards active ageing, so you can live well, and age well. Be like Mdm Wong here. This is Mdm Wong, she is 97 – still going strong, still playing Mahjong! Making HDB Homes and Precincts More Senior-Friendly. Besides encouraging active ageing, we will also make our HDB homes and precincts more senior-friendly. Many seniors have told us that they want to live out their golden years in their own homes and neighbourhoods. It is a familiar environment, a cosy sanctuary, and they have established deep roots and meaningful friendships. This is a very good thing. We will help as many seniors to age in place as we can. We need to make a few moves to realise this. First, we will make your homes more senior-friendly. Today, under the EASE programme (Enhancement for Active Seniors), you  can install fittings like ramps to help you get in and out of your homes. In toilets, you can install grab bars and make the floors slip-resistant for safety. Soon, seniors can choose from an expanded suite of fittings under EASE 2.0. For example, you can have foldable shower seats, you can have your toilet entrances widened, so that if you or your spouse needs a wheelchair, your wheelchair can be brought into the toilet, and life can be safer and much more convenient. Secondly, we will make it safer and more comfortable for seniors to move about their neighbourhoods. For example: we will revamp streets and linkways frequented by seniors; we will build more shelters and rest points; also more therapeutic gardens, fitness trails, and exercise machines, to encourage seniors to stay active. The roads will be made more pedestrian-friendly. For example, longer green man signals to give seniors more time to cross the road. You see down here, 36 seconds – the motorcycles and cars have to wait, does not matter, old folks’ safety is important. This is their home, their area, we put them first. Barrier-free ramps and raised zebra crossings so that wheelchair users can cross over without having a step to go down and a step to come back up again at the curb; 3D road markings and narrowed roads to slow the cars down. We will also install larger and more colourful block signs, with familiar symbols to help seniors remember their own block and find their way home. This is not where the satay stall is – it is the Satay Block, Block 113 in Kebun Bahru. Third, for those who need a little more help, we will build more assisted living facilities. Take for example IDeAL – the Integrated Dementia Assisted Living project at Block 115. At the Kueh Lapis Block, Block 115 in Kebun Baru. I went to open it last year. It is a ground-up project by Dementia Singapore, with some Government support, to help seniors with mild dementia. Many of the seniors live in their own flats in this block. The void deck has become their community living room, where seniors have fun and games, exercise, get their haircuts, and eat together. Whenever a senior needs anything, he or she will go to the second floor. The residents affectionately call this “Ji Lao” – the Second Floor. There are shared facilities there like a community kitchen, where they cook and hang out, and a telehealth room, where staff can monitor their vital signs and arrange for telemedicine checkups. All at Ji Lao! At IDeAL, seniors with dementia can live happily on their own upstairs, and enjoy the warmth and care of friends and community downstairs. HDB is building similar assisted living projects called Community Care Apartments. They will serve not just seniors suffering from dementia, but also those who are well. We will need a whole range of these facilities – whether for assisted living, or the AACs – to serve old folks with different needs. Some of these already exist today. But we need to improve them, scale them up, and get ready for the large numbers down the road. We will keep making Singapore a more friendly home for current and future seniors: investing resources to senior-proof your homes; making precincts safer and easier to navigate; and constructing more assisted living residences, with integrated accommodation, care, and community facilities. We will do this across Singapore, starting with the towns with the most seniors like Ang Mo Kio and Bukit Merah. So we are taking major steps to prepare for a super-aged society. It is part of a new national programme called “Age Well SG”, which will complement Healthier SG in improving the health of our seniors. The Ministries will share more about this in the coming months. Together, we will make Singapore an endearing home for all ages, where seniors can age with dignity and grace, connected with friends and family. SECTION 4: HOUSING Now I want to speak about public housing. HDB is a vital part of the Singapore story. Right from the very start, the PAP Government put heavy emphasis on public housing. Here is Mr Lee Kuan Yew laying the foundation stone at the Cantonment Road Housing Estate in March 1963. This was HDB’s earliest projects, one of them. Mr Lee was convinced that housing would give Singaporeans a stake in our nation’s future. Because, he explained, housing “gives more than a sensation of permanence; it is permanent…then we dig our toes in and we fight!” So, housing has always meant much more to us than a roof over our heads. It also gives every Singaporean a valuable asset and a powerful reason to fight for our country and our future. Fast forward 60 years, today we have one of the highest home ownership rates in the world. Nine in ten Singaporeans own our homes. Eight in ten live in HDB flats. Our Singaporean identity is deeply intertwined with our HDB flats and towns. These are homes we are proud to own, neighbourhoods we raise our families in, and communities we build together. Over the decades, the Government has kept housing affordable and accessible for Singaporeans. We price HDB flats at a substantial discount below their market value. We also provide a wide range of generous grants to first-timers, families, and the lower-income. COVID disrupted our public housing programme. BTO projects were delayed. The shortfall of new flats pushed up prices of resale flats. MND and HDB have been working very hard to catch up. They are making good progress. So far, HDB has completed and delivered more than 70% of the flats that had been delayed by the pandemic. We are also ramping up supply. We have launched 50,000 flats since 2021. By 2025, in two years’ time, we will have launched another 50,000 flats, making a total of 100,000 flats. With supply catching up, the market is calming down. First-timer BTO application rates are lower. Waiting times are shorter. Resale prices are gradually stabilising. Recently, MND announced changes to help Singaporeans with urgent housing needs get their flats earlier. Families with young children and young married couples buying their first home will receive more ballot chances, and higher priority in flat allocation. HDB will also launch more BTO flats with shorter waiting times of two to three years. We are working extra hard, and I am confident that we will get over this hump soon. Changing housing landscape Tonight, I want to look beyond this hump, to the longer-term public housing landscape. Since the early 90s, HDB has used a simple framework to guide the public in buying flats. We distinguish between Mature Estates and Non-Mature Estates. Mature Estates are places like Ang Mo Kio, Toa Payoh, or Queenstown. This is a picture of Dawson, in Queenstown, where I recorded my National Day Message this year. Mature Estates are usually more centrally located, better connected, and with more amenities. Naturally these places are more popular, and their prices reflect the higher demand. Then there are the Non-Mature Estates: These are less central and further out, like Jurong East, Woodlands, and Punggol. What Non-Mature Estates lose in terms of location and convenience initially, they make up for in lower BTO prices. Between Mature and Non-Mature Estates, HDB has been able to offer a flat for every household’s budget. This framework is easy to understand, and has worked well for many years. But our housing landscape is evolving. Firstly, we have fewer and fewer large tracts of undeveloped land left to build new towns and estates. Tengah will be the last new town for quite some time, at least until Paya Lebar Air Base moves out to Changi and the site is cleared and redeveloped. Increasingly, we will have to build new HDB flats within or near to existing estates. These will often be more centrally located. For example, at Mount Pleasant, where the Police Academy used to be, we are planning to build 5,000 new homes there. New projects like these – nestled in older, more developed areas with a lot more amenities – will be more popular, and will naturally cost more. Secondly, even what we call “Non-Mature Estates” today have become much more developed. We imagine non-mature estates as being very barebones. Blocks of flats surrounded by empty barren ground. Our mental image is this photo, and once upon a time it looked like this – this was Toa Payoh in the 1960s. Toa Payoh does not look like this today. But look around you now. Towns like Jurong East, Woodlands, or Punggol, that is not barebones. These towns have matured, and now have excellent connectivity and a full suite of amenities, so the distinction between Mature and Non-Mature Estates is blurring. This is reflected in BTO applications. Some choicer projects in Non-Mature Estates are even more popular than projects in Mature Estates. It shows that buyers are discerning – you know when you see a good deal, and what matters to you are the specific attributes of the project, rather than whether we call it Mature or Non-Mature. In future, many more BTO developments will be in estates or locations that are effectively “mature”. That means that the framework of Mature and Non-Mature Estates will no longer work and we need a new framework. This framework has to achieve three important objectives: One, it has to keep home ownership affordable to all income groups. Two, it has to maintain a good social mix in every town and every region. And three, it has to keep the system fair for everyone. And here is how we will do it. For a start, we will keep HDB flat prices affordable. We will gradually provide more housing grants, especially grants that are means-tested, like the Enhanced Housing Grant. This way, lower- and middle-income households will get the most support to own their homes. There will always be an HDB flat to meet every budget. And we can maintain a good social mix in every town and region. This will work for most HDB projects, but it will still leave one particular problem and that is, with projects in choicer locations within a region. For example, projects near an MRT station or near the town centre. Such flats see the highest demand during BTO exercises. People know that HDB is offering a good deal, because these flats will fetch much higher resale prices afterwards. This turns the BTO exercise into a lottery. Those who are lucky enough to ballot such a flat stand to reap a windfall upon resale. This will not be fair to the many more who miss out. Take for example one recent BTO project: Central Weave at Ang Mo Kio. This is Ang Mo Kio; let me zoom in, Central Weave is right at the town centre. Let me zoom in a little bit more. It is next to the MRT and bus interchange. The market is there, the hawker centre is there, Ang Mo Kio Hub is nearby. So location, location, location. This is an excellent location; this is a highly desirable project. HDB’s selling price had to reflect these attributes. So, the prices for the biggest flats at Central Weave – the 5-Room and 3-Gen flats – ranged from $713,000 to $877,000, before grants. Even then, these flats were heavily discounted off their true market value. Some people complained that these prices were exorbitant and unaffordable. Yet these units were heavily oversubscribed. More than 6,500 households applied for just 372 such flats – 17 applicants for every flat! Clearly, these applicants must have found the Central Weave project affordable, and thought that the prices offered them good value. And no doubt many hoped their flats would fetch strong resale prices later on. So think about it. If you were HDB CEO sitting down, confronted with this problem, what will you do? You have a dilemma, a dilemma with projects at choicer locations like Central Weave: should we price them higher, or lower? If we price them higher, we will shrink the windfall gains, reduce the lottery effect. This will moderate demand and that is good. But there will be sticker shock – these flats will become expensive and unaffordable to most families. And we will not get a good social mix. Some people will still afford them, but mainly those near the income ceiling, or those whose parents can help them pay for the flat. The result: the precinct will become a higher-income enclave, and that is not what we want. But if HDB prices such prices lower, more households can afford them; we achieve a better social mix, that is good, but we exacerbate the lottery effect, because the potential windfall will be even richer. Even more families will try for these flats. For every one happy successful buyer, there will be 20 or more unsuccessful buyers, and they will understandably and justifiably frustrated and very unhappy. And this is not fair. So under the present framework, whether we price such flats higher or lower, we cannot fully achieve all three objectives: affordability, a good social mix, and a fair outcome. So what do we do? Central Weave is already sold, but we can expect more such situations as we build more flats in existing towns. How do we solve this problem? New “Plus” model Our solution is to introduce a new “Plus” model for selling HDB flats at choicer locations, with stricter sale conditions so that we can moderate the prices. Let me explain carefully what I mean. Today, HDB launches BTO projects all over Singapore. We are all familiar with them. Everybody knows the BTO rules. For example, a 5-year Minimum Occupation Period, or MOP, after which the owner can resell the flat. There is no income ceiling for resale buyers. These are the standard rules and apply to what I will call “Standard” projects. In future, most HDB projects will still be Standard projects. But within each region, some HDB projects will be in choicer locations. Take for example, Bayshore, in Bedok. This is Bedok town. Bayshore is there, let me zoom in. And you can see, Bayshore is a very good location for homes, because you have got two MRT stations, Bedok South and Bayshore MRT stations. Shopping malls will be built in this corner, Siglap CC is there. East Coast Park is across the road, and then you have the waterfront, 有山有水. We will have both private and public housing in Bayshore. The HDB projects will likely be Plus projects and will be sold under different rules. HDB will give more subsidies for these Plus flats, over and above the subsidies for Standard BTO flats. This will moderate the prices of Plus flats and put them within reach of more households. But to make the scheme fair, HDB will also impose more restrictive sale conditions. For example: A longer MOP of 10 years, to favour buyers who are planning to stay there for the longer term, and discourage those who may be thinking of flipping the property and moving out as soon as they can. Tighter restrictions when the home-owner resells the Plus flat later on, such as a subsidy recovery applied on the resale price, certain percentage when you sell the flat, you pay back a certain percentage back to HDB to take back the extra discounts you enjoyed upfront since you are moving out. This is to be fair to other buyers who did not get these Plus flats. Also there will be an income ceiling on resale buyers, just like how we have an income ceiling on first-time buyers. This will moderate resale prices and help to maintain a better social mix, even in the resale market in the longer term. As we build more projects in mature areas, this Plus model will help us to meet our three objectives: affordability, good social mix, and fairness. I keep on showing you this slide because this is crucial. These are the three fundamental objectives and we must bear that in mind, when we think about what is HDB doing, why is HDB doing this, how does doing this help us to achieve these objectives? It will help Singaporeans find a house that suits your needs, even in good locations. Not just for you, but for your children too. Standard, Plus and Prime Actually, the Plus model is not entirely new; we already have something similar, called the Prime Location Public Housing Model. Let us call them Prime projects for short. One example is Bukit Merah Ridge. Like other Prime projects, it is very close to the city centre. Naturally, these flats are very desirable, and will be more expensive. But we have been able to keep their selling prices reasonable by imposing tight restrictions and a subsidy recovery to moderate the windfall gains. The Prime model has shown good outcomes so far. In Bukit Merah Ridge. The selling price for a four-room flat ranged from $540,000 to $737,000, before grants. Each flat attracted 5.4 applicants – far fewer than Central Weave, even if you compare to the four-room flats in Central Weave. And we hope that Plus projects will achieve similarly good outcomes. So, let me go back to Central Weave because you are probably thinking and asking: had we sold Central Weave as a Plus project, with the tighter restrictions and additional subsidies, would HDB have priced it lower than it actually did? The answer is yes, it would, because that is the whole point of Plus projects: to enable HDB to moderate the prices of flats in choicer locations, and still be fair to all flat buyers. So think of it like this. Standard flats are good flats built all over Singapore, and will have HDB’s standard subsidies and standard restrictions. Plus flats are in the choicer locations within a region, and will have more subsidies and tighter restrictions than Standard flats. And Prime flats are in the choicest and most central locations in the whole of Singapore, and they will have the most subsidies and tightest restrictions. Let me show you the whole picture in one slide. The new framework of Standard, Plus, and Prime. Standard islandwide, subsidies are standard and restrictions are standard – that is why we called it standard. Plus flats have choicer region, more subsidies and tighter restrictions. Prime flats, have the choicest locations, most subsidies and tightest restrictions. Actually the quality of the flats, they are all good flats, good flats, good flats. But the locations are different, the subsidies are different and the restrictions are different. And this new framework – Standard, Plus and Prime will be a major change to the way HDB sells flats. HDB will roll out this framework for all new projects from the second half of next year. It will not affect existing projects. Your current homes, or the homes you have already booked, will not be reclassified. In time to come, we will no longer refer to new projects as Mature and Non-Mature. Instead, we will build a good mix of projects within and across regions, to cater to different needs and budgets. And that is how we can fulfil our commitment to keep high-quality HDB flats accessible and affordable to you and your children for a very long time to come. Enhancing Access for Singles The new framework – Standard, Plus, and Prime – will affect everyone buying a new HDB flat, but there is one special group I want to address: the singles. More and more Singaporeans are choosing to be single. Singles too hope to own their homes and have more housing choices. We hear your concerns. Today, first-timer singles can apply for new flats, but only 2-room Flexi flats and only in Non-Mature Estates. They cannot buy new Flexi flats in Mature Estates. Singles are also not allowed to buy Prime flats. These rules are to prioritise our limited supply of flats, but unfortunately, they have restricted singles’ choices. We will do something about this. When we roll out the new framework, singles will be allowed to buy 2-room Flexi flats across all types of BTO projects – Standard, Plus, and Prime. HDB will tell you the details soon. But I am sure singles will welcome this move to have more choices to find your own home and to write your own part in Singapore’s housing story. Our Housing Foundations I spoke earlier about the foundation stone that Mr Lee laid back in 1963. Today, Cantonment Road Housing Estate has been redeveloped into our most iconic public housing project – The Pinnacle @ Duxton. But 60 years on, that original foundation stone is still there, at the Heritage Garden at the Pinnacle. It occupies pride of place, to remind each new generation that our housing programme is a vital part of the Singapore story; that the Singapore we live in today, is what we have inherited from those who came before us; and that we owe an immense debt to the hard work and good governance of the founding fathers and the pioneer generation. SECTION 5: CONCLUSION Our Founding Values In a few weeks’ time, we will be marking the 100th birth anniversary of Mr Lee Kuan Yew. It is timely to reflect upon the values and ideals championed by Mr Lee and the founding fathers., and for us to renew our commitment to these enduring values and ideals. I spoke in my Malay speech earlier about an exhibition now on at the National Museum: Semangat yang Baru: Forging a New Singapore Spirit. The exhibition tells the story of the birth of our nation. It transports us back to our early years of nation-building. It honours the indomitable spirit of the Pioneer Generation. They dared; they fought ferociously for Singapore; they never gave up. And thus they turned mudflats into a metropolis, and took us from Third World to First. But our forefathers built more than a nation of bricks and mortar, skyscrapers and a thriving economy. They created a nation founded on ideals: justice and equality; religious freedom and racial harmony; a commitment to excellence; a fair system of meritocracy; and an uncompromising insistence on honest, clean government. Integrity and incorruptibility are fundamental to Singapore. They are the foundation on which we run a clean and effective Government, and deliver results for Singaporeans. Mr Lee Kuan Yew considered these the most crucial ideals of all. On his 90th birthday, he came to Parliament to attend the sitting. We celebrated his birthday in the Members’ Room. We were all there – PAP MPs and Ministers, Opposition MPs led by Mr Low Thia Khiang, as well as NMPs. Frail and ill, Mr Lee said just a few words. He spoke only about one thing – he reminded us Singapore must always remain clean and incorruptible, and Ministers and MPs have to set the example. Otherwise, he said, we are finished. I will never forget those brief words. For the sake of the country, I will do all I can to keep faith with Mr Lee’s hope. No matter the price; no matter the embarrassment or political cost – I will do my utmost to keep the system clean. Every generation of PAP leaders must also stand by this – this is what Singapore depends upon. The ideals I spoke about are not just abstract aspirations. They are our compass; they guide every decision we make. They give purpose and meaning to our nation building. They make Singapore stand tall amongst the nations of the world. By upholding these ideals, we have over decades built a high trust society. One where people trust one another – “regardless of race, language or religion”. Where the people and the Government trust each other too, in big ways and small. This is what saw us through the ups and downs over the past 58 years, and this is how we weathered COVID-19. COVID now seems like a bad dream. Confused, receding, getting forgotten. But the pandemic could well have been a real nightmare. Things could easily have gone horribly wrong, as they did in many other countries. Thankfully, we avoided disastrous outcomes, and averted the loss of many, many lives. Our people came through safe and sound; our economy revived in good shape; and our society has grown more united, not less. We owe this to our collective efforts, and our strong mutual trust, which we must strengthen and deepen to secure a bright future for our people. Succession As we prepare for the future, one crucial task is leadership renewal. As you all know, my original plan was to hand over and step down as Prime Minister by 2022, before my 70th birthday. But the pandemic disrupted this plan. I promised Singaporeans that I would see the nation through the crisis, together with both the current and the 4G leadership. Now COVID is behind us, and my succession plans are back on track. Recently, several controversial issues have drawn Singaporeans’ attention. I have spoken about them in Parliament, and in my National Day Message. We dealt with each of them thoroughly and transparently. Let me assure you: these incidents will not delay my timetable for renewal. We are on track. More and more, my task is to support the 4G team and their agenda. I want to get them off to the best start possible. They are increasingly setting the pace. My themes in this Rally are all Forward SG themes. The 4G will soon wrap up the Forward SG exercise, but their journey is just beginning. Our nation’s future depends on them, working as one with you to take Singapore forward. I have every confidence in Lawrence Wong and his team. We share the same core convictions – that we are stewards of Singapore, entrusted with the immense responsibility to lead and care for our nation; that our time as stewards is transient, but we are building a Singapore for the ages; and that our best service to this nation is to hand over a better, stronger Singapore to those who follow us. My team and I are deeply grateful to you for standing with us through thick and thin. Please give Lawrence and his team your fullest support too, now and after they take over. Closing The Singapore story has been an unlikely one from the start. Our nation is still young, and we will always be tiny. We will forever be an unlikely nation, created out of the sheer collective will of our people. Nobody expected us to survive, much less to flourish. But each time, we beat the odds; each time, we showed the world what Singapore can be. It has been an exceptional story; but I believe the best parts are still to be written. Let us hold fast to: our values, which anchor us and guide our way; our mutual trust, which is the bedrock of our success; and our dreams, which drive us ever forward. Let us come together with a renewed spirit and vigour, a semangat yang baru, to build a better, brighter Singapore for generations to come. Majulah Singapura! Topics: Economy , Families and communities , Finance , Foreign affairs , Founding Fathers , Governance , Healthcare , Housing , Jobs and productivity , National Day Rally , Population , Social safety nets Videos and transcripts National Day Rally 2023 (Malay) National Day Rally 2023 (Mandarin) Watch more National Day Rally 2022 National Day Rally 2021 National Day Rally 2019 Prime Minister\\'s Office Singapore Contact FAQs Reach.Gov.Sg Report Vulnerability Privacy Statement Terms of Use Sitemap © 2024 Government of Singapore. Website Last Updated on 9 August 2024 TOP', {'Skip to main content': 'https://www.pmo.gov.sg#main-content', \"Prime Minister's Office Singapore\": 'https://www.pmo.gov.sg/', 'Home': 'https://www.pmo.gov.sg/', 'About Us': 'https://www.pmo.gov.sg/About-Us', 'Newsroom': 'https://www.pmo.gov.sg/Newsroom', 'Photos': 'https://www.pmo.gov.sg/Photos', 'Topics': 'https://www.pmo.gov.sg/Topics', 'National Awards': 'https://www.pmo.gov.sg/National-Day-Awards', 'National Awards (COVID-19)': 'https://www.pmo.gov.sg/National-Awards', 'National Day Rally 2023': 'https://www.pmo.gov.sg/Newsroom/National-Day-Rally-2023', 'Play Icon': 'https://www.pmo.gov.sg', 'Facebook Icon': 'https://www.facebook.com/LawrenceWongST', 'Twitter Icon': 'https://twitter.com/LawrenceWongST', 'Email Icon': 'https://www.pmo.gov.sg', 'Whatsapp Icon': 'https://www.pmo.gov.sg', 'Print': 'https://www.pmo.gov.sg', 'Text Size Decrease': 'https://www.pmo.gov.sg', 'Text Size Increase': 'https://www.pmo.gov.sg', 'SM Lee Hsien Loong': 'https://www.pmo.gov.sg/cabinet/Mr-LEE-Hsien-Loong', 'Malay': 'https://www.pmo.gov.sg/Newsroom/National-Day-Rally-2023-Malay', 'Mandarin': 'https://www.pmo.gov.sg/Newsroom/National-Day-Rally-2023-Chinese', 'Economy': 'https://www.pmo.gov.sg/Topics/Economy', 'Families and communities': 'https://www.pmo.gov.sg/Topics/Families-and-communities', 'Finance': 'https://www.pmo.gov.sg/Topics/Finance', 'Foreign affairs': 'https://www.pmo.gov.sg/Topics/Foreign-affairs', 'Founding Fathers': 'https://www.pmo.gov.sg/Topics/Founding-Fathers', 'Governance': 'https://www.pmo.gov.sg/Topics/Governance', 'Healthcare': 'https://www.pmo.gov.sg/Topics/Healthcare', 'Housing': 'https://www.pmo.gov.sg/Topics/Housing', 'Jobs and productivity': 'https://www.pmo.gov.sg/Topics/Jobs-and-productivity', 'National Day Rally': 'https://www.pmo.gov.sg/Topics/National-Day-Rally', 'Population': 'https://www.pmo.gov.sg/Topics/Population', 'Social safety nets': 'https://www.pmo.gov.sg/Topics/Social-safety-nets', 'National Day Rally 2023 (Malay)': 'https://www.pmo.gov.sg/Newsroom/National-Day-Rally-2023-Malay', 'National Day Rally 2023 (Mandarin)': 'https://www.pmo.gov.sg/Newsroom/National-Day-Rally-2023-Chinese', 'National Day Rally 2022': 'https://www.pmo.gov.sg/Newsroom/National-Day-Rally-2022-English', 'National Day Rally 2021': 'https://www.pmo.gov.sg/Newsroom/National-Day-Rally-2021-English', 'National Day Rally 2019': 'https://www.pmo.gov.sg/Newsroom/National-Day-Rally-2019', 'Contact': 'https://www.pmo.gov.sg/ContactInfo', 'FAQs': 'https://www.pmo.gov.sg/frequently-asked-questions', 'Reach.Gov.Sg': 'https://www.reach.gov.sg/', 'Instagram Icon': 'https://www.instagram.com/lawrencewongst/', 'Telegram Icon': 'https://t.me/pm_lawrencewong', 'YouTube Icon': 'https://www.youtube.com/pmosingapore', 'Report Vulnerability': 'https://tech.gov.sg/report_vulnerability', 'Privacy Statement': 'https://www.pmo.gov.sg/Privacy-Statement', 'Terms of Use': 'https://www.pmo.gov.sg/Terms-of-Use', 'Sitemap': 'https://www.pmo.gov.sg/Sitemap', 'TOP': 'https://www.pmo.gov.sg'})\n"
     ]
    }
   ],
   "source": [
    "from gai.lib.tools.scraper import Scraper\n",
    "\n",
    "scraper = Scraper()\n",
    "text = scraper.scrape(\"https://www.pmo.gov.sg/Newsroom/National-Day-Rally-2023\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Chunker' object has no attribute 'chunk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chunker\n\u001b[1;32m      3\u001b[0m chunker \u001b[38;5;241m=\u001b[39m Chunker()\n\u001b[0;32m----> 4\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[43mchunker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk\u001b[49m(text)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(chunks)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Chunker' object has no attribute 'chunk'"
     ]
    }
   ],
   "source": [
    "from gai.lib.tools.chunker import Chunker\n",
    "\n",
    "chunker = Chunker()\n",
    "chunks = chunker.chunk(text)\n",
    "print(chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roylai/miniconda/envs/gai-sdk/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2 0 2 g u A 2 ] L C . s c [ 7 v 2 6 7 3 0 . 6 0 7 1 : v i X r a Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need Ashish Vaswani∗ Google Brain avaswani@google.com Noam Shazeer∗ Google Brain noam@google.com Niki Parmar∗ Google Research nikip@google.com Jakob Uszkoreit∗ Google Research usz@google.com Llion Jones∗ Google Research llion@google.com Aidan N. Gomez∗ † University of Toronto aidan@cs.toronto.edu Łukasz Kaiser∗ Google Brain lukaszkaiser@google.com Illia Polosukhin∗ ‡ illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. ∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. †Work performed while at Google Brain. ‡Work performed while at Google Research. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA. 1 Introduction Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]. Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34]. To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9]. 3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next. 2 Figure 1: The Transformer - model architecture. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. 3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512. Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3 Scaled Dot-Product Attention Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel. of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. 3.2.1 Scaled Dot-Product Attention We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the dk, and apply a softmax function to obtain the weights on the query with all keys, divide each by values. √ In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as: Attention(Q, K, V ) = softmax( QK T √ dk )V The two most commonly used attention functions are additive attention [2], and dot-product (multi- plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor . Additive attention computes the compatibility function using a feed-forward network with of a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code. 1√ dk While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4. To counteract this effect, we scale the dot products by 1√ dk . 3.2.2 Multi-Head Attention Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional 4To illustrate why the dot products get large, assume that the components of q and k are independent random i=1 qiki, has mean 0 and variance dk. variables with mean 0 and variance 1. Then their dot product, q · k = dk 4 (1) output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this. MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O where headi = Attention(QW Q i , KW K i , V W V i ) Where the projections are parameter matrices W Q and W O ∈ Rhdv×dmodel. i ∈ Rdmodel×dk , W K i ∈ Rdmodel×dk , W V i ∈ Rdmodel×dv In this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality. 3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]. The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2. 3.3 Position-wise Feed-Forward Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between. FFN(x) = max(0, xW1 + b1)W2 + b2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality df f = 2048. 3.4 Embeddings and Softmax Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor- mation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax dmodel. linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √ 5 Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention. Layer Type Self-Attention Recurrent Convolutional Self-Attention (restricted) Complexity per Layer O(n2 · d) O(n · d2) O(k · n · d2) O(r · n · d) Sequential Maximum Path Length Operations O(1) O(n) O(1) O(1) O(1) O(n) O(logk(n)) O(n/r) 3.5 Positional Encoding Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9]. In this work, we use sine and cosine functions of different frequencies: P E(pos,2i) = sin(pos/100002i/dmodel) P E(pos,2i+1) = cos(pos/100002i/dmodel) where pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of P Epos. We also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training. 4 Why Self-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types. As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence 6 length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work. A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences. 5 Training This section describes the training regime for our models. 5.1 Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source- target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens. 5.2 Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days). 5.3 Optimizer We used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning rate over the course of training, according to the formula: lrate = d−0.5 model · min(step_num−0.5, step_num · warmup_steps−1.5) This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000. 5.4 Regularization We employ three types of regularization during training: 7 (3) Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost. Model ByteNet [18] Deep-Att + PosUnk [39] GNMT + RL [38] ConvS2S [9] MoE [32] Deep-Att + PosUnk Ensemble [39] GNMT + RL Ensemble [38] ConvS2S Ensemble [9] Transformer (base model) Transformer (big) BLEU EN-DE EN-FR 23.75 24.6 25.16 26.03 26.30 26.36 27.3 28.4 39.2 39.92 40.46 40.56 40.4 41.16 41.29 38.1 41.8 Training Cost (FLOPs) EN-DE EN-FR 2.3 · 1019 9.6 · 1018 2.0 · 1019 1.8 · 1020 7.7 · 1019 1.0 · 1020 1.4 · 1020 1.5 · 1020 1.2 · 1020 8.0 · 1020 1.1 · 1021 1.2 · 1021 3.3 · 1018 2.3 · 1019 Residual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1. Label Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. 6 Results 6.1 Machine Translation On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3. For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU 5. 6.2 Model Variations To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the 5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively. 8 Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities. base (A) (B) (C) (D) N dmodel 6 512 2 4 8 256 1024 dff 2048 1024 4096 h 8 1 4 16 32 dk 64 512 128 32 16 16 32 32 128 dv 64 512 128 32 16 32 128 Pdrop 0.1 0.0 0.2 ϵls 0.1 0.0 0.2 PPL train steps (dev) 100K 4.92 5.29 5.00 4.91 5.01 5.16 5.01 6.11 5.19 4.88 5.75 4.66 5.12 4.75 5.77 4.95 4.67 5.47 4.92 300K 4.33 BLEU params ×106 (dev) 25.8 65 24.9 25.5 25.8 25.4 25.1 25.4 23.7 25.3 25.5 24.5 26.0 25.4 26.2 24.6 25.5 25.3 25.7 25.7 26.4 58 60 36 50 80 28 168 53 90 (E) big 6 positional embedding instead of sinusoids 1024 4096 16 0.3 213 development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3. In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model. 6.3 English Constituency Parsing To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]. We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting. We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we 9 Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ) Parser Training Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative semi-supervised semi-supervised semi-supervised semi-supervised semi-supervised multi-task generative Petrov et al. (2006) [29] Zhu et al. (2013) [40] Dyer et al. (2016) [8] Transformer (4 layers) Zhu et al. (2013) [40] Huang & Harper (2009) [14] McClosky et al. (2006) [26] Vinyals & Kaiser el al. (2014) [37] Transformer (4 layers) Luong et al. (2015) [23] Dyer et al. (2016) [8] WSJ 23 F1 88.3 90.4 90.4 91.7 91.3 91.3 91.3 92.1 92.1 92.7 93.0 93.3 increased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3 for both WSJ only and the semi-supervised setting. Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur- prisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]. In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley- Parser [29] even when training only on the WSJ training set of 40K sentences. 7 Conclusion In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours. The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor. Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration. References [1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014. [3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. CoRR, abs/1703.03906, 2017. [4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733, 2016. 10 [5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014. [6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357, 2016. [7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014. [8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In Proc. of NAACL, 2016. [9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu- tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017. [10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013. [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016. [12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001. [13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997. [14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 832–841. ACL, August 2009. [15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016. [16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS), 2016. [17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016. [18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017. [19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations, 2017. [20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. [21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722, 2017. [22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017. [23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015. [24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention- based neural machine translation. arXiv preprint arXiv:1508.04025, 2015. 11 [25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993. [26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152–159. ACL, June 2006. [27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016. [28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017. [29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July 2006. [30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859, 2016. [31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015. [32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. [33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929–1958, 2014. [34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015. [35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014. [36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015. [37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural Information Processing Systems, 2015. [38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016. [39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016. [40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers), pages 434–443. ACL, August 2013. 12 Attention Visualizations this this difficult difficult process process majority have more more new new American American have of It passed passed registration registration governments governments voting It in in since since or or majority of voting laws <pad> <EOS> <EOS> Input-Input Layer5 . . <pad> a the the spirit spirit that that making <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> 2009 2009 is is making a laws Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color. 13 application be be never just The Law will but Input-Input Layer5 , just just its should perfect just <pad> Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5 and 6. Note that the attentions are very sharp for this word. 14 application be be never just The Law will but Input-Input Layer5 , just just its should perfect just <pad> Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks. 15\n"
     ]
    }
   ],
   "source": [
    "from gai.lib.tools.pdf_convert import PDFConvert\n",
    "file_path=\"attention-is-all-you-need.pdf\"\n",
    "text = PDFConvert.pdf_to_text(file_path)\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
