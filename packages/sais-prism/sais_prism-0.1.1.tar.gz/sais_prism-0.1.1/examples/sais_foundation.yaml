foundation:
  experiment_name: "example_llm_finetune"

unified_data_access:
  enabled: true
  token: "demo_token"
  data_access:
    - dataset_names: ["ear5", "mars"]

ml:
  enabled: true
  auto_log: true
  system_tracing: true
  parameters:
    learning_rate: 0.001
    batch_size: 32
    epochs: 10
  metrics: ["train_loss","val_loss","train_accuracy","val_accuracy","f1"]
  artifacts: [] 
  model_repo:
    model_uri: "runs:/{run_id}/artifacts/model"
    name: "llama_models"
    await_registration_for: 300
    tag:
      framework: "pytorch"
      task_type: "language-model"
      model_type: "llama"
      base_model: "Llama-3.2-1B-Instruct"
    version: "1.0.1"
