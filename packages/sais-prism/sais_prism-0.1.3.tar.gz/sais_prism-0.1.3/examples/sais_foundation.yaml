foundation:
  experiment_name: "example_llm_finetune"

unified_data_access:
  enabled: false
  token: "demo_token"
  data_access:
    - dataset_names: ["alpaca_sft_dataset.jsonl", "mars"]
ml:
  enabled: true
  auto_log: true
  system_tracing: true
  parameters:
    base_model: "meta-llama/Llama-3.2-1B"
    output_dir: "./results"
    num_train_epochs: 3  # Number of training epochs
    per_device_train_batch_size: 2  # Small batch size to fit in memory
    gradient_accumulation_steps: 8  # Accumulate gradients to simulate larger batch size
    learning_rate: 2e-5
    weight_decay: 0.01
    warmup_steps: 200  # Reduced warmup steps
    save_total_limit: 1  # Keep only one checkpoint
    logging_dir: "./logs"
    logging_steps: 20  # Log less frequently to save resources
    save_strategy: "epoch"  # Save at the end of each epoch
    evaluation_strategy: "no"  # No evaluation during training
    report_to: none"  # Disable reporting to external tools
    optim: "adamw_torch"  # Optimized AdamW optimizer
    gradient_checkpointing: True
  metrics: ["train_loss","val_loss","train_accuracy","val_accuracy","f1"]
  artifacts: [] 
  model_repo:
    model_uri: "runs:/{run_id}/artifacts/model"
    name: "llama_models"
    await_registration_for: 300
    tag:
      framework: "pytorch"
      task_type: "language-model"
      model_type: "llama"
      base_model: "meta-llama/Llama-3.2-1B"
    version: "1.0.1"
