{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../rtsvg/') # base location of the .py classes\n",
    "from rtsvg import *\n",
    "rt = RACETrack()\n",
    "\n",
    "from transformers import AutoTokenizer, RobertaForMaskedLM\n",
    "import torch\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Source:  https://en.wikipedia.org/wiki/Apollo_13\n",
    "# ... four sentence summary\n",
    "# ... chatgpt gave me five though :(\n",
    "#\n",
    "_text = \"\"\"Apollo 13 (April 11–17, 1970) was the seventh crewed mission in the Apollo space program and the third meant to land on the Moon. The craft was launched from Kennedy Space Center on April 11, 1970, but the lunar landing was aborted after an oxygen tank in the service module (SM) failed two days into the mission. The crew instead looped around the Moon in a circumlunar trajectory and returned safely to Earth on April 17. The mission was commanded by Jim Lovell, with Jack Swigert as command module (CM) pilot and Fred Haise as Lunar Module (LM) pilot. Swigert was a late replacement for Ken Mattingly, who was grounded after exposure to rubella.\n",
    "A routine stir of an oxygen tank ignited damaged wire insulation inside it, causing an explosion that vented the contents of both of the SM's oxygen tanks to space. Without oxygen, needed for breathing and for generating electric power, the SM's propulsion and life support systems could not operate. The CM's systems had to be shut down to conserve its remaining resources for reentry, forcing the crew to transfer to the LM as a lifeboat. With the lunar landing canceled, mission controllers worked to bring the crew home alive.\n",
    "Although the LM was designed to support two men on the lunar surface for two days, Mission Control in Houston improvised new procedures so it could support three men for four days. The crew experienced great hardship, caused by limited power, a chilly and wet cabin and a shortage of potable water. There was a critical need to adapt the CM's cartridges for the carbon dioxide scrubber system to work in the LM; the crew and mission controllers were successful in improvising a solution. The astronauts' peril briefly renewed public interest in the Apollo program; tens of millions watched the splashdown in the South Pacific Ocean on television.\n",
    "An investigative review board found fault with preflight testing of the oxygen tank and Teflon being placed inside it. The board recommended changes, including minimizing the use of potentially combustible items inside the tank; this was done for Apollo 14. The story of Apollo 13 has been dramatized several times, most notably in the 1995 film Apollo 13 based on Lost Moon, the 1994 memoir co-authored by Lovell – and an episode of the 1998 miniseries From the Earth to the Moon.\"\"\"\n",
    "# model, tokenizer, device = rt.__textTrainRoBERTaModel__(_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = RobertaForMaskedLM.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_commented_out = '''\n",
    "inputs = tokenizer('<mask> One landed at Kennedy Space Center. ', return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    output = model(**inputs)\n",
    "    logits = output.logits\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "print('mask_token_index =', int(mask_token_index))\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "tokenizer.decode(predicted_token_id)\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_commented_out = '''\n",
    "top_k_indices = tf.math.top_k(logits.detach().numpy(), 10).indices[0].numpy()\n",
    "# tokenizer.decode(top_k_indices[mask_token_index])\n",
    "for x in top_k_indices[mask_token_index]:\n",
    "    print(tokenizer.decode(x))\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_commented_out = '''\n",
    "labels = tokenizer('Flight One landed at Kennedy Space Center. ', return_tensors=\"pt\")[\"input_ids\"]\n",
    "# labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n",
    "outputs = model(**inputs, labels=labels)\n",
    "round(outputs.loss.item(), 2)\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_commented_out = '''\n",
    "token_i = int(mask_token_index)\n",
    "_sort   = []\n",
    "for i in range(len(outputs[1][0][token_i])):\n",
    "    _sort.append((outputs[1][0][token_i][i], i, tokenizer.decode(i)))\n",
    "    _str = tokenizer.decode(i).strip().lower()\n",
    "    if 'flight' in _str:\n",
    "        print('\"' + tokenizer.decode(i) + '\"' + f' {outputs[1][0][token_i][i]}')\n",
    "_sorted = sorted(_sort, reverse=True)\n",
    "for i in range(0,20):\n",
    "    print(_sorted[i])\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_commented_out = '''\n",
    "histogram  = outputs[1][0][token_i].histogram(bins=20)\n",
    "word_score = float(outputs[1][0][token_i][labels[0][token_i]])\n",
    "print('word score =', word_score)\n",
    "bg         = {'#ff0000':[(word_score,0),(word_score-1,10000),(word_score+1,10000)]}\n",
    "_df        = pd.DataFrame({'w':histogram.hist.detach().numpy(), 'x':histogram.bin_edges[1:].detach().numpy(), 'gb':np.ones(len(histogram.hist))})\n",
    "rt.displaySVG(rt.xy(_df, x_field='x', y_field='w', count_by='w', dot_size='small', line_groupby_field='gb', bg_shape_lu=bg, w=1024,h=96, draw_x_gridlines=True))\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_score = float(outputs[1][0][token_i][labels[0][token_i]])\n",
    "word_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
