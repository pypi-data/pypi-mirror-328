{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import heapq\n",
    "import sys\n",
    "sys.path.insert(1, '../rtsvg')\n",
    "from rtsvg import *\n",
    "rt = RACETrack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'fm':['a','a','b','b','b','c','e','f'],\n",
    "                   'to':['b','d','a','c','d','d','d','d']})\n",
    "params = {'df':df, 'relationships':[('fm','to')], 'draw_labels':True, 'txt_h':20, 'w':256, 'h':256, 'x_ins':10, 'y_ins':10}\n",
    "rt.tile([rt.chordDiagram(                                           **params),\n",
    "         rt.chordDiagram(link_style='wide',                         **params),\n",
    "         rt.chordDiagram(                   equal_size_nodes=True,  **params),\n",
    "         rt.chordDiagram(link_style='wide', equal_size_nodes=True,  **params)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2024-03-08 - doesn't work ... module 'networkx' has no attribute 'to_scipy_sparse_matrix'\n",
    "#\n",
    "# Following from the documentation located here:\n",
    "#\n",
    "# https://markov-clustering.readthedocs.io/en/latest/readme.html\n",
    "#\n",
    "\n",
    "#import markov_clustering as mc\n",
    "#import networkx as nx\n",
    "#import random\n",
    "## number of nodes to use\n",
    "#numnodes = 200\n",
    "## generate random positions as a dictionary where the key is the node id and the value\n",
    "## is a tuple containing 2D coordinates\n",
    "#positions = {i:(random.random() * 2 - 1, random.random() * 2 - 1) for i in range(numnodes)}\n",
    "## use networkx to generate the graph\n",
    "#network = nx.random_geometric_graph(numnodes, 0.3, pos=positions)\n",
    "## then get the adjacency matrix (in sparse form)\n",
    "#matrix = nx.to_scipy_sparse_matrix(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_lots_of_netflow = True\n",
    "if load_lots_of_netflow:\n",
    "    _base_ = '../../data/2013_vast_challenge/mc3_netflow/nf/'\n",
    "    df = pl.concat([pl.read_csv(_base_ + 'nf-chunk1.csv'),\n",
    "                    pl.read_csv(_base_ + 'nf-chunk2.csv'),\n",
    "                    pl.read_csv(_base_ + 'nf-chunk3.csv')])\n",
    "    df = df.rename({'TimeSeconds':'secs',                  'parsedDate':'timestamp',                'dateTimeStr':'timestamp_str',\n",
    "                    'ipLayerProtocol':'pro_str',           'ipLayerProtocolCode':'pro',             'firstSeenSrcIp':'sip',\n",
    "                    'firstSeenDestIp':'dip',               'firstSeenSrcPort':'spt',                'firstSeenDestPort':'dpt',\n",
    "                    'moreFragments':'mfrag',               'contFragments':'cfrag',                 'durationSeconds':'dur',\n",
    "                    'firstSeenSrcPayloadBytes':'soct_pay', 'firstSeenDestPayloadBytes':'doct_pay',  'firstSeenSrcTotalBytes':'soct',\n",
    "                    'firstSeenDestTotalBytes':'doct',      'firstSeenSrcPacketCount':'spkt',        'firstSeenDestPacketCount':'dpkt',\n",
    "                    'recordForceOut':'out'})\n",
    "    df = df.sample(100000)\n",
    "else:\n",
    "    df = pl.DataFrame({'sip':['1.2.3.4'], 'dip':['5.6.7.8']})\n",
    "cd  = rt.chordDiagram(df, [('sip','dip')], equal_size_nodes=False, draw_labels=False, txt_h=16, w=200, h=200, x_ins=2, y_ins=2, dendrogram_algorithm='hdbscan')\n",
    "cd2 = rt.chordDiagram(df, [('sip','dip')], equal_size_nodes=False, draw_labels=False, txt_h=16, w=200, h=200, x_ins=2, y_ins=2, dendrogram_algorithm=None)\n",
    "rt.tile([cd,cd2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd.time_lu['dendrogram'], cd2.time_lu['dendrogram']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "handled   = set()\n",
    "fmto_list = []\n",
    "span_list = []\n",
    "xs_list   = []\n",
    "ys_list   = []\n",
    "for node in cd.node_dir_arc:\n",
    "    for fm in cd.node_dir_arc[node]:\n",
    "        for to in cd.node_dir_arc[node][fm]:\n",
    "            key = str(fm) + '|' + str(to)\n",
    "            if key not in handled:\n",
    "                handled.add(key)\n",
    "                fm_span  = cd.node_dir_arc[node][fm][to]\n",
    "                fm_coord = (fm_span[0]+fm_span[1])/720.0 - 0.5\n",
    "                xs_list.append(fm_coord)\n",
    "                if fm == node:\n",
    "                    to_span  = cd.node_dir_arc[to][fm][to]\n",
    "                    to_coord = (to_span[0]+to_span[1])/720.0 - 0.5\n",
    "                    fmto_list.append(key)\n",
    "                    span_list.append((fm_coord,to_coord))\n",
    "                    ys_list.append(to_coord)\n",
    "                else:\n",
    "                    to_span  = cd.node_dir_arc[fm][fm][to]\n",
    "                    to_coord = (to_span[0]+to_span[1])/720.0 - 0.5\n",
    "                    fmto_list.append(key)\n",
    "                    span_list.append((fm_coord,to_coord))\n",
    "                    ys_list.append(to_coord)\n",
    "            else:\n",
    "                pass\n",
    "                # print(f'\"{key}\" already handled')\n",
    "clusterer = hdbscan.HDBSCAN()\n",
    "clusterer.fit(span_list)\n",
    "print(\"n_clusters =\", len(set(clusterer.labels_)))\n",
    "rt.xy(pd.DataFrame({'x':xs_list,'y':ys_list,'c':clusterer.labels_}),x_field='x',y_field='y',color_by='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "fms, tos, children, all, parent_to_children = [],[], set(), set(), {}\n",
    "for edge in clusterer.single_linkage_tree_.to_networkx().edges():\n",
    "    _fm_, _to_ = int(edge[0]), int(edge[1])\n",
    "    fms.append(_fm_), tos.append(_to_)\n",
    "    children.add(_to_), all.add(_fm_), all.add(_to_)\n",
    "    if _fm_ not in parent_to_children.keys():\n",
    "        parent_to_children[_fm_] = set()\n",
    "    parent_to_children[_fm_].add(_to_)\n",
    "dfg  = pd.DataFrame({'fm':fms, 'to':tos})\n",
    "root = (all - children).__iter__().__next__()\n",
    "\n",
    "def __place__(pos, node, x0, x1, y): # whole bunch of nope :( ... doesn't give children enough space because it divides space in half everytime...\n",
    "    pos[node] = ((x0+x1)/2.0, y)\n",
    "    if node in parent_to_children.keys():\n",
    "        num_of_children = len(parent_to_children[node])\n",
    "        perc            = (x1-x0)*0.05\n",
    "        x0 += perc\n",
    "        x1 -= perc\n",
    "        children_w = (x1 - x0)/num_of_children\n",
    "        for child in parent_to_children[node]:\n",
    "            __place__(pos, child, x0, x0+children_w, y-1)\n",
    "            x0 += children_w\n",
    "pos = {}\n",
    "__place__(pos, root, -10.0, 10.0, 0)\n",
    "\n",
    "def __leafWalk__(node):\n",
    "    if node in parent_to_children.keys():\n",
    "        ls = []\n",
    "        for child in parent_to_children[node]:\n",
    "            ls_children = __leafWalk__(child)\n",
    "            if ls_children is not None:\n",
    "                ls.extend(ls_children)\n",
    "        return ls\n",
    "    else:\n",
    "        return [node]\n",
    "leaves_in_order = __leafWalk__(root)\n",
    "x, pos = 0.0, {}\n",
    "for leaf in leaves_in_order:\n",
    "    pos[leaf] = (x, 0)\n",
    "    x += 1.0\n",
    "for edge in clusterer.single_linkage_tree_.to_networkx().edges():\n",
    "    _fm_, _to_ = int(edge[0]), int(edge[1])\n",
    "    if _to_ in pos.keys():\n",
    "        all_children_placed = True\n",
    "        x_min, x_max = pos[_to_][0], pos[_to_][0]\n",
    "        y_min, y_max = pos[_to_][1], pos[_to_][1]\n",
    "        for child in parent_to_children[_fm_]:\n",
    "            if child not in pos.keys():\n",
    "                all_children_placed = False\n",
    "            else:\n",
    "                x_min, x_max = min(x_min, pos[child][0]), max(x_max, pos[child][0])\n",
    "                y_min, y_max = min(y_min, pos[child][1]), max(y_max, pos[child][1])\n",
    "        if all_children_placed:\n",
    "            pos[_fm_] = (x_min, y_max + 1.0)\n",
    "            for child in parent_to_children[_fm_]:\n",
    "                pos[child] = (pos[child][0], y_max)\n",
    "print(leaves_in_order)\n",
    "rt.linkNode(dfg, [('fm','to')], pos, w=900, h=900, draw_labels=True, node_size='small', link_arrow=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_ = clusterer.condensed_tree_.to_pandas()\n",
    "_df_[_df_['child_size'] == 1]['child']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __dist__(a,b):\n",
    "    print(a,b)\n",
    "    return 1.0\n",
    "my_hdbscan = hdbscan.HDBSCAN(metric=__dist__)\n",
    "items = [(0,),(1,),(2,),(3,)]\n",
    "my_hdbscan.fit(items)\n",
    "my_hdbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = df.group_by(['sip','dip']).agg(pl.col('soct').sum()).partition_by(['sip','dip'], as_dict=True)\n",
    "# my_dict[('172.30.0.4','10.7.7.10')]['soct'][0] # example value lookup\n",
    "items_as_str = list(set(df['sip']) | set(df['dip']))\n",
    "items        = [(int(x),) for x in range(len(items_as_str))]\n",
    "def __dist__(ai,bi):\n",
    "    a = items_as_str[int(ai[0])]\n",
    "    b = items_as_str[int(bi[0])]\n",
    "    if   (a,b) in my_dict:\n",
    "        return 1.0 / my_dict[(a,b)]['soct'][0]\n",
    "    elif (b,a) in my_dict:\n",
    "        return 1.0 / my_dict[(b,a)]['soct'][0]\n",
    "    else:\n",
    "        return 10.0\n",
    "my_hdbscan = hdbscan.HDBSCAN(metric=__dist__)\n",
    "my_hdbscan.fit(items)\n",
    "my_hdbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
