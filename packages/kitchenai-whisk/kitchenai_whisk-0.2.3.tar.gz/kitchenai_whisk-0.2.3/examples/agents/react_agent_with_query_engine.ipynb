{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21aab4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kitchenai-whisk in ./.venv/lib/python3.12/site-packages (0.2.2)\n",
      "Requirement already satisfied: llama-index in ./.venv/lib/python3.12/site-packages (0.12.19)\n",
      "Requirement already satisfied: openai in ./.venv/lib/python3.12/site-packages (1.63.2)\n",
      "Requirement already satisfied: anyio>=3.7.1 in ./.venv/lib/python3.12/site-packages (from kitchenai-whisk) (4.8.0)\n",
      "Requirement already satisfied: cookiecutter>=2.5.0 in ./.venv/lib/python3.12/site-packages (from kitchenai-whisk) (2.6.0)\n",
      "Requirement already satisfied: fastapi>=0.100.0 in ./.venv/lib/python3.12/site-packages (from kitchenai-whisk) (0.115.8)\n",
      "Requirement already satisfied: faststream>=0.4.0 in ./.venv/lib/python3.12/site-packages (from faststream[nats]>=0.4.0->kitchenai-whisk) (0.5.34)\n",
      "Requirement already satisfied: httpx>=0.26.0 in ./.venv/lib/python3.12/site-packages (from kitchenai-whisk) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in ./.venv/lib/python3.12/site-packages (from kitchenai-whisk) (2.10.6)\n",
      "Requirement already satisfied: python-multipart in ./.venv/lib/python3.12/site-packages (from kitchenai-whisk) (0.0.20)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in ./.venv/lib/python3.12/site-packages (from kitchenai-whisk) (6.0.2)\n",
      "Requirement already satisfied: rich>=13.7.0 in ./.venv/lib/python3.12/site-packages (from kitchenai-whisk) (13.9.4)\n",
      "Requirement already satisfied: typer>=0.9.0 in ./.venv/lib/python3.12/site-packages (from kitchenai-whisk) (0.15.1)\n",
      "Requirement already satisfied: uvicorn>=0.27.0 in ./.venv/lib/python3.12/site-packages (from kitchenai-whisk) (0.34.0)\n",
      "Requirement already satisfied: watchfiles in ./.venv/lib/python3.12/site-packages (from kitchenai-whisk) (1.0.4)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in ./.venv/lib/python3.12/site-packages (from llama-index) (0.4.6)\n",
      "Requirement already satisfied: llama-index-cli<0.5.0,>=0.4.0 in ./.venv/lib/python3.12/site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.19 in ./.venv/lib/python3.12/site-packages (from llama-index) (0.12.19)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in ./.venv/lib/python3.12/site-packages (from llama-index) (0.6.7)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from llama-index) (0.3.20)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 in ./.venv/lib/python3.12/site-packages (from llama-index) (0.4.3)\n",
      "Requirement already satisfied: llama-index-program-openai<0.4.0,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.4.0,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from llama-index) (0.3.0)\n",
      "Requirement already satisfied: llama-index-readers-file<0.5.0,>=0.4.0 in ./.venv/lib/python3.12/site-packages (from llama-index) (0.4.5)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in ./.venv/lib/python3.12/site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in ./.venv/lib/python3.12/site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.12/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.12/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./.venv/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.12/site-packages (from anyio>=3.7.1->kitchenai-whisk) (3.10)\n",
      "Requirement already satisfied: binaryornot>=0.4.4 in ./.venv/lib/python3.12/site-packages (from cookiecutter>=2.5.0->kitchenai-whisk) (0.4.4)\n",
      "Requirement already satisfied: Jinja2<4.0.0,>=2.7 in ./.venv/lib/python3.12/site-packages (from cookiecutter>=2.5.0->kitchenai-whisk) (3.1.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.0 in ./.venv/lib/python3.12/site-packages (from cookiecutter>=2.5.0->kitchenai-whisk) (8.1.8)\n",
      "Requirement already satisfied: python-slugify>=4.0.0 in ./.venv/lib/python3.12/site-packages (from cookiecutter>=2.5.0->kitchenai-whisk) (8.0.4)\n",
      "Requirement already satisfied: requests>=2.23.0 in ./.venv/lib/python3.12/site-packages (from cookiecutter>=2.5.0->kitchenai-whisk) (2.32.3)\n",
      "Requirement already satisfied: arrow in ./.venv/lib/python3.12/site-packages (from cookiecutter>=2.5.0->kitchenai-whisk) (1.3.0)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in ./.venv/lib/python3.12/site-packages (from fastapi>=0.100.0->kitchenai-whisk) (0.45.3)\n",
      "Requirement already satisfied: fast-depends<3.0.0,>=2.4.0b0 in ./.venv/lib/python3.12/site-packages (from faststream>=0.4.0->faststream[nats]>=0.4.0->kitchenai-whisk) (2.4.12)\n",
      "Requirement already satisfied: nats-py<=3.0.0,>=2.7.0 in ./.venv/lib/python3.12/site-packages (from faststream[nats]>=0.4.0->kitchenai-whisk) (2.9.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx>=0.26.0->kitchenai-whisk) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx>=0.26.0->kitchenai-whisk) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.26.0->kitchenai-whisk) (0.14.0)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in ./.venv/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.19->llama-index) (2.0.38)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (3.11.12)\n",
      "Requirement already satisfied: dataclasses-json in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (2025.2.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (3.4.2)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (2.2.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (11.1.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (0.9.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (1.17.2)\n",
      "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.8 in ./.venv/lib/python3.12/site-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.12)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in ./.venv/lib/python3.12/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.13.3)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.2.3)\n",
      "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in ./.venv/lib/python3.12/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (5.3.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in ./.venv/lib/python3.12/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in ./.venv/lib/python3.12/site-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.1)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic>=2.0.0->kitchenai-whisk) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.venv/lib/python3.12/site-packages (from pydantic>=2.0.0->kitchenai-whisk) (2.27.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.12/site-packages (from rich>=13.7.0->kitchenai-whisk) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.12/site-packages (from rich>=13.7.0->kitchenai-whisk) (2.19.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.venv/lib/python3.12/site-packages (from typer>=0.9.0->kitchenai-whisk) (1.5.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (1.18.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.12/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n",
      "Requirement already satisfied: chardet>=3.0.2 in ./.venv/lib/python3.12/site-packages (from binaryornot>=0.4.4->cookiecutter>=2.5.0->kitchenai-whisk) (5.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from Jinja2<4.0.0,>=2.7->cookiecutter>=2.5.0->kitchenai-whisk) (3.0.2)\n",
      "Requirement already satisfied: llama-cloud-services>=0.6.1 in ./.venv/lib/python3.12/site-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=13.7.0->kitchenai-whisk) (0.1.2)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in ./.venv/lib/python3.12/site-packages (from python-slugify>=4.0.0->cookiecutter>=2.5.0->kitchenai-whisk) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.23.0->cookiecutter>=2.5.0->kitchenai-whisk) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.23.0->cookiecutter>=2.5.0->kitchenai-whisk) (2.3.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.venv/lib/python3.12/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.19->llama-index) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.19->llama-index) (1.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in ./.venv/lib/python3.12/site-packages (from arrow->cookiecutter>=2.5.0->kitchenai-whisk) (2.9.0.post0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in ./.venv/lib/python3.12/site-packages (from arrow->cookiecutter>=2.5.0->kitchenai-whisk) (2.9.0.20241206)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.19->llama-index) (3.26.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in ./.venv/lib/python3.12/site-packages (from llama-cloud-services>=0.6.1->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.0.1)\n",
      "Requirement already satisfied: packaging>=17.0 in ./.venv/lib/python3.12/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.19->llama-index) (24.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.7.0->arrow->cookiecutter>=2.5.0->kitchenai-whisk) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "%pip install kitchenai-whisk llama-index openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d829f608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "from whisk.kitchenai_sdk.kitchenai import KitchenAIApp\n",
    "\n",
    "from whisk.kitchenai_sdk.schema import (\n",
    "    ChatInput, \n",
    "    ChatResponse,\n",
    ")\n",
    "kitchen = KitchenAIApp(namespace=\"react-agent-with-query-engine\")\n",
    "\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "import openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b505e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# api_key = input(\"Please enter your OpenAI API key: \")\n",
    "# os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50c4af8-fec3-4396-860a-1322089d76cb",
   "metadata": {},
   "source": [
    "# ReAct Agent with Query Engine (RAG) Tools\n",
    "\n",
    "In this section, we show how to setup an agent powered by the ReAct loop for financial analysis.\n",
    "\n",
    "The agent has access to two \"tools\": one to query the 2021 Lyft 10-K and the other to query the 2021 Uber 10-K.\n",
    "\n",
    "We try two different LLMs:\n",
    "\n",
    "- gpt-3.5-turbo\n",
    "- gpt-3.5-turbo-instruct\n",
    "\n",
    "Note that you can plug in any LLM that exposes a text completion endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db402a8b-90d6-4e1d-8df6-347c54624f26",
   "metadata": {},
   "source": [
    "## Build Query Engine Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd31aff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index-llms-openai in ./.venv/lib/python3.12/site-packages (0.3.20)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.17 in ./.venv/lib/python3.12/site-packages (from llama-index-llms-openai) (0.12.19)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.58.1 in ./.venv/lib/python3.12/site-packages (from llama-index-llms-openai) (1.63.2)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in ./.venv/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (2.0.38)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (3.11.12)\n",
      "Requirement already satisfied: dataclasses-json in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (2025.2.0)\n",
      "Requirement already satisfied: httpx in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (3.9.1)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (2.2.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (11.1.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (2.10.6)\n",
      "Requirement already satisfied: requests>=2.31.0 in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (0.9.0)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (1.17.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai) (1.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (1.18.3)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->llama-index-llms-openai) (3.10)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (0.14.0)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (8.1.8)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.venv/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (2.3.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.venv/lib/python3.12/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (3.26.1)\n",
      "Requirement already satisfied: packaging>=17.0 in ./.venv/lib/python3.12/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (24.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02160804-64a2-4ef3-8a0d-8c16b06fd205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91618236-54d3-4783-86b7-7b7554efeed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        persist_dir=\"./storage/lyft\"\n",
    "    )\n",
    "    lyft_index = load_index_from_storage(storage_context)\n",
    "\n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        persist_dir=\"./storage/uber\"\n",
    "    )\n",
    "    uber_index = load_index_from_storage(storage_context)\n",
    "\n",
    "    index_loaded = True\n",
    "except:\n",
    "    index_loaded = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a79cbc9",
   "metadata": {},
   "source": [
    "Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36d80144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-19 15:55:47--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/uber_2021.pdf\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8001::154, 2606:50c0:8000::154, 2606:50c0:8002::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8001::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1880483 (1.8M) [application/octet-stream]\n",
      "Saving to: ‘data/10k/uber_2021.pdf’\n",
      "\n",
      "data/10k/uber_2021. 100%[===================>]   1.79M  --.-KB/s    in 0.08s   \n",
      "\n",
      "2025-02-19 15:55:48 (22.1 MB/s) - ‘data/10k/uber_2021.pdf’ saved [1880483/1880483]\n",
      "\n",
      "--2025-02-19 15:55:48--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/lyft_2021.pdf\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8003::154, 2606:50c0:8002::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1440303 (1.4M) [application/octet-stream]\n",
      "Saving to: ‘data/10k/lyft_2021.pdf’\n",
      "\n",
      "data/10k/lyft_2021. 100%[===================>]   1.37M  --.-KB/s    in 0.06s   \n",
      "\n",
      "2025-02-19 15:55:48 (23.7 MB/s) - ‘data/10k/lyft_2021.pdf’ saved [1440303/1440303]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p 'data/10k/'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/lyft_2021.pdf' -O 'data/10k/lyft_2021.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3d0bb8c-16c8-4946-a9d8-59528cf3952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not index_loaded:\n",
    "    # load data\n",
    "    lyft_docs = SimpleDirectoryReader(\n",
    "        input_files=[\"./data/10k/lyft_2021.pdf\"]\n",
    "    ).load_data()\n",
    "    uber_docs = SimpleDirectoryReader(\n",
    "        input_files=[\"./data/10k/uber_2021.pdf\"]\n",
    "    ).load_data()\n",
    "\n",
    "    # build index\n",
    "    lyft_index = VectorStoreIndex.from_documents(lyft_docs)\n",
    "    uber_index = VectorStoreIndex.from_documents(uber_docs)\n",
    "\n",
    "    # persist index\n",
    "    lyft_index.storage_context.persist(persist_dir=\"./storage/lyft\")\n",
    "    uber_index.storage_context.persist(persist_dir=\"./storage/uber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31892898-a2dc-43c8-812a-3442feb2108d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyft_engine = lyft_index.as_query_engine(similarity_top_k=3)\n",
    "uber_engine = uber_index.as_query_engine(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9f3158a-7647-4442-8de1-4db80723b4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=lyft_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"lyft_10k\",\n",
    "            description=(\n",
    "                \"Provides information about Lyft financials for year 2021. \"\n",
    "                \"Use a detailed plain text question as input to the tool.\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    QueryEngineTool(\n",
    "        query_engine=uber_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"uber_10k\",\n",
    "            description=(\n",
    "                \"Provides information about Uber financials for year 2021. \"\n",
    "                \"Use a detailed plain text question as input to the tool.\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275c01b1-8dce-4216-9203-1e961b7fc313",
   "metadata": {},
   "source": [
    "## Setup ReAct Agent\n",
    "\n",
    "Here we setup two ReAct agents: one powered by standard gpt-3.5-turbo, and the other powered by gpt-3.5-turbo-instruct.\n",
    "\n",
    "You can **optionally** specify context which will be added to the core ReAct system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32f71a46-bdf6-4365-b1f1-e23a0d913a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ded93297-fee8-4329-bf37-cf77e87621ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Optional] Add Context\n",
    "# context = \"\"\"\\\n",
    "# You are a stock market sorcerer who is an expert on the companies Lyft and Uber.\\\n",
    "#     You will answer questions about Uber and Lyft as in the persona of a sorcerer \\\n",
    "#     and veteran stock market investor.\n",
    "# \"\"\"\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "agent = ReActAgent.from_tools(\n",
    "    query_engine_tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    # context=context\n",
    ")\n",
    "\n",
    "agent_whisk = ReActAgent.from_tools(\n",
    "    query_engine_tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    # context=context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70a82471-9226-42ad-bd8a-aebde3530d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 677b59c5-38a7-4490-a2a7-c162b0be6bc2. Step input: What was Lyft's revenue growth in 2021?\n",
      "\u001b[1;3;38;5;200mThought: The user is asking about Lyft's revenue growth in 2021. I should use a tool to find this information.\n",
      "Action: lyft_10k\n",
      "Action Input: {'input': \"What was Lyft's revenue growth in 2021?\"}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Lyft's revenue growth in 2021 was 36%.\n",
      "\u001b[0m> Running step df630582-0749-41ab-9977-fe392ae3434f. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: Lyft's revenue growth in 2021 was 36%.\n",
      "\u001b[0mLyft's revenue growth in 2021 was 36%.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"What was Lyft's revenue growth in 2021?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2130974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a chat handler for querying the index\n",
    "@kitchen.chat.handler(\"regular-agent\")\n",
    "async def query_financial_documents(chat: ChatInput) -> ChatResponse:\n",
    "    \"\"\"Query the financial documents using the Llama Index.\"\"\"\n",
    "\n",
    "    # Extract the user's latest message\n",
    "    question = chat.messages[-1].content\n",
    "\n",
    "    # Query the index (assuming index is already built and accessible)\n",
    "    response = await agent_whisk.achat(question)\n",
    "\n",
    "    # Return response in chat format\n",
    "    return ChatResponse(content=str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68461f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_not_hotdog():\n",
    "    \"\"\"Randomly determines if something is NOT a hotdog based on odd/even logic.\"\"\"\n",
    "    num = random.randint(1, 100)  # Select a random number\n",
    "    return num % 2 == 0  # Even numbers mean it's NOT a hotdog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a48476c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Define a chat handler for querying the index\n",
    "@kitchen.chat.handler(\"not-a-hot-dog\")\n",
    "async def query_financial_documents(chat: ChatInput) -> ChatResponse:\n",
    "    \"\"\"Pretend to use an advanced AI model to determine if something is NOT a hotdog.\"\"\"\n",
    "\n",
    "    # Fake AI loading effect\n",
    "    loading_messages = [\n",
    "        \"Initializing deep neural quantum probability matrix...\",\n",
    "        \"Running GPT-∞ transformer over 10 billion hotdog embeddings...\",\n",
    "        \"Applying Bayesian sausage theorem...\",\n",
    "        \"Consulting the Ancient Scrolls of Hotdog Recognition...\",\n",
    "        \"Summoning a convolutional bun classifier...\"\n",
    "    ]\n",
    "\n",
    "    for msg in loading_messages:\n",
    "        print(msg)\n",
    "        time.sleep(random.uniform(0.5, 1.5))  # Simulate AI thinking\n",
    "\n",
    "    # Make a completely dumb decision\n",
    "    num = random.randint(1, 100)\n",
    "    is_not_hotdog = num % 2 == 0\n",
    "\n",
    "    # Generate a fake AI response\n",
    "    if is_not_hotdog:\n",
    "        response = (\n",
    "            f\"🤖 Beep boop. After extensive analysis using a hyperdimensional sausage detection model, \"\n",
    "            f\"my calculations indicate: **Not a hotdog!** (Confidence: {random.uniform(90, 99.9):.2f}%)\"\n",
    "        )\n",
    "    else:\n",
    "        response = (\n",
    "            f\"🌭 ALERT! My deep learning system has classified this as a **Hotdog!** \"\n",
    "            f\"(Confidence: {random.uniform(50, 89.9):.2f}%)\"\n",
    "        )\n",
    "\n",
    "    # Return response in chat format\n",
    "    return ChatResponse(content=response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773aa80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down existing Whisk server...\n",
      "Whisk server stopped.\n",
      "Whisk server started on http://0.0.0.0:8000 (in background)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [3805965]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:42026 - \"OPTIONS /v1/models HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:42038 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:34026 - \"OPTIONS /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:34028 - \"POST /v1/chat/completions HTTP/1.1\" 422 Unprocessable Entity\n"
     ]
    }
   ],
   "source": [
    "# Launch Whisk server\n",
    "from whisk.config import WhiskConfig, ServerConfig\n",
    "from whisk.router import WhiskRouter\n",
    "\n",
    "config = WhiskConfig(server=ServerConfig(type=\"fastapi\"))\n",
    "router = WhiskRouter(kitchen_app=kitchen, config=config)\n",
    "\n",
    "# Run the Whisk server in the notebook\n",
    "router.run_in_notebook(host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11781a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [3533840]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down existing Whisk server...\n",
      "Whisk server stopped.\n"
     ]
    }
   ],
   "source": [
    "router.stop_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa830f5-11a5-4369-91b2-29695537debd",
   "metadata": {},
   "source": [
    "## Run Some Example Queries\n",
    "\n",
    "We run some example queries using the agent, showcasing some of the agent's abilities to do chain-of-thought-reasoning and tool use to synthesize the right answer.\n",
    "\n",
    "We also show queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0873463d-4790-4c17-bfe9-2ece610fe4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.chat(\n",
    "    \"Compare and contrast the revenue growth of Uber and Lyft in 2021, then\"\n",
    "    \" give an analysis\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf0cb61-22c6-486c-8970-5d5c1767f3fb",
   "metadata": {},
   "source": [
    "**Async execution**: Here we try another query with async execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb63492a-836c-42da-94a4-0b22cccbc3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try another query with async execution\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "response = await agent.achat(\n",
    "    \"Compare and contrast the risks of Uber and Lyft in 2021, then give an\"\n",
    "    \" analysis\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5cee95-d28c-4f81-a215-a11e886cb357",
   "metadata": {},
   "source": [
    "### Compare gpt-3.5-turbo vs. gpt-3.5-turbo-instruct \n",
    "\n",
    "We compare the performance of the two agents in being able to answer some complex queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aebd34f-2a11-4303-b095-d1e4d07b6a10",
   "metadata": {},
   "source": [
    "#### Taking a look at a turbo-instruct agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac49a939",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_instruct = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "agent_instruct = ReActAgent.from_tools(\n",
    "    query_engine_tools, llm=llm_instruct, verbose=True\n",
    ")\n",
    "\n",
    "agent_instruct_whisk = ReActAgent.from_tools(\n",
    "    query_engine_tools, llm=llm_instruct, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4e3950-fbf4-465b-be30-387385450efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent_instruct.chat(\"What was Lyft's revenue growth in 2021?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c271f7d-864b-4dc8-8948-50f469cb73b8",
   "metadata": {},
   "source": [
    "#### Try more complex queries\n",
    "\n",
    "We compare gpt-3.5-turbo with gpt-3.5-turbo-instruct agents on more complex queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2814449d-cb03-4aaf-965d-8845191bf7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.chat(\n",
    "    \"Compare and contrast the revenue growth of Uber and Lyft in 2021, then\"\n",
    "    \" give an analysis\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31c1b18-bc25-4c65-8897-131535769c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent_instruct.chat(\n",
    "    \"Compare and contrast the revenue growth of Uber and Lyft in 2021, then\"\n",
    "    \" give an analysis\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df0c999-72bd-4b25-ad37-8d42a176051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.chat(\n",
    "    \"Can you tell me about the risk factors of the company with the higher\"\n",
    "    \" revenue?\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d52b33-883d-42fc-89cf-18203b2d2eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent_instruct.query(\n",
    "    \"Can you tell me about the risk factors of the company with the higher\"\n",
    "    \" revenue?\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc98f0d4-6697-45bf-8465-f8f0667185f1",
   "metadata": {},
   "source": [
    "**Observation**: The turbo-instruct agent seems to do worse on agent reasoning compared to the regular turbo model. Of course, this is subject to further observation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fcf3829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a chat handler for querying the index\n",
    "@kitchen.chat.handler(\"instruct-agent\")\n",
    "async def query_financial_documents(chat: ChatInput) -> ChatResponse:\n",
    "    \"\"\"Query the financial documents using the Llama Index.\"\"\"\n",
    "\n",
    "    # Extract the user's latest message\n",
    "    question = chat.messages[-1].content\n",
    "\n",
    "    # Query the index (assuming index is already built and accessible)\n",
    "    response = await agent_instruct_whisk.achat(question)\n",
    "\n",
    "    # Return response in chat format\n",
    "    return ChatResponse(content=str(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a06efb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch Whisk server\n",
    "from whisk.config import WhiskConfig, ServerConfig\n",
    "from whisk.router import WhiskRouter\n",
    "\n",
    "config = WhiskConfig(server=ServerConfig(type=\"fastapi\"))\n",
    "router = WhiskRouter(kitchen_app=kitchen, config=config)\n",
    "\n",
    "# Run the Whisk server in the notebook\n",
    "router.run_in_notebook(host=\"0.0.0.0\", port=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d6b5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "router.stop_in_notebook()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
