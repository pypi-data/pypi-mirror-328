{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install llama-index nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisk.kitchenai_sdk.http_schema import (\n",
    "    ChatCompletionRequest,\n",
    "    ChatCompletionResponse,\n",
    "    ChatCompletionChoice,\n",
    "    ChatResponseMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisk.kitchenai_sdk.kitchenai import KitchenAIApp\n",
    "from whisk.kitchenai_sdk.http_schema import (\n",
    "    ChatCompletionRequest,\n",
    "    ChatCompletionResponse,\n",
    "    ChatCompletionChoice,\n",
    "    ChatResponseMessage\n",
    ")\n",
    "\n",
    "# Initialize the app\n",
    "kitchen = KitchenAIApp(namespace=\"whisk-jupyter-app-modified\", version=\"0.0.2\")\n",
    "\n",
    "@kitchen.chat.handler(\"chat.completions.new\")\n",
    "async def handle_chat(request: ChatCompletionRequest) -> ChatCompletionResponse:\n",
    "    \"\"\"Simple chat handler that echoes back the last message\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    return ChatCompletionResponse(\n",
    "        model=request.model,\n",
    "        choices=[\n",
    "            ChatCompletionChoice(\n",
    "                index=0,\n",
    "                message=ChatResponseMessage(\n",
    "                    role=\"assistant\",\n",
    "                    content=f\"Echo demo: {request.messages[-1].content}\"\n",
    "                ),\n",
    "                finish_reason=\"stop\"\n",
    "            )\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisk.kitchenai_sdk.schema import (\n",
    "    ChatInput, \n",
    "    ChatResponse,\n",
    "    DependencyType,\n",
    "    SourceNode\n",
    ")\n",
    "# # Register dependency\n",
    "# vector_store = MyVectorStore()\n",
    "# kitchen.register_dependency(DependencyType.VECTOR_STORE, vector_store)\n",
    "\n",
    "@kitchen.chat.handler(\"chat.rag\", DependencyType.VECTOR_STORE, DependencyType.LLM)\n",
    "async def rag_handler(chat: ChatInput, vector_store, llm) -> ChatResponse:\n",
    "    \"\"\"RAG-enabled chat handler\"\"\"\n",
    "    # Get the user's question\n",
    "    question = chat.messages[-1].content\n",
    "    \n",
    "    # Search for relevant documents\n",
    "    retriever = vector_store.as_retriever(similarity_top_k=2)\n",
    "    nodes = retriever.retrieve(question)\n",
    "    \n",
    "    # Create context from retrieved documents\n",
    "    context = \"\\n\".join(node.node.text for node in nodes)\n",
    "    prompt = f\"\"\"Answer based on context: {context}\\nQuestion: {question}\"\"\"\n",
    "    \n",
    "    # Get response from LLM\n",
    "    response = await llm.acomplete(prompt)\n",
    "    \n",
    "    # Return response with sources\n",
    "    return ChatResponse(\n",
    "        content=response.text,\n",
    "        sources=[\n",
    "            SourceNode(\n",
    "                text=node.node.text,\n",
    "                metadata=node.node.metadata,\n",
    "                score=node.score\n",
    "            ) for node in nodes\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisk.kitchenai_sdk.schema import (\n",
    "    WhiskStorageSchema,\n",
    "    WhiskStorageResponseSchema\n",
    ")\n",
    "import time\n",
    "\n",
    "@kitchen.storage.handler(\"storage\")\n",
    "async def storage_handler(data: WhiskStorageSchema) -> WhiskStorageResponseSchema:\n",
    "    \"\"\"Storage handler for document ingestion\"\"\"\n",
    "    if data.action == \"list\":\n",
    "        return WhiskStorageResponseSchema(\n",
    "            id=int(time.time()),\n",
    "            name=\"list\",\n",
    "            files=[]\n",
    "        )\n",
    "        \n",
    "    if data.action == \"upload\":\n",
    "        return WhiskStorageResponseSchema(\n",
    "            id=int(time.time()),\n",
    "            name=data.filename,\n",
    "            label=data.model.split('/')[-1],\n",
    "            metadata={\n",
    "                \"namespace\": data.model.split('/')[0],\n",
    "                \"model\": data.model\n",
    "            },\n",
    "            created_at=int(time.time())\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-28' coro=<Server.serve() done, defined at /home/epuerta/.local/share/hatch/env/virtual/kitchenai-whisk/jlwMXi0k/kitchenai-whisk/lib/python3.11/site-packages/uvicorn/server.py:68> exception=KeyboardInterrupt()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/epuerta/.local/share/hatch/env/virtual/kitchenai-whisk/jlwMXi0k/kitchenai-whisk/lib/python3.11/site-packages/uvicorn/main.py\", line 579, in run\n",
      "    server.run()\n",
      "  File \"/home/epuerta/.local/share/hatch/env/virtual/kitchenai-whisk/jlwMXi0k/kitchenai-whisk/lib/python3.11/site-packages/uvicorn/server.py\", line 66, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/epuerta/.local/share/hatch/env/virtual/kitchenai-whisk/jlwMXi0k/kitchenai-whisk/lib/python3.11/site-packages/nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/epuerta/.local/share/hatch/env/virtual/kitchenai-whisk/jlwMXi0k/kitchenai-whisk/lib/python3.11/site-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
      "    self._run_once()\n",
      "  File \"/home/epuerta/.local/share/hatch/env/virtual/kitchenai-whisk/jlwMXi0k/kitchenai-whisk/lib/python3.11/site-packages/nest_asyncio.py\", line 133, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/epuerta/.pyenv/versions/3.11.8/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/epuerta/.pyenv/versions/3.11.8/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n",
      "    self.__step()\n",
      "  File \"/home/epuerta/.pyenv/versions/3.11.8/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/epuerta/.local/share/hatch/env/virtual/kitchenai-whisk/jlwMXi0k/kitchenai-whisk/lib/python3.11/site-packages/uvicorn/server.py\", line 69, in serve\n",
      "    with self.capture_signals():\n",
      "  File \"/home/epuerta/.pyenv/versions/3.11.8/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
      "    next(self.gen)\n",
      "  File \"/home/epuerta/.local/share/hatch/env/virtual/kitchenai-whisk/jlwMXi0k/kitchenai-whisk/lib/python3.11/site-packages/uvicorn/server.py\", line 330, in capture_signals\n",
      "    signal.raise_signal(captured_signal)\n",
      "KeyboardInterrupt\n",
      "INFO:     Started server process [2481637]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59318 - \"OPTIONS /v1/models HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59320 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:47744 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [2481637]\n"
     ]
    }
   ],
   "source": [
    "from whisk.config import WhiskConfig, ServerConfig\n",
    "from whisk.router import WhiskRouter\n",
    "import asyncio\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Configure and create router\n",
    "config = WhiskConfig(server=ServerConfig(type=\"fastapi\"))\n",
    "router = WhiskRouter(kitchen_app=kitchen, config=config)\n",
    "\n",
    "# Run the server\n",
    "\n",
    "router.run(host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kitchenai-whisk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
