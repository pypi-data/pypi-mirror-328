{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(tutorials-hubbard-parallel)=\n",
    "\n",
    "# Parallelizing the computation of Hubbard parameters\n",
    "\n",
    "In this tutorial you will learn how to parallelize the computation of the Hubbard parameters using the {py:class}`~aiida_hubbard.workflows.hp.main.HpWorkChain`.\n",
    "\n",
    "We can divide this goal in two phases:\n",
    "\n",
    "* __Parallelize over independent atoms__: parallelize the ``hp.x`` calculation with multiple sub-``hp.x`` running single atoms.\n",
    "* __Parallelize over independent q points__: parallelize each atom sub-``hp.x`` with other sub-``hp.x`` running single q points.\n",
    "\n",
    "As we learnt from the [previous tutorial](./1_computing_hubbard.ipynb), first we need to compute the ground-state with a ``pw.x`` calculation.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from local_module import load_temp_profile\n",
    "\n",
    "# If you download this file, you can run it with your own profile.\n",
    "# Put these lines instead:\n",
    "# from aiida import load_profile\n",
    "# load_profile()\n",
    "data = load_temp_profile(\n",
    "    name=\"hubbard-parallel-tutorial\",\n",
    "    add_computer=True,\n",
    "    add_pw_code=True,\n",
    "    add_hp_code=True,\n",
    "    add_sssp=True,\n",
    "    add_structure_licoo=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from aiida.engine import run_get_node\n",
    "from aiida.orm import KpointsData\n",
    "from aiida_quantumespresso.workflows.pw.base import PwBaseWorkChain\n",
    "from aiida_quantumespresso.common.types import ElectronicType\n",
    "kpoints = KpointsData()\n",
    "kpoints.set_kpoints_mesh([1,1,1])\n",
    "\n",
    "builder = PwBaseWorkChain.get_builder_from_protocol(\n",
    "    code=data.pw_code, # modify here if you downloaded the notebook\n",
    "    structure=data.structure, # modify here if you downloaded the notebook\n",
    "    protocol=\"fast\",\n",
    "    electronic_type=ElectronicType.INSULATOR,\n",
    "    overrides={\"kpoints\":kpoints, \"clean_workdir\":False}\n",
    ")\n",
    "results, pw_node = run_get_node(builder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelize over atoms\n",
    "\n",
    "To parallelize over atoms, we need a _new_ workchain which is dedicated to this purpose: the {py:class}`~aiida_hubbard.workflows.hp.main.HpWorkChain`. This workchain is able to parallelize both over atoms and over q points.\n",
    "\n",
    "Let's see first the atom parallelization. As usual, we need to get the `builder` and fill the inputs.\n",
    "Specifying the input `parallelize_atoms` as `True` in `HpWorkChain`, each _independent atom_ will be run as a separate `HpBaseWorkChain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "from aiida_hubbard.workflows.hp.main import HpWorkChain\n",
    "\n",
    "builder = HpWorkChain.get_builder_from_protocol(\n",
    "    code=data.hp_code,\n",
    "    protocol=\"fast\",\n",
    "    parent_scf_folder=pw_node.outputs.remote_folder,\n",
    "    overrides={\n",
    "        \"parallelize_atoms\":True, \n",
    "        \"parallelize_qpoints\":False, \n",
    "        \"hp\":{\"hubbard_structure\":data.structure},\n",
    "        \"qpoints_distance\": 100.0, # to get few q points\n",
    "        }\n",
    ")\n",
    "\n",
    "results, hp_node = run_get_node(builder)\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%verdi process status {hp_node.pk}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following just happened:\n",
    "- A grid of q points is generated automatically using the distance (between points) in $\\r{A}^{-1}$ we gave in input (of 100 $\\r{A}^{-1}$ to have very sparse - it is just a tutorial!).\n",
    "- The `HpParallelizeAtomsWorkChain` is called.\n",
    "- This work chain calls first a `HpBaseWorkChain` to get the independent atoms to perturb.\n",
    "- **Three** `HpBaseWorkChain` are submitted __simultaneously__, one for cobalt, and two for the two oxygen sites.\n",
    "- The response matrices ($\\chi^{(0)}$,$\\chi$) of each atom are collected to post-process them and compute the final U/V values using $V_{IJ} = (\\chi^{(0) -1} -\\chi^{-1})_{IJ}$\n",
    "\n",
    "As for the `HpBaseWorkChain`, we also have here the `hubbard_structure` output namespace, containing the same results as the serial execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiida_quantumespresso.utils.hubbard import HubbardUtils\n",
    "print(HubbardUtils(results['hubbard_structure']).get_hubbard_card())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelize q points for each perturbed atom\n",
    "\n",
    "In density-functional perturbation theory, we can simulate linear responses in reciprocal space as monocrhomatic perturbations, described via a grid of __q points__: each q point a monocrhomatic perturbation. The number of q points can be reduced using symmetries, and each Hubbard atom (manifold) will have in principle different number of perturbations.\n",
    "\n",
    "Specifying the input `parallelize_qpoints` as `True` in `HpWorkChain`, each single independent q point _of each atom_ will run as a separate `HpBaseWorkChain`.\n",
    "\n",
    ":::{important}\n",
    "To parallelize over q points you __MUST__ parallelize over atoms as well.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "builder = HpWorkChain.get_builder_from_protocol(\n",
    "    code=data.hp_code,\n",
    "    protocol=\"fast\",\n",
    "    parent_scf_folder=pw_node.outputs.remote_folder,\n",
    "    overrides={\n",
    "        \"parallelize_atoms\":True, \n",
    "        \"parallelize_qpoints\":True,  \n",
    "        \"hp\":{\"hubbard_structure\":data.structure},\n",
    "        \"qpoints_distance\": 1000, # to get few q points\n",
    "        \"max_concurrent_base_workchains\": 2, # useful to not overload HPC or local computer\n",
    "    }\n",
    ")\n",
    "\n",
    "results, hp_node = run_get_node(builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%verdi process status {hp_node.pk}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following just happened:\n",
    "- A grid of q points was generated automatically using the distance (between points) in $\\r{A}^{-1}$ we gave in input (of 1000 $\\r{A}^{-1}$ to have very sparse - it is just a tutorial!).\n",
    "- The `HpParallelizeAtomsWorkChain` is called.\n",
    "- This work chain calls first a `HpBaseWorkChain` to get the independent atoms to perturb.\n",
    "- For independent each atom (three in total) an `HpParallelizeQpointsWorkChain` is submitted __simultaneously__, one for cobalt, and two for the two oxygen sites.\n",
    "- Each of such work chain submit a fist `HpBaseWorkChain` to get the independent q points (in this case, only 1).\n",
    "- An `HpBaseWorkCahin` is run for every q points, executed at the same time. __Imagine this on an HPC!__ :rocket:\n",
    "- The response matrices ($\\chi^{(0)}_{\\mathbf{q}}$,$\\chi_{\\mathbf{q}}$) of each q point for each atom are collected to post-process them and compute the atomic response matrices.\n",
    "- A last final `HpBaseWorkChain` collects such matrices to compute U/V values.\n",
    "\n",
    "And we check the results are the same as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(HubbardUtils(results['hubbard_structure']).get_hubbard_card())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final considerations\n",
    "\n",
    "We managed to compute the Hubbard parameters __parallelizing over atoms and q points__! :tada:\n",
    "\n",
    "Still, you might need to converge self-consistently the parameters using the iterative procedure of relax -> scf -> hubbard.\n",
    "Learn the automated way [in the last tutorial](./3_self_consistent.ipynb)!\n",
    "\n",
    ":::{admonition} Learn more and in details\n",
    ":class: hint\n",
    "\n",
    "To learn the full sets of inputs, to use proficiently the `get_builder_from_protocol` and more, have a look at the following sections:\n",
    "- [Specific how tos](howto/workflows/hp/main.md)\n",
    "- [General information of the implemented workchain](topics/workflows/hp/main.md)\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
