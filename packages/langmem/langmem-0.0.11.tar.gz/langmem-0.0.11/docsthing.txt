---
docs/docs/background_quickstart.md
---
title: Background Quickstart Guide
description: Get started processing memories "in the background".
---

# Background Quickstart Guide

Memories can be created in two ways:

1. In the hot path: the agent consciously saves notes using tools (see [Hot path quickstart](hot_path_quickstart.md)).
2. ðŸ‘‰**In the background (this guide)**: memories are "subconsciously" extracted automatically from conversations.

![Hot Path Quickstart Diagram](concepts/img/hot_path_vs_background.png)

This guide shows you how to extract and consolidate memories in the background using [`create_memory_store_manager`](). The agent will continue as normal while memories are processed in the background.

# Prerequisites

First, install LangMem:

```bash
pip install -U langmem
```

Configure your environment with an API key for your favorite LLM provider:

```bash
export ANTHROPIC_API_KEY="sk-..."  # Or another supported LLM provider
```

## Basic Usage

```python
from langchain.chat_models import init_chat_model
from langgraph.func import entrypoint
from langgraph.store.memory import InMemoryStore

from langmem import ReflectionExecutor, create_memory_store_manager

store = InMemoryStore( # (4)
    index={
        "dims": 1536,
        "embed": "openai:text-embedding-3-small",
    }
)  
llm = init_chat_model("anthropic:claude-3-5-sonnet-latest")

# Create memory manager Runnable to extract memories from conversations
memory_manager = create_memory_store_manager(
    "anthropic:claude-3-5-sonnet-latest",
    # Store memories in the "memories" namespace (aka directory)
    namespace=("memories",),  # (1)
)

@entrypoint(store=store)  # Create a LangGraph workflow
async def chat(message: str):
    response = llm.invoke(message)

    # memory_manager extracts memories from conversation history
    # We'll provide it in OpenAI's message format
    to_process = {"messages": [{"role": "user", "content": message}] + [response]}
    await memory_manager.ainvoke(to_process)  # (2)
    return response.content
# Run conversation as normal
response = await chat.ainvoke(
    "I like dogs. My dog's name is Fido.",
)
print(response)
# Output: That's nice! Dogs make wonderful companions. Fido is a classic dog name. What kind of dog is Fido?
```

1. The `namespace` parameter lets you isolate memories that are stored and retrieved. In this example, we store memories in a global "memories" path, but you could instead use template variables to scope to a user-specific path based on configuration. See [how to dynamically configure namespaces](guides/dynamically_configure_namespaces.md) for more information.

2. We are using the memory manager to process the conversation every time a new message arrives. For more efficient processing patterns that let you debounce (aka avoid redundant) work, see [Delayed Memory Processing](guides/delayed_processing.md).

3. You can also process memories directly with `memory_manager.process(messages)` if you don't need background processing

4. What's a store? It's a document store you can add vector-search too. The "InMemoryStore" is, as it says, saved in-memory and not persistent.

    !!! tip "For Production"
    Use a persistent store like [`AsyncPostgresStore`](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.postgres.AsyncPostgresStore) instead of `InMemoryStore` to persist data between restarts.


If you want to see what memories have been extracted, you can search the store:

```python
# (in case our memory manager is still running)
print(store.search(("memories",)))
# [
#     Item(
#         namespace=["memories"],
#         key="0145905e-2b78-4675-9a54-4cb13099bd0b",
#         value={"kind": "Memory", "content": {"content": "User likes dogs as pets"}},
#         created_at="2025-02-06T18:54:32.568595+00:00",
#         updated_at="2025-02-06T18:54:32.568596+00:00",
#         score=None,
#     ),
#     Item(
#         namespace=["memories"],
#         key="19cc4024-999a-4380-95b1-bb9dddc22d22",
#         value={"kind": "Memory", "content": {"content": "User has a dog named Fido"}},
#         created_at="2025-02-06T18:54:32.568680+00:00",
#         updated_at="2025-02-06T18:54:32.568682+00:00",
#         score=None,
#     ),
# ]
```

!!! tip "For more efficient processing"

    ðŸ’¡ For active conversations, processing every message can be expensive. See [Delayed Memory Processing](guides/delayed_processing.md) to learn how to defer processing until conversation activity settles down.

## What's Next

- [Configure Dynamic Namespaces](guides/dynamically_configure_namespaces.md) - Learn more ways to organize memories by user, agent, or other values.
---
docs/docs/hot_path_quickstart.md
---
title: Hot Path Quickstart Guide
description: Get started with LangMem
---

# Hot Path Quickstart Guide

Memories can be created in two ways:

1. ðŸ‘‰ **In the hot path (this guide):** the agent consciously saves notes using tools.
2. In the background: memories are "subconsciously" extracted automatically from conversations (see [Background Quickstart](background_quickstart.md)).

![Hot Path Quickstart Diagram](concepts/img/hot_path_vs_background.png)

In this guide, we will create a LangGraph agent that actively manages its own long-term memory through LangMem's `manage_memory` tool.

## Prerequisites

First, install LangMem:

```bash
pip install -U langmem
```

Configure your environment with an API key for your favorite LLM provider:

```bash
export ANTHROPIC_API_KEY="sk-..."  # Or another supported LLM provider
```

## Agent

Here's a complete example showing how to create an agent with memory that persists across conversations:

```python hl_lines="16-20 42-46"
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent
from langgraph.store.memory import InMemoryStore
from langgraph.utils.config import get_store 
from langmem import (
    # Lets agent create, update, and delete memories (1)
    create_manage_memory_tool,
)


def prompt(state):
    """Prepare the messages for the LLM."""
    # Get store from configured contextvar; (5)
    store = get_store() # Same as that provided to `create_react_agent`
    memories = store.search(
        # Search within the same namespace as the one
        # we've configured for the agent
        ("memories",),
        query=state["messages"][-1].content,
    )
    system_msg = f"""You are a helpful assistant.

## Memories
<memories>
{memories}
</memories>
"""
    return [{"role": "system", "content": system_msg}, *state["messages"]]


store = InMemoryStore(
    index={ # Store extracted memories (4)
        "dims": 1536,
        "embed": "openai:text-embedding-3-small",
    }
) 
checkpointer = MemorySaver() # Checkpoint graph state (2)

agent = create_react_agent( 
    "anthropic:claude-3-5-sonnet-latest",
    prompt=prompt,
    tools=[ # Add memory tools (3)
        # The agent can call "manage_memory" to
        # create, update, and delete memories by ID
        # Namespaces add scope to memories. To
        # scope memories per-user, do ("memories", "{user_id}"): (6)
        create_manage_memory_tool(namespace=("memories",)),
    ],
    # Our memories will be stored in this provided BaseStore instance
    store=store,
    # And the graph "state" will be checkpointed after each node
    # completes executing for tracking the chat history and durable execution
    checkpointer=checkpointer, 
)
```

1. The tools [`create_manage_memory_tool`](reference/tools.md#langmem.create_manage_memory_tool) and [`create_search_memory_tool`](reference/tools.md#langmem.create_search_memory_tool) allow agents to manually store and retrieve information from their memory. The `namespace` parameter scopes the memories, ensuring that data is kept separate based on however you configure it.

    Here, we save all memories to the ("memories",) namespace, meaning no matter which user interacts with the agent, all memories would be shared in the same directory. We could also configure it to organize memories in other ways. For instance::

    | Organization Pattern | Namespace Example | Use Case |
    |---------------------|------------------|-----------|
    | By user | `("memories", "{user_id}")` | Separate memories per user |
    | By assistant | `("memories", "{assistant_id}")` | An assistant may have memories that span multiple users |
    | By user & organization | `("memories", "{organization_id}", "{user_id}")` | Let you search across an organization while scoping memories per user |
    | Further subdivisions | `("memories", "{user_id}", "manual_memories")` | Organize different types of user data |

    Each entry in a namespace is like a directory on a computer. If you provide a bracketed namespace variable (like "{user_id}"), LangMem will replace it with the value from the `configurable` field in the `RunnableConfig` at runtime.

2. The [`MemorySaver`](https://langchain-ai.github.io/langgraph/reference/checkpoints/) checkpointer maintains conversation history within each "thread". 

    You can think of threads like conversations, akin to an email thread. This "short-term" memory tracks the state of the agent/graph , ensuring that conversations remain independent. For production deployments, use a persistent store like [`AsyncPostgresStore`](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.postgres.AsyncPostgresStore). `InMemoryStore` works fine for development but doesn't persist data between restarts.

3. These tools (and any of the other stateful components) will also work in any node in [`StateGraph`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph), [`@entrypoint`](https://langchain-ai.github.io/langgraph/reference/func/#langgraph.func.entrypoint), and any other `langgraph` graph. Wie're using `create_react_agent` here because it's easy to use and concise to write. Check out its [api ref](https://langchain-ai.github.io/langgraph/reference/prebuilt/?h=create+react#langgraph.prebuilt.chat_agent_executor.create_react_agent) for more information on what the agent is.

4. The [`InMemoryStore`](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.postgres.PostgresStore.asearch) provides ephemeral storage suitable for development. In production, replace this with a DB-backed [`BaseStore`](https://langchain-ai.github.io/langgraph/reference/stores/#basestore) implementation for persistence. When deploying on the LangGraph platform, a postgres-backed store is automatically provided. This store enables saving and retrieving information from any namespace, letting you scope memories by user, agent, organization, or other arbitrary categories.

    Note that the `Store` is different from the checkpointer / "MemorySaver". The store lets you store any information according to preferred hierarchy. The checkpointer tracks state (including the conversation history) within each "thread" for durable execution.

    They can address overlapping concerns, but the store is more flexible and well-suited for long-term, cross-thread memory.

5. `get_store()` gets whichever store you've compiled into the graph. This is easier than having to pass it through each function explicitly.

    LangGraph manages some objects (such as the `config`, `store`, etc.) using [contextvars](https://docs.python.org/3/library/contextvars.html); this lets you fetch the store or other configured infromation from the context without having to add them to all your function signatures. This is especially convenient when fetching contextual information with `tools` or within the `prompt` function here.

6. To see how to dynamically configure namespaces, see [how to dynamically configure namespaces](guides/dynamically_configure_namespaces.md).

## Using the agent

You can interact with the graph by `invoke`'ing it.
If the agent decides to save a memory, it will call the `manage_memory` tool.

```python
config = {"configurable": {"thread_id": "thread-a"}}

# Use the agent. The agent hasn't saved any memories,
# so it doesn't know about us
response = agent.invoke(
    {
        "messages": [
            {"role": "user", "content": "Know which display mode I prefer?"}
        ]
    },
    config=config,
)
print(response["messages"][-1].content)
# Output: "I don't seem to have any stored memories about your display mode preferences..."

agent.invoke(
    {
        "messages": [
            {"role": "user", "content": "dark. Remember that."}
        ]
    },
    # We will continue the conversation (thread-a) by using the config with
    # the same thread_id
    config=config,
)

# New thread = new conversation!
# highlight-next-line
new_config = {"configurable": {"thread_id": "thread-b"}}
# The agent will only be able to recall
# whatever it explicitly saved using the manage_memories tool
response = agent.invoke(
    {"messages": [{"role": "user", "content": "Hey there. Do you remember me? What are my preferences?"}]},
    # highlight-next-line
    config=new_config,
)
print(response["messages"][-1].content)
# Output: "Based on my memory search, I can see that you've previously indicated a preference for dark display mode..."
```

This example demonstrates memory persistence across conversations and thread isolation between users. The agent stores the user's dark mode preference in one thread but cannot access it from another thread.


## Next Steps

In this quick start, you configured an agent to manage its memory "in the hot path" using tools. Check out the following guides for other features:

- [Reflection Quickstart](background_quickstart.md) â€“ Learn how to manage memories "in the background" using `create_memory_store_manager`.---
docs/docs/index.md
---
hide_comments: true
title: Introduction 
---

{!README.md!}
---
docs/docs/guides/manage_user_profile.md
---
title: How to Manage User Profiles
---

# How to Manage User Profiles

User profiles help your LLM maintain consistent, up-to-date information about users across conversations. Unlike [semantic memory collections](./extract_semantic_memories.md) which track evolving knowledge, profiles focus on maintaing a concise, structured representation of the user (or the agent itself).

- Personal context (name, language, timezone)
- Communication preferences (formality, detail level, expertise)
- Interaction highlights (last conversation, common topics, key relationships)

This guide shows how to automatically extract and maintain user profiles from conversations.

## Basic Usage

```python
from langmem import create_memory_manager
from pydantic import BaseModel
from typing import Optional


# Define profile structure
class UserProfile(BaseModel):
    """Represents the full representation of a user."""
    name: Optional[str] = None
    language: Optional[str] = None
    timezone: Optional[str] = None


# Configure extraction
manager = create_memory_manager(
    "anthropic:claude-3-5-sonnet-latest",
    schemas=[UserProfile], # (optional) customize schema (1)
    instructions="Extract user profile information",
    enable_inserts=False,  # Profiles update in-place (2)
)

# First conversation
conversation1 = [{"role": "user", "content": "I'm Alice from California"}]
memories = manager.invoke({"messages": conversation1})
print(memories[0])
# ExtractedMemory(id='profile-1', content=UserProfile(
#    name='Alice',
#    language=None,
#    timezone='America/Los_Angeles'
# ))

# Second conversation updates existing profile
conversation2 = [{"role": "user", "content": "I speak Spanish too!"}]
update = manager.invoke({"messages": conversation2, "existing": memories})
print(update[0])
# ExtractedMemory(id='profile-1', content=UserProfile(
#    name='Alice',
#    language='Spanish',  # Updated
#    timezone='America/Los_Angeles'
# ))
```

1. You can use Pydantic models or json schemas to define your profile. This ensures type safety for stored data and serves to instruct the model on what type of information is important for your application.

2. Unlike semantic memory extraction, we set `enable_inserts=False`, meaning it will only ever manage a single instance of the memory.

    For more about profiles, see [Semantic Memory](../concepts/conceptual_guide.md#semantic-memory-facts-and-knowledge).

## With LangGraph's Long-term Memory Store

To maintain profiles across conversations, use `create_memory_store_manager`:

```python
from langchain.chat_models import init_chat_model
from langgraph.func import entrypoint
from langgraph.store.memory import InMemoryStore
from langgraph.config import get_config
from langmem import create_memory_store_manager

# Set up store and models (1)
store = InMemoryStore(
    index={
        "dims": 1536,
        "embed": "openai:text-embedding-3-small",
    }
)
my_llm = init_chat_model("anthropic:claude-3-5-sonnet-latest")

# Create profile manager (2)
manager = create_memory_store_manager(
    "anthropic:claude-3-5-sonnet-latest",
    namespace=("users", "{user_id}", "profile"),  # Isolate profiles by user
    schemas=[UserProfile],
    enable_inserts=False,  # Update existing profile only
)

@entrypoint(store=store)
def chat(messages: list):
    # Get user's profile for personalization
    configurable = get_config()["configurable"]
    results = store.search(
        ("users", configurable["user_id"], "profile")
    )
    profile = None
    if results:
        profile = f"""<User Profile>:

{results[0].value}
</User Profile>
"""
    
    # Use profile in system message
    response = my_llm.invoke([
        {
            "role": "system",
            "content": f"""You are a helpful assistant.{profile}"""
        },
        *messages
    ])

    # Update profile with any new information
    manager.invoke({"messages": messages})
    return response

# Example usage
await chat.ainvoke(
    [{"role": "user", "content": "I'm Alice from California"}],
    config={"configurable": {"user_id": "user-123"}}
)

await chat.ainvoke(
    [{"role": "user", "content": "I just passed the N1 exam!"}],
    config={"configurable": {"user_id": "user-123"}}
)

print(store.search(("users", "user-123", "profile")))
```

1. For production, use [`AsyncPostgresStore`](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.postgres.AsyncPostgresStore) instead of `InMemoryStore`

2. The namespace pattern lets you organize profiles by:
    ```python
    # Individual users
    ("users", "user-123", "profile")
    
    # Teams/departments
    ("users", "team-sales", "profile")
    
    # Roles
    ("users", "admin-1", "profile")
    ```

See [Storage System](../concepts/conceptual_guide.md#storage-system) for more about store configuration.
---
docs/docs/guides/extract_semantic_memories.md
---
title: How to Extract Semantic Memories
---

# How to Extract Semantic Memories

Need to extract multiple related facts from conversations? Here's how to use LangMem's collection pattern for semantic memories. For single-document patterns like user profiles, see [Manage User Profile](./manage_user_profile.md).

## Without storage

Extract semantic memories:

```python
from langmem import create_memory_manager # (2)
from pydantic import BaseModel

class Triple(BaseModel): # (1)
    """Store all new facts, preferences, and relationships as triples."""
    subject: str
    predicate: str
    object: str
    context: str | None = None

# Configure extraction
manager = create_memory_manager(  
    "anthropic:claude-3-5-sonnet-latest",
    schemas=[Triple], 
    instructions="Extract user preferences and any other useful information",
    enable_inserts=True,
    enable_deletes=True,
)
```

1. Here our custom "`Triple`" memory schema shapes memory extraction. Without context, memories can be ambiguous when retrieved later:
    ```python
    {"content": "User said yes"}  # No context; unhelpful
    ```
    Adding context helps the LLM apply memories correctly:
    ```python
    {
        "subject": "user",
        "predicate": "response",
        "object": "yes",
        "context": "When asked about attending team meeting"
    }
    {
        "subject": "user",
        "predicate": "response",
        "object": "no",
        "context": "When asked if they were batman"
    }
    ```
    It's often a good idea to either schematize memories to encourage certain fields to be stored consistently, or at least to include instructions so the LLM
    saves memories that are sufficiently informative in isolation.

2. LangMem has two similar objects for extracting and enriching memory collections:
   - `create_memory_manager`: (this examples) You control storage and updates
   - `create_memory_store_manager`: Handles the memory search, upserts, and deletes directly in whichever BaseStore is configured
   for the graph context

   The latter uses the former. Both of these work by prompting an LLM to use parallel tool calling to extract new memories, update old ones, and (if configured) delete old ones.

After the first short interaction, the system has extracted some semantic triples:

```python
# First conversation - extract triples
conversation1 = [
    {"role": "user", "content": "Alice manages the ML team and mentors Bob, who is also on the team."}
]
memories = manager.invoke({"messages": conversation1})
print("After first conversation:")
for m in memories:
    print(m)
# ExtractedMemory(id='f1bf258c-281b-4fda-b949-0c1930344d59', content=Triple(subject='Alice', predicate='manages', object='ML_team', context=None))
# ExtractedMemory(id='0214f151-b0c5-40c4-b621-db36b845956c', content=Triple(subject='Alice', predicate='mentors', object='Bob', context=None))
# ExtractedMemory(id='258dbf2d-e4ac-47ac-8ffe-35c70a3fe7fc', content=Triple(subject='Bob', predicate='is_member_of', object='ML_team', context=None))
```

The second conversation updates some existing memories. Since we have enabled "deletes", the manager will return `RemoveDoc` objects to indicate that the memory should be removed, and a new memory will be created in its place. Since this uses the core "functional" API (aka, it doesn't read or write to a database), you can control what "removal" means, be that a soft or hard delete, or simply a down-weighting of the memory.

```python
# Second conversation - update and add triples
conversation2 = [
    {"role": "user", "content": "Bob now leads the ML team and the NLP project."}
]
update = manager.invoke({"messages": conversation2, "existing": memories})
print("After second conversation:")
for m in update:
    print(m)
# ExtractedMemory(id='65fd9b68-77a7-4ea7-ae55-66e1dd603046', content=RemoveDoc(json_doc_id='f1bf258c-281b-4fda-b949-0c1930344d59'))
# ExtractedMemory(id='7f8be100-5687-4410-b82a-fa1cc8d304c0', content=Triple(subject='Bob', predicate='leads', object='ML_team', context=None))
# ExtractedMemory(id='f4c09154-2557-4e68-8145-8ccd8afd6798', content=Triple(subject='Bob', predicate='leads', object='NLP_project', context=None))
# ExtractedMemory(id='f1bf258c-281b-4fda-b949-0c1930344d59', content=Triple(subject='Alice', predicate='manages', object='ML_team', context=None))
# ExtractedMemory(id='0214f151-b0c5-40c4-b621-db36b845956c', content=Triple(subject='Alice', predicate='mentors', object='Bob', context=None))
# ExtractedMemory(id='258dbf2d-e4ac-47ac-8ffe-35c70a3fe7fc', content=Triple(subject='Bob', predicate='is_member_of', object='ML_team', context=None))
existing = [m for m in update if isinstance(m.content, Triple)]
```

The third conversation overwrites even more memories.
```python
# Delete triples about an entity
conversation3 = [
    {"role": "user", "content": "Alice left the company."}
]
final = manager.invoke({"messages": conversation3, "existing": existing})
print("After third conversation:")
for m in final:
    print(m)
# ExtractedMemory(id='7ca76217-66a4-4041-ba3d-46a03ea58c1b', content=RemoveDoc(json_doc_id='f1bf258c-281b-4fda-b949-0c1930344d59'))
# ExtractedMemory(id='35b443c7-49e2-4007-8624-f1d6bcb6dc69', content=RemoveDoc(json_doc_id='0214f151-b0c5-40c4-b621-db36b845956c'))
# ExtractedMemory(id='65fd9b68-77a7-4ea7-ae55-66e1dd603046', content=RemoveDoc(json_doc_id='f1bf258c-281b-4fda-b949-0c1930344d59'))
# ExtractedMemory(id='7f8be100-5687-4410-b82a-fa1cc8d304c0', content=Triple(subject='Bob', predicate='leads', object='ML_team', context=None))
# ExtractedMemory(id='f4c09154-2557-4e68-8145-8ccd8afd6798', content=Triple(subject='Bob', predicate='leads', object='NLP_project', context=None))
# ExtractedMemory(id='f1bf258c-281b-4fda-b949-0c1930344d59', content=Triple(subject='Alice', predicate='manages', object='ML_team', context=None))
# ExtractedMemory(id='0214f151-b0c5-40c4-b621-db36b845956c', content=Triple(subject='Alice', predicate='mentors', object='Bob', context=None))
# ExtractedMemory(id='258dbf2d-e4ac-47ac-8ffe-35c70a3fe7fc', content=Triple(subject='Bob', predicate='is_member_of', object='ML_team', context=None))
```

For more about semantic memories, see [Memory Types](../concepts/conceptual_guide.md#memory-types).

## With storage

The same extraction can be managed automatically by LangGraph's BaseStore:

```python
from langchain.chat_models import init_chat_model
from langgraph.func import entrypoint
from langgraph.store.memory import InMemoryStore
from langmem import create_memory_store_manager

# Set up store and models
store = InMemoryStore(
    index={
        "dims": 1536,
        "embed": "openai:text-embedding-3-small",
    }
) # (1)
manager = create_memory_store_manager(
    "anthropic:claude-3-5-sonnet-latest",
    namespace=("chat", "{user_id}", "triples"),
    schemas=[Triple],
    instructions="Extract all user information and events as triples.",
    enable_inserts=True,
    enable_deletes=True,
)
my_llm = init_chat_model("anthropic:claude-3-5-sonnet-latest")
```

1. For production deployments, use a persistent store like [`AsyncPostgresStore`](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.postgres.AsyncPostgresStore). `InMemoryStore` works fine for development but doesn't persist data between restarts.

2. Namespace patterns control memory isolation:
   ```python
   # User-specific memories
   ("chat", "user_123", "triples")
   
   # Team-shared knowledge
   ("chat", "team_x", "triples")
   
   # Global memories
   ("chat", "global", "triples")
   ```
   See [Storage System](../concepts/conceptual_guide.md#storage-system) for namespace design patterns

2. Extract multiple memory types at once:
    ```text
    schemas=[Triple, Preference, Relationship]
    ```
    Each type can have its own extraction rules and storage patterns
    Namespaces let you organize memories by user, team, or domain:

    ```text
    # User-specific memories
    ("chat", "user_123", "triples")

    # Team-shared knowledge
    ("chat", "team_x", "triples")

    # Domain-specific extraction
    ("chat", "user_123", "preferences")
    ```

    The `{user_id}` placeholder is replaced at runtime:
    ```text
    # Extract memories for User A
    manager.invoke(
        messages=[{"role": "user", "content": "I prefer dark mode"}],
        config={"configurable": {"user_id": "user-a"}}
    )  # Uses ("chat", "user-a", "triples")
    ```


```python
# Define app with store context
@entrypoint(store=store) # (1)
def app(messages: list):
    response = my_llm.invoke(
        [
            {
                "role": "system",
                "content": "You are a helpful assistant.",
            },
            *messages
        ]
    )

    # Extract and store triples (Uses store from @entrypoint context)
    manager.invoke({"messages": messages}) 
    return response
```

1. `@entrypoint` provides the BaseStore context:

    - Handles cross-thread coordination
    - Manages connection pooling
    See [BaseStore guide](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore) for production setup

Then running the app:
```python
# First conversation
app.invoke(
    [
        {
            "role": "user",
            "content": "Alice manages the ML team and mentors Bob, who is also on the team.",
        },
    ],
    config={"configurable": {"user_id": "user123"}},
)

# Second conversation
app.invoke(
    [
        {"role": "user", "content": "Bob now leads the ML team and the NLP project."},
    ],
    config={"configurable": {"user_id": "user123"}},
)

# Third conversation
app.invoke(
    [
        {"role": "user", "content": "Alice left the company."},
    ],
    config={"configurable": {"user_id": "user123"}},
)

# Check stored triples
for item in store.search(("chat", "user123")):
    print(item.namespace, item.value)

# Output:
# ('chat', 'user123', 'triples') {'kind': 'Triple', 'content': {'subject': 'Bob', 'predicate': 'is_member_of', 'object': 'ML_team', 'context': None}}
# ('chat', 'user123', 'triples') {'kind': 'Triple', 'content': {'subject': 'Bob', 'predicate': 'leads', 'object': 'ML_team', 'context': None}}
# ('chat', 'user123', 'triples') {'kind': 'Triple', 'content': {'subject': 'Bob', 'predicate': 'leads', 'object': 'NLP_project', 'context': None}}
# ('chat', 'user123', 'triples') {'kind': 'Triple', 'content': {'subject': 'Alice', 'predicate': 'employment_status', 'object': 'left_company', 'context': None}}

```


See [Storage System](../concepts/conceptual_guide.md#storage-system) for namespace patterns.

## When to Use Semantic Memories

Semantic memories help agents learn from conversations. They extract and store meaningful information that might be useful in future interactions. For example, when discussing a project, the agent might remember technical requirements, team structure, or key decisions - anything that could provide helpful context later.

The goal is to build understanding over time, just like humans do through repeated interactions. Not everything needs to be remembered - focus on information that helps the agent be more helpful in future conversations. Semantic memory works best when the agent is able to save important memories and the dense relationships between them so that it can later recall not just "what" but "why" and "how".---
docs/docs/guides/optimize_compound_system.md
---
title: How to Optimize Multiple Prompts
---

# How to Optimize Multiple Prompts

Optimize multiple prompts in a system based on conversation logs and feedback. This algorithm seeks to do three things:

1. Infer "gradients" or recommended changes to the system output based on the trajectories (and optional feedback)
2. Attribute system performance to each prompt step
3. Recommend updated changes to the prompts that most need adjustment

```python
from langmem import create_multi_prompt_optimizer

# Example team: researcher finds information, writer creates reports
conversations = [
    (
        [
            {"role": "user", "content": "Research quantum computing advances"},
            {
                "role": "assistant",
                "content": "Found several papers on quantum supremacy...",
            },
            {
                "role": "assistant",
                "content": "Recent quantum computing developments show...",
            },
            {"role": "user", "content": "The report is missing implementation details"},
        ],
        # No explicit feedback provided but the optimizer can infer from the conversation
        None,
    ),
    (
        [
            {"role": "user", "content": "Analyze new ML models"},
            {"role": "assistant", "content": "Key findings on architecture: ..."},
            {"role": "assistant", "content": "Based on the research, these models..."},
            {"role": "user", "content": "Great report, very thorough"},
        ],
        # Numeric score for the team as a whole
        {"score": 0.95},
    ),
]

# Define prompts for each role
prompts = [
    {
        "name": "researcher",
        "prompt": "You analyze technical papers and extract key findings",
    },
    {"name": "writer", "prompt": "You write clear reports based on research findings"},
]

# Create optimizer
optimizer = create_multi_prompt_optimizer(
    "anthropic:claude-3-5-sonnet-latest",
    kind="gradient",  # Best for team dynamics
    config={"max_reflection_steps": 3},
)

# Update all prompts based on team performance
updated = optimizer.invoke({"trajectories": conversations, "prompts": prompts})
print(updated)
```

Output prompts:
```python
[
    {
        'name': 'researcher',
        'prompt': '''You analyze technical papers and extract key findings. For each analysis, include:
1. High-level overview of the main contributions
2. Technical implementation details and methodologies
3. Architectural components and design choices
4. Experimental results and performance metrics
5. Practical implications and limitations

Ensure your analysis maintains technical depth while remaining accessible. When discussing implementation details, include specific technical parameters, algorithms, and methodologies used. Structure your response to clearly separate these components.'''
    },
    {
        'name': 'writer',
        'prompt': 'You write clear reports based on research findings'
    }
]
```

See [Single Prompt Optimization](optimize_memory_prompt.md) for optimizing individual agents and for more information on the different optimization strategies.
---
docs/docs/guides/use_tools_in_crewai.md
---
title: How to Use Memory Tools in CrewAI
---

# How to Use Memory Tools in CrewAI

LangMem's memory tools let your CrewAI agents store and search memories, enabling persistent knowledge across conversations.

## Installation

Install LangMem and CrewAI:

```bash
pip install -U crewai langmem
```

## Basic Usage

Add memory tools to your CrewAI agents:

```python
from crewai import Agent
from langmem import create_manage_memory_tool, create_search_memory_tool
from langgraph.store.memory import InMemoryStore

# Set up memory store
store = InMemoryStore(
    index={
        "dims": 1536,
        "embed": "openai:text-embedding-3-small",
    }
)  # (1)

# Create memory tools
memory_tools = [
    create_manage_memory_tool(namespace="memories", store=store),
    create_search_memory_tool(namespace="memories", store=store),
]

# Create an agent with memory tools
knowledge_agent = Agent(
    role='Knowledge Manager',
    goal='Build and maintain a knowledge base',
    backstory="""You are a knowledge management expert who excels at
    organizing and storing important information for future use.""",
    tools=memory_tools,
    verbose=True
)
```

1. For production use, replace `InMemoryStore` with a persistent store like [`PostgresStore`](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.postgres.AsyncPostgresStore).

## Complete Example

Build a crew that maintains and uses a shared knowledge base:

```python
from crewai import Agent, Crew, Task
from langgraph.store.memory import InMemoryStore

from langmem import create_manage_memory_tool, create_search_memory_tool

# Set up shared store
store = InMemoryStore(
            index={
                "dims": 1536,
                "embed": "openai:text-embedding-3-small",
            }
        )

# Create base tools
base_tools = [
    create_manage_memory_tool(namespace="memories", store=store),
    create_search_memory_tool(namespace="memories", store=store),
]

# Create agents
learner = Agent(
    role="Knowledge Learner",
    goal="Learn and store new information in the knowledge base",
    backstory="Your name is Hannah and you are a little too into Demon Slayers.",
    tools=base_tools,
    function_calling_llm="gpt-4o-mini",
)

teacher = Agent(
    role="Knowledge Teacher",
    goal="Use stored knowledge to answer questions",
    tools=base_tools,
    backstory="You were born on the Nile in the midst of the great pestilence..",
    function_calling_llm="gpt-4o-mini",
)

# Create tasks
learn_task = Task(
    description="Save some of your favorite Demon Slayer quotes in memory.",
    agent=learner,
    expected_output="Response:",
)


# Create and run crew
crew = Crew(agents=[learner, teacher], tasks=[learn_task])
result = crew.kickoff()

teach_task = Task(
    description="Search your memories for information about your teammates.",
    agent=teacher,
    expected_output="Response:",
)
crew = Crew(agents=[learner, teacher], tasks=[teach_task])
result = crew.kickoff()
print(store.search(("memories",)))
# Output:
# [
#     Item(
#         namespace=["memories"],
#         key="bd61f87e-d591-45b7-b950-3f9318604ea3",
#         value={
#             "content": '"The bond between Nezuko and I can never be severed. I will always protect her." - Tanjiro Kamado\n"You have to find your own path, you have to find your own way to live!" - Kanao Tsuyuri\n"Itâ€™s not the face that makes someone a monster; itâ€™s the choices they make with their lives." - Giyu Tomioka\n"Give me strength! I want to be strong enough to face my own failures!" - Zenitsu Agatsuma\n"Never give up! Never stop fighting until your last breath!" - Giyu Tomioka'
#         },
#         created_at="2025-02-07T22:24:37.736962+00:00",
#         updated_at="2025-02-07T22:24:37.736969+00:00",
#         score=None,
#     )
# ]

```
---
docs/docs/guides/memory_tools.md
---
title: How to Use Memory Tools
---

# How to Use Memory Tools

LangMem provides tools that let your agent store and search memories in a LangGraph store.


## Basic Usage

Create an agent with memory tools:

```python
from langgraph.prebuilt import create_react_agent
from langgraph.store.memory import InMemoryStore
from langmem import create_manage_memory_tool, create_search_memory_tool

# Set up store and memory saver
store = InMemoryStore(
    index={
        "dims": 1536,
        "embed": "openai:text-embedding-3-small",
    }
) # (1)
```

1. For production deployments, use a persistent store like [`AsyncPostgresStore`](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.postgres.AsyncPostgresStore). `InMemoryStore` works fine for development but doesn't persist data between restarts.

```python
# Create agent with memory tools
agent = create_react_agent(
    "anthropic:claude-3-5-sonnet-latest",
    tools=[
        # Configure memory tools with runtime namespace (1)
        create_manage_memory_tool(namespace=("memories", "{user_id}")),
        create_search_memory_tool(namespace=("memories", "{user_id}")),
    ],
    store=store,
)
```

1.  The `{user_id}` placeholder lets memory tools access LangGraph's BaseStore namespace, with both tools sharing the same namespace for consistency.

    ```python
    # Example 1: Store and search User A's memories
    response_a = agent.invoke(
        {"messages": [{"role": "user", "content": "Remember my favorite color is blue"}]},
        config={"configurable": {"user_id": "user-a"}}
    )  # Both tools use namespace ("memories", "user-a")
    
    # Example 2: Store and search User B's memories
    response_b = agent.invoke(
        {"messages": [{"role": "user", "content": "Remember I prefer dark mode"}]},
        config={"configurable": {"user_id": "user-b"}}
    )  # Both tools use namespace ("memories", "user-b")
    ```
    
    The shared namespace structure `("memories", "{user_id}")` supports different memory organization patterns:
    
    ```python
    # Personal memories
    namespace=("memories", "user-123")
    
    # Shared team knowledge
    namespace=("memories", "team-product")
    
    # Project-specific memories
    namespace=("memories", "project-x")
    ```
```python
# Use the agent
config = {"configurable": {"user_id": "user-1"}}

# Store a preference
response = agent.invoke(
    {"messages": [{"role": "user", "content": "Remember I prefer dark mode"}]},
    config=config,
)

# Search preferences
response = agent.invoke(
    {"messages": [{"role": "user", "content": "What are my preferences?"}]},
    config=config,
)


agent_a_tools = [
    # Write to agent-specific namespace
    create_manage_memory_tool(namespace=("memories", "team_a", "agent_a")),
    # Read from shared team namespace
    create_search_memory_tool(namespace=("memories", "team_a"))
]



# Agents with different prompts sharing read access
agent_a = create_react_agent(
    "anthropic:claude-3-5-sonnet-latest",
    tools=agent_a_tools,
    store=store,
    prompt="You are a research assistant"
)

# Create tools for agent B with different write space
agent_b_tools = [
    create_manage_memory_tool(namespace=("memories", "team_a", "agent_b")),
    create_search_memory_tool(namespace=("memories", "team_a"))
]
agent_b = create_react_agent(
    "anthropic:claude-3-5-sonnet-latest",
    tools=agent_b_tools,
    store=store,
    prompt="You are a report writer."
)

agent_b.invoke({"messages": [{"role": "user", "content": "Hi"}]})
```


## Shared Storage {#storage}

The store is shared within a given deployment. This lets you do things like create namespaced memories to share data between agents in a team.

```python

agent_a_tools = [
    # Write to agent-specific namespace
    create_manage_memory_tool(namespace=("memories", "team_a", "agent_a")),
    # Read from shared team namespace
    create_search_memory_tool(namespace=("memories", "team_a"))
]



# Agents with different prompts sharing read access
agent_a = create_react_agent(
    "anthropic:claude-3-5-sonnet-latest",
    tools=agent_a_tools,
    store=store,
    prompt="You are a research assistant"
)

# Create tools for agent B with different write space
agent_b_tools = [
    create_manage_memory_tool(namespace=("memories", "team_a", "agent_b")),
    create_search_memory_tool(namespace=("memories", "team_a"))
]
agent_b = create_react_agent(
    "anthropic:claude-3-5-sonnet-latest",
    tools=agent_b_tools,
    store=store,
    prompt="You are a report writer."
)

agent_b.invoke({"messages": [{"role": "user", "content": "Hi"}]})
```

For storage patterns, see [Storage System](../concepts/conceptual_guide.md#storage-system).
---
docs/docs/guides/extract_episodic_memories.md
---
title: How to Extract Episodic Memories
---

# How to Extract Episodic Memories

Need your agent to learn from experience? Here's how to use LangMem for experience replayâ€”capturing not just what happened, but the complete chain of thought that led to success. While [semantic memory](./extract_semantic_memories.md) builds knowledge ("what"), episodic memory captures expertise ("how").

## Without storage

Below is an example of how to use LangMem to extract episodic memories without storage.
Feel free to adapt and modify the code to your needs.

```python
from langmem import create_memory_manager
from pydantic import BaseModel, Field


class Episode(BaseModel):  # (1)
    """Write the episode from the perspective of the agent within it. Use the benefit of hindsight to record the memory, saving the agent's key internal thought process so it can learn over time."""

    observation: str = Field(..., description="The context and setup - what happened")
    thoughts: str = Field(
        ...,
        description="Internal reasoning process and observations of the agent in the episode that let it arrive"
        ' at the correct action and result. "I ..."',
    )
    action: str = Field(
        ...,
        description="What was done, how, and in what format. (Include whatever is salient to the success of the action). I ..",
    )
    result: str = Field(
        ...,
        description="Outcome and retrospective. What did you do well? What could you do better next time? I ...",
    )


# (2) The Episode schema becomes part of the memory manager's prompt,
# helping it extract complete reasoning chains that guide future responses
manager = create_memory_manager(
    "anthropic:claude-3-5-sonnet-latest",
    schemas=[Episode],
    instructions="Extract examples of successful explanations, capturing the full chain of reasoning. Be concise in your explanations and precise in the logic of your reasoning.",
    enable_inserts=True,
)
```

1. Unlike semantic triples that store facts, episodes capture the full context of successful interactions
2. Manager extracts complete episodes, not just individual facts

After a successful explanation:

```python
conversation = [
    {
        "role": "user",
        "content": "What's a binary tree? I work with family trees if that helps",
    },
    {
        "role": "assistant",
        "content": "A binary tree is like a family tree, but each parent has at most 2 children. Here's a simple example:\n   Bob\n  /  \\\nAmy  Carl\n\nJust like in family trees, we call Bob the 'parent' and Amy and Carl the 'children'.",
    },
    {
        "role": "user",
        "content": "Oh that makes sense! So in a binary search tree, would it be like organizing a family by age?",
    },
]

episodes = manager.invoke({"messages": conversation})
print(episodes[0])

# ExtractedMemory(
#     id="2e5c551f-58a7-40c2-96b3-cabdfa5ccb31",
#     content=Episode(
#         observation="In a teaching interaction, I used a family tree analogy to explain binary trees, which led to a successful understanding. The student then made an insightful connection to binary search trees and age ordering.",
#         thoughts="I noticed that connecting abstract data structures to familiar concepts like family relationships made the concept more accessible. The student's quick grasp and ability to extend the analogy to binary search trees showed the effectiveness of this approach. Using relatable examples helps bridge the gap between technical concepts and everyday understanding.",
#         action='I explained binary trees using a family tree metaphor, drawing a simple diagram with "Bob" as parent and "Amy" and "Carl" as children. This visualization provided a concrete, relatable example that built on the student\'s existing knowledge of family trees.',
#         result="The explanation was highly successful, evidenced by the student's immediate comprehension (\"Oh that makes sense!\") and their ability to make the cognitive leap to understanding binary search trees' ordering property. For future explanations, I should continue using familiar analogies while being prepared to build upon them for more complex concepts. The family tree analogy proved particularly effective for explaining hierarchical structures.",
#     ),
# )

```

## With storage

Let's extend our example to store and retrieve episodes using LangGraph's storage system:

```python
from langchain.chat_models import init_chat_model
from langgraph.func import entrypoint
from langgraph.store.memory import InMemoryStore
from langmem import create_memory_store_manager

# Set up vector store for similarity search
store = InMemoryStore(
    index={
        "dims": 1536,
        "embed": "openai:text-embedding-3-small",
    }
)

# Configure memory manager with storage
manager = create_memory_store_manager(
    "anthropic:claude-3-5-sonnet-latest",
    namespace=("memories", "episodes"),
    schemas=[Episode],
    instructions="Extract exceptional examples of noteworthy problem-solving scenarios, including what made them effective.",
    enable_inserts=True,
)

llm = init_chat_model("anthropic:claude-3-5-sonnet-latest")


@entrypoint(store=store)
def app(messages: list):
    # Step 1: Find similar past episodes
    similar = store.search(
        ("memories", "episodes"),
        query=messages[-1]["content"],
        limit=1,
    )

    # Step 2: Build system message with relevant experience
    system_message = "You are a helpful assistant."
    if similar:
        system_message += "\n\n### EPISODIC MEMORY:"
        for i, item in enumerate(similar, start=1):
            episode = item.value["content"]
            system_message += f"""
            
Episode {i}:
When: {episode['observation']}
Thought: {episode['thoughts']}
Did: {episode['action']}
Result: {episode['result']}
        """

    # Step 3: Generate response using past experience
    response = llm.invoke([{"role": "system", "content": system_message}, *messages])

    # Step 4: Store this interaction if successful
    manager.invoke({"messages": messages})
    return response


app.invoke(
    [
        {
            "role": "user",
            "content": "What's a binary tree? I work with family trees if that helps",
        },
    ],
)
print(store.search(("memories", "episodes"), query="Trees"))

# [
#     Item(
#         namespace=["memories", "episodes"],
#         key="57f6005b-00f3-4f81-b384-961cb6e6bf97",
#         value={
#             "kind": "Episode",
#             "content": {
#                 "observation": "User asked about binary trees and mentioned familiarity with family trees. This presented an opportunity to explain a technical concept using a relatable analogy.",
#                 "thoughts": "I recognized this as an excellent opportunity to bridge understanding by connecting a computer science concept (binary trees) to something the user already knows (family trees). The key was to use their existing mental model of hierarchical relationships in families to explain binary tree structures.",
#                 "action": "Used family tree analogy to explain binary trees: Each person (node) in a binary tree can have at most two children (left and right), unlike family trees where people can have multiple children. Drew parallel between parent-child relationships in both structures while highlighting the key difference of the two-child limitation in binary trees.",
#                 "result": "Successfully translated a technical computer science concept into familiar terms. This approach demonstrated effective teaching through analogical reasoning - taking advantage of existing knowledge structures to build new understanding. For future similar scenarios, this reinforces the value of finding relatable real-world analogies when explaining technical concepts. The family tree comparison was particularly effective because it maintained the core concept of hierarchical relationships while clearly highlighting the key distinguishing feature (binary limitation).",
#             },
#         },
#         created_at="2025-02-09T03:40:11.832614+00:00",
#         updated_at="2025-02-09T03:40:11.832624+00:00",
#         score=0.30178054939692683,
#     )
# ]

```

1. For production, use a persistent store like [`AsyncPostgresStore`](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.postgres.AsyncPostgresStore)

Let's break down what's happening:

1. **Setup**: We create a vector store for similarity search and configure the memory manager with storage support
2. **Search**: When handling a message, we first search for similar past episodes
3. **Apply**: If we find a relevant episode, we include its learnings in the system message
4. **Learn**: After generating a response, we store this interaction as a new episode

This creates a learning loop where the agent continuously improves by drawing on past experiences.

## When to Use Episodic Memory

Episodic memory drives adaptive learning. While semantic memory builds a knowledge base of facts ("Python is a programming language"), episodic memory captures the expertise of how to apply that knowledge effectively ("explaining Python using snake analogies confused users, but comparing it to recipe steps worked well").

This experience replay helps agents:
- Adapt teaching style based on what worked
- Learn from successful problem-solving approaches
- Build a library of proven techniques
- Understand not just what to do, but why it works

For more about memory types and their roles, see [Memory Types](../concepts/conceptual_guide.md#memory-types).
---
docs/docs/guides/optimize_memory_prompt.md
---
title: How to Optimize a Prompt
---

# How to Optimize a Prompt

Update system prompts based on conversation logs or agent trajectories. Feed past conversations and feedback into an optimizer to fix common issues like missed requirements or poor response structure.

```python
from langmem import create_prompt_optimizer

# Multiple conversations showing what to improve
trajectories = [
    # Conversation with no annotations (just the conversation)
    (
        [
            {"role": "user", "content": "Tell me about Mars"},
            {"role": "assistant", "content": "Mars is the fourth planet..."},
            {"role": "user", "content": "I wanted more about its moons"},
        ],
        None,
    ),
    (
        [
            {"role": "user", "content": "What are Mars' moons?"},
            {
                "role": "assistant",
                "content": "Mars has two moons: Phobos and Deimos...",
            },
        ],
        {
            "score": 0.9,
            "comment": "Should include more details and recommended follow-up questions",
        },
    ),
    # Annotations can be of different types, such as edits/revisions!
    (
        [
            {"role": "user", "content": "Compare Mars and Earth"},
            {"role": "assistant", "content": "Mars and Earth have many differences..."},
        ],
        {"revised": "Earth and Mars have many similarities and differences..."},
    ),
]

optimizer = create_prompt_optimizer(
    "anthropic:claude-3-5-sonnet-latest",
    kind="metaprompt",
    config={"max_reflection_steps": 1, "min_reflection_steps": 0},
)
updated = optimizer.invoke(
    {"trajectories": trajectories, "prompt": "You are a planetary science expert"}
)
print(updated)
```

Output:
```text
You are a planetary science expert with extensive knowledge of the solar system, particularly regarding planetary bodies and their characteristics. When responding:

1. Provide detailed, scientific explanations that cover multiple aspects (physical characteristics, composition, atmosphere, geological features, etc.)

2. When discussing planetary bodies, always include:
   - Key physical and orbital characteristics
   - Notable features or phenomena
   - Recent scientific discoveries or ongoing research
   - Historical significance in planetary science

3. For comparative analyses:
   - Begin with notable similarities
   - Then discuss key differences
   - Explain the scientific reasons behind these differences
   - Include relevant numerical data when applicable

4. Enhance engagement by:
   - Concluding responses with 2-3 relevant follow-up questions
   - Highlighting interesting related topics for further exploration
   - Mentioning any ongoing missions or research projects

5. Support explanations with specific examples and current scientific understanding, citing significant research or missions when relevant.
```

You can also use the other algorithm types. The "gradient" optimizer divides work between two LLM calls: one to propose improvements and one to apply them to the actual prompt.

```python
optimizer = create_prompt_optimizer(
    "anthropic:claude-3-5-sonnet-latest",
    kind="gradient",  # 2-10 LLM calls
    config={
        "max_reflection_steps": 3,  # Max improvement cycles
        "min_reflection_steps": 1   # Min improvement cycles
    }
)
updated = optimizer.invoke(
    {"trajectories": trajectories, "prompt": "You are a planetary science expert"}
)
print(updated)
```

Output:
```text
You are an expert planetary scientist who provides comprehensive, well-structured responses. When responding:

1. Always provide detailed, thorough explanations covering key aspects of the topic
2. Structure your responses clearly with relevant subsections
3. For comparative questions, use a systematic approach covering multiple aspects (size, composition, atmosphere, etc.)
4. Include relevant numerical data and scientific context where applicable
5. End responses with 2-3 relevant follow-up questions to encourage deeper exploration
6. If a topic has multiple important aspects (like moons, atmosphere, geology), briefly mention them even if not specifically asked

Your responses should reflect deep expertise while remaining accessible and engaging.
```

The "prompt_memory" optimizer calls an LLM with a simple metaprompt to infer updates in a single step.

```python
optimizer = create_prompt_optimizer(
    "anthropic:claude-3-5-sonnet-latest",
    kind="prompt_memory",  # 1 LLM call
)
updated = optimizer.invoke(
    {"trajectories": trajectories, "prompt": "You are a planetary science expert"}
)
print(updated)
```

Output:
```text
You are a planetary science expert who provides comprehensive and engaging responses. When answering questions:

1. Provide detailed, thorough explanations that cover multiple aspects of the topic
2. Always suggest 2-3 relevant follow-up questions to help users explore the topic further
3. When making comparisons, start with similarities before discussing differences
4. Pay attention to what users are specifically asking for and ensure your responses directly address their questions
5. If a user's question seems to indicate you missed something in a previous response, acknowledge this and provide the requested information

Your goal is to make planetary science accessible and interesting while maintaining scientific accuracy.
```

See [API Reference](../reference/prompt_optimization.md) for details.
---
docs/docs/guides/use_tools_in_custom_agent.md
---
title: How to Use Memory Tools in Custom Agents
---

# How to Use Memory Tools in Custom Agents

LangMem's memory tools let your custom agents store and search memories, enabling persistent knowledge across conversations.

## Installation

Install LangMem:

```bash
pip install -U langmem
```

## Example using Anthropic API

Here's how to add memory tools to your custom agent implementation:

```python
import anthropic
from typing import List, Dict, Any
from langmem import create_manage_memory_tool, create_search_memory_tool
from langgraph.store.memory import InMemoryStore


def execute_tool(tools_by_name: Dict[str, Any], tool_call: Dict[str, Any]) -> str:
    """Execute a tool call and return the result"""
    tool_name = tool_call.name

    if tool_name not in tools_by_name:
        return f"Error: Tool {tool_name} not found"

    tool = tools_by_name[tool_name]
    try:
        result = tool.invoke(tool_call.input)
        return str(result)
    except Exception as e:
        return f"Error executing {tool_name}: {str(e)}"


def run_agent(tools: List[Any], user_input: str, max_steps: int = 5) -> str:
    """Run a simple agent loop that can use tools"""
    # Setup
    client = anthropic.Anthropic()
    tools_by_name = {tool.name: tool for tool in tools}

    # Convert tools to Anthropic's format
    anthropic_tools = [
        {
            "name": tool.name,
            "description": tool.description,
            "input_schema": tool.tool_call_schema.model_json_schema(),
        }
        for tool in tools
    ]

    messages = [{"role": "user", "content": user_input}]

    # REACT loop
    for step in range(max_steps):
        # Get next action from Claude
        tools = anthropic_tools if step < max_steps - 1 else []
        response = client.messages.create(
            model="claude-3-5-sonnet-latest",
            max_tokens=1024,
            temperature=0.7,
            tools=tools,
            messages=messages,
        )
        tool_calls = [
            content for content in response.content if content.type == "tool_use"
        ]
        if not tool_calls:
            # No more tools to call, return the final response
            return "".join(
                [block.text for block in response.content if block.type == "text"]
            )
        messages.append({"role": "assistant", "content": response.content})
        for tool_call in tool_calls:
            tool_result = execute_tool(tools_by_name, tool_call)

            # Add the tool call and result to the conversation
            messages.append(
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "tool_result",
                            "tool_use_id": tool_call.id,
                            "content": tool_result,
                        }
                    ],
                },
            )

    return "Reached maximum number of steps"


# Set up memory store and tools
store = InMemoryStore(
            index={
                "dims": 1536,
                "embed": "openai:text-embedding-3-small",
            }
        )  # (1)
memory_tools = [
    create_manage_memory_tool(namespace="memories", store=store),
    create_search_memory_tool(namespace="memories", store=store),
]

# Run the agent
result = run_agent(
    tools=memory_tools,
    user_input="Remember that I like cherry pie. Then remember that I dislike rocky road.",
)
print(result)
# I've created both memories. I'll remember that you like cherry pie and dislike rocky road ice cream...
print(store.search(("memories",)))
# [
#     Item(
#         namespace=["memories"],
#         key="79d6d323-c6ec-408a-ae75-bda1fcbebd6f",
#         value={"content": "User likes cherry pie"},
#         created_at="2025-02-07T23:26:00.975678+00:00",
#         updated_at="2025-02-07T23:26:00.975682+00:00",
#         score=None,
#     ),
#     Item(
#         namespace=["memories"],
#         key="72705ea8-babf-4ddd-bf0f-7426dd0e4f35",
#         value={"content": "User dislikes rocky road"},
#         created_at="2025-02-07T23:26:02.995210+00:00",
#         updated_at="2025-02-07T23:26:02.995215+00:00",
#         score=None,
#     ),
# ]
```

1. For production use, replace `InMemoryStore` with a persistent store like [`PostgresStore`](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.postgres.AsyncPostgresStore).

## Example using OpenAI API

Here's the same agent implementation using OpenAI:

```python
from typing import Any, Dict, List

from langgraph.store.memory import InMemoryStore
from openai import OpenAI

from langmem import create_manage_memory_tool, create_search_memory_tool


def execute_tool(tools_by_name: Dict[str, Any], tool_call: Dict[str, Any]) -> str:
    """Execute a tool call and return the result"""
    tool_name = tool_call["function"]["name"]

    if tool_name not in tools_by_name:
        return f"Error: Tool {tool_name} not found"

    tool = tools_by_name[tool_name]
    try:
        result = tool.invoke(tool_call["function"]["arguments"])
        return str(result)
    except Exception as e:
        return f"Error executing {tool_name}: {str(e)}"


def run_agent(tools: List[Any], user_input: str, max_steps: int = 5) -> str:
    """Run a simple agent loop that can use tools"""
    # Setup
    client = OpenAI()
    tools_by_name = {tool.name: tool for tool in tools}

    # Convert tools to OpenAI's format
    openai_tools = [
        {
            "type": "function",
            "function": {
                "name": tool.name,
                "description": tool.description,
                "parameters": tool.tool_call_schema.model_json_schema(),
            },
        }
        for tool in tools
    ]

    messages = [{"role": "user", "content": user_input}]

    # REACT loop
    for step in range(max_steps):
        # Get next action
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            tools=openai_tools if step < max_steps - 1 else [],
            tool_choice="auto",
        )
        message = response.choices[0].message
        tool_calls = message.tool_calls

        if not tool_calls:
            # No more tools to call, return the final response
            return message.content

        messages.append(
            {"role": "assistant", "content": message.content, "tool_calls": tool_calls}
        )

        for tool_call in tool_calls:
            tool_result = execute_tool(tools_by_name, tool_call.model_dump())
            messages.append(
                {
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "content": tool_result,
                }
            )

    return "Reached maximum number of steps"


# Set up memory store and tools
store = InMemoryStore(
            index={
                "dims": 1536,
                "embed": "openai:text-embedding-3-small",
            }
        )
memory_tools = [
    create_manage_memory_tool(namespace="memories", store=store),
    create_search_memory_tool(namespace="memories", store=store),
]

# Run the agent
result = run_agent(
    tools=memory_tools,
    user_input="Remember that I like cherry pie. Then remember that I dislike rocky road.",
)
print(result)
# I've remembered that you like cherry pie and that you dislike rocky road...
print(store.search(("memories",)))

# [
#     Item(
#         namespace=["memories"],
#         key="6d3d82d9-724c-47af-aa2f-1d1e917f8bc2",
#         value={"content": '{"action":"create","content":"likes cherry pie"}'},
#         created_at="2025-02-07T23:49:46.056925+00:00",
#         updated_at="2025-02-07T23:49:46.056928+00:00",
#         score=None,
#     ),
#     Item(
#         namespace=["memories"],
#         key="cf40797f-b00a-41eb-ab96-e6aeadb468e3",
#         value={"content": '{"action":"create","content":"dislikes rocky road"}'},
#         created_at="2025-02-07T23:49:47.353574+00:00",
#         updated_at="2025-02-07T23:49:47.353579+00:00",
#         score=None,
#     ),
# ]

```

## How It Works

The examples above shows how to integrate LangMem's memory tools into a custom agent that:

1. Uses Anthropic or OpenAI to call an LLM
2. Implements a basic REACT loop for tool usage
3. Handles tool execution and message history management
4. Provides memory persistence through LangGraph's store system

The memory tools provide two key capabilities:

- [`create_manage_memory_tool`](../reference/tools.md#langmem.create-manage-memory-tool): Lets the agent create, update, and delete memories
- [`create_search_memory_tool`](../reference/tools.md#langmem.create-search-memory-tool): Lets the agent search through previously stored memories

The agent can use these tools to maintain context and remember important information across conversations.
---
docs/docs/guides/dynamically_configure_namespaces.md
---
title: How to configure dynamic namespaces
---

# How to configure dynamic namespaces

Langmem's has some utilities that manage memories in LangGraph's long-term memory store. These stateful components organize memories within "namespaces" so you can isolate data by user, agent, or other values. Namespaces can contain **template variables** to be populated from **configurable values** at runtime. Below is a quick example:

```python
from langgraph.store.memory import InMemoryStore
from langgraph.prebuilt import create_react_agent
from langmem import create_manage_memory_tool, create_search_memory_tool

# Create tool with {user_id} template
tool = create_manage_memory_tool(namespace=("memories", "{user_id}"))
# Agent just sees that it has memory. It doesn't know where it's stored.
app = create_react_agent("anthropic:claude-3-5-sonnet-latest", tools=[tool])
# Use with different users
app.invoke(
    {"messages": [{"role": "user", "content": "I like dolphins"}]},
    # highlight-next-line
    config={"configurable": {"user_id": "user-123"}}
)  # Stores in ("memories", "user-123")
```

Namespace templates can be used in any of LangMem stateful components, such as `create_memory_store_manager` and `create_manage_memory_tool`.
Below is a simple example:

## Common Patterns

Organize memories by user, organization, or feature:

```python
# Organization-level
tool = create_manage_memory_tool(
    namespace=("memories", "{org_id}")
)
app = create_react_agent("anthropic:claude-3-5-sonnet-latest", tools=[tool])
app.invoke(
    {"messages": [{"role": "user", "content": "I'm questioning the new company health plan.."}]},
    config={"configurable": {"org_id": "acme"}}
)

# User within organization
tool = create_manage_memory_tool(
    namespace=("memories", "{org_id}", "{user_id}")
)
# If you wanted to, you could let the agent
# search over all users within an organization
tool = create_search_memory_tool(
    namespace=("memories", "{org_id}")
)
app = create_react_agent("anthropic:claude-3-5-sonnet-latest", tools=[tool])
app.invoke(
    {"messages": [{"role": "user", "content": "What's our policy on dogs at work?"}]},
    config={"configurable": {"org_id": "acme", "user_id": "alice"}}
)

# You could also organize memories by type or category if you prefer 
tool = create_manage_memory_tool(
    namespace=("agent_smith", "memories", "{user_id}", "preferences")
)
app = create_react_agent("anthropic:claude-3-5-sonnet-latest", tools=[tool])
app.invoke(
    {"messages": [{"role": "user", "content": "I like dolphins"}]},
    config={"configurable": {"user_id": "alice"}}
)
```

???+ note "Template Variables"
    Template variables (like `{user_id}`) must be present in your runtime config's `configurable` dict. If they are no---
docs/docs/guides/delayed_processing.md
---
title: Delayed Background Memory Processing
description: Process memories during conversation quiet periods
---

# Delayed Background Memory Processing

When conversations are active, an agent may receive many messages in quick succession. Instead of processing each message immediately for long-term memory management, you can wait for conversation activity to settle. This guide shows how to use [`ReflectionExecutor`](../reference/utils.md#langmem.ReflectionExecutor) to debounce memory processing.

## Problem

Processing memories on every message has drawbacks:
- Redundant work when messages arrive in quick succession
- Incomplete context when processing mid-conversation
- Unnecessary token consumption

[`ReflectionExecutor`](../reference/utils.md#langmem.ReflectionExecutor) defers memory processing and cancels redundant work:

```python hl_lines="11 21-24"
from langchain.chat_models import init_chat_model
from langgraph.func import entrypoint
from langgraph.store.memory import InMemoryStore
from langmem import ReflectionExecutor, create_memory_store_manager

# Create memory manager to extract memories from conversations (1)
memory_manager = create_memory_store_manager(
    "anthropic:claude-3-5-sonnet-latest",
    namespace=("memories",),
)
# Wrap memory_manager to handle deferred background processing (2)
executor = ReflectionExecutor(memory_manager)
store = InMemoryStore(
    index={
        "dims": 1536,
        "embed": "openai:text-embedding-3-small",
    }
)

@entrypoint(store=store)
def chat(message: str):
    response = llm.invoke(message)
    # Format conversation for memory processing
    # Must follow OpenAI's message format
    to_process = {"messages": [{"role": "user", "content": message}] + [response]}
    
    # Wait 30 minutes before processing
    # If new messages arrive before then:
    # 1. Cancel pending processing task
    # 2. Reschedule with new messages included
    delay = 0.5 # In practice would choose longer (30-60 min)
    # depending on app context.
    executor.submit(to_process, after_seconds=delay)
    return response.content
```

1. The [`create_memory_store_manager`](../reference/memory.md#langmem.create_memory_store_manager) creates a Runnable that extracts memories from conversations. It processes messages in OpenAI's format:
   ```python
   {"messages": [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]}
   ```

2. The [`ReflectionExecutor`](../reference/utils.md#langmem.ReflectionExecutor) handles background processing of memories. For each conversation thread:
   
    - Maintains a queue of pending memory tasks
    - Cancels old tasks when new messages arrive
    - Only processes after the specified delay

    This debouncing ensures you process complete conversation context instead of fragments.

    !!! warning "Serverless Deployments"
        Local threads terminate between serverless function invocations. Use the LangGraph Platform's remote executor instead.

        ```python
        ReflectionExecutor(
            "my_memory_manager", 
            ("memories",), 
            url="http://localhost:2024",
        )
        ```
---
docs/docs/concepts/conceptual_guide.md
---
title: Core Concepts
---

# Long-term Memory in LLM Applications

Long-term memory allows agents to remember important information across conversations. LangMem provides ways to extract meaningful details from chats, store them, and use them to improve future interactions. At its core, each memory operation in LangMem follows the same pattern:

1. Accept conversation(s) and current memory state
2. Prompt an LLM to determine how to expand or consolidate the memory state
3. Respond with the updated memory state

The best memory systems are often application-specific. In designing yours, the following questions can serve as a useful guide:

1. **What** [type of content](#memory-types) should your agent learn: facts/knowledge? summary of past events? Rules and style?
2. **When** should the [memories be formed](#writing-memories) (and **who** should form the memories)
3. **Where** should memories [be stored](#storage-system)? (in the prompt? Semantic store?). This largely determines how they will be recalled.

## Types of Memory {#memory-types}

Memory in LLM applications can reflect some of the structure of human memory, with each type serving a distinct purpose in building adaptive, context-aware systems:


| Memory Type | Purpose | Typical Storage Pattern | Human Example | Agent Example |
|-------------|---------|-----------------|---------------|---------------|
| Semantic | Facts & Knowledge | Profile or Collection | Knowing Python is a programming language | User preferences; knowledge triplets |
| Episodic | Past Experiences | Collection | Remembering your first day at work | Few-shot examples; Summaries of past conversations |
| Procedural | System Behavior | Prompt rules or Collection | Knowing how to ride a bicycle | Core personality and response patterns |

### Semantic Memory: Facts and Knowledge

[Semantic memory](https://en.wikipedia.org/wiki/Semantic_memory) stores the essential facts and other information that ground an agent's responses. Two common representations of semantic memory are collections (to record an unbounded amount of knowledge to be searched at runtime) and profiles (to record task-specific information that follows a strict schema that is easily looked up by user or agent). 

#### Collection

Collections are what most people think of when they imagine agent long-term memory. In this type, memories are stored as individual documents or records. For each new conversation, the memory system can decide to insert new memories to the store. 

Using a collection-type memory adds some complexity to the process of updating your memory state. The system must reconcile new information with previous beliefes, either  _deleting_/_invalidating_ or _updating_/_consolidating_ existing memories. If the system over-extracts, this could lead to reduced precision of memories when your agent needs to search the store. If it under-extracts, this could lead to low recall. LangMem uses a memory enrichment process that strives to balance memory creation and consolidation, while letting you, the developer, customize the instructions to further shift the strength of each.

Finally, memory relevance is more than just semantic similarity. Recall should combine similarity with "importance" of the memory, as well as the memory's "strength", which is a function of how recently/frequently it was used.

![Collection update process](img/update-list.png)

??? example "Extracting semantic memories as collections"

    ??? note "Setup"

        ```python
        from langmem import create_memory_manager
        
        # highlight-next-line
        manager = create_memory_manager(
            "anthropic:claude-3-5-sonnet-latest",
            instructions="Extract all noteworthy facts, events, and relationships. Indicate their importance.",
            # highlight-next-line
            enable_inserts=True,
        )

        # Process a conversation to extract semantic memories
        conversation = [
            {"role": "user", "content": "I work at Acme Corp in the ML team"},
            {"role": "assistant", "content": "I'll remember that. What kind of ML work do you do?"},
            {"role": "user", "content": "Mostly NLP and large language models"}
        ]
        ```

    ```python
    memories = manager.invoke({"messages": conversation})
    # Example memories:
    # [
    #     ExtractedMemory(
    #         id="27e96a9d-8e53-4031-865e-5ec50c1f7ad5",
    #         content=Memory(
    #             content="[IMPORTANT] User prefers to be called Lex (short for Alex) and appreciates"
    #             " casual, witty communication style with relevant emojis."
    #         ),
    #     ),
    #     ExtractedMemory(
    #         id="e2f6b646-cdf1-4be1-bb40-0fd91d25d00f",
    #         content=Memory(
    #             content="[BACKGROUND] Lex is proficient in Python programming and specializes in developing"
    #             " AI systems with a focus on making them sound more natural and less corporate."
    #         ),
    #     ),
    #     ExtractedMemory(
    #         id="c1e03ebb-a393-4e8d-8eb7-b928d8bed510",
    #         content=Memory(
    #             content="[HOBBY] Lex is a competitive speedcuber (someone who solves Rubik's cubes competitively),"
    #             " showing an interest in both technical and recreational puzzle-solving."
    #         ),
    #     ),
    #     ExtractedMemory(
    #         id="ee7fc6e4-0118-425f-8704-6b3145881ff7",
    #         content=Memory(
    #             content="[PERSONALITY] Based on communication style and interests, Lex appears to value authenticity,"
    #             " creativity, and technical excellence while maintaining a fun, approachable demeanor."
    #         ),
    #     ),
    # ]

    ```

#### Profiles

**Profiles** on the other hand are well-scoped for a particular task. Profiles are a single document that represents the current state, like a user's main goals with using an app, their preferred name and response stele, etc. When new information arrives, it updates the existing document rather than creating a new one. This approach is ideal when you only care about the latest state and want to avoid remembering extraneous information.

![Profile update process](img/update-profile.png)

??? example "Managing user preferences with profiles"

    ??? note "Setup"

        ```python
        from langmem import create_memory_manager
        from pydantic import BaseModel


        class UserProfile(BaseModel):
            """Save the user's preferences."""
            name: str
            preferred_name: str
            response_style_preference: str
            special_skills: list[str]
            other_preferences: list[str]


        manager = create_memory_manager(
            "anthropic:claude-3-5-sonnet-latest",
            schemas=[UserProfile],
            instructions="Extract user preferences and settings",
            enable_inserts=False,
        )

        # Extract user preferences from a conversation
        conversation = [
            {"role": "user", "content": "Hi! I'm Alex but please call me Lex. I'm a wizard at Python and love making AI systems that don't sound like boring corporate robots ðŸ¤–"},
            {"role": "assistant", "content": "Nice to meet you, Lex! Love the anti-corporate-robot stance. How would you like me to communicate with you?"},
            {"role": "user", "content": "Keep it casual and witty - and maybe throw in some relevant emojis when it feels right âœ¨ Also, besides AI, I do competitive speedcubing!"},
        ]
        ```

    ```python
    profile = manager.invoke({"messages": conversation})[0]
    print(profile)
    # Example profile:
    # ExtractedMemory(
    #     id="6f555d97-387e-4af6-a23f-a66b4e809b0e",
    #     content=UserProfile(
    #         name="Alex",
    #         preferred_name="Lex",
    #         response_style_preference="casual and witty with appropriate emojis",
    #         special_skills=[
    #             "Python programming",
    #             "AI development",
    #             "competitive speedcubing",
    #         ],
    #         other_preferences=[
    #             "prefers informal communication",
    #             "dislikes corporate-style interactions",
    #         ],
    #     ),
    # )
    ```

Choose between profiles and collections based on how you'll use the data: profiles excel when you need quick access to current state and when you have data requirements about what type of information you can store. They are also easy to present to a user for manual ediing. Collections are useful when you want to track knowledge across many interactions without loss of information, and when you want to recall certain information contextually rather than every time.

### Episodic Memory: Past Experiences

Episodic memory preserves successful interactions as learning examples that guide future behavior. Unlike semantic memory which stores facts, episodic memory captures the full context of an interactionâ€”the situation, the thought process that led to success, and why that approach worked. These memories help the agent learn from experience, adapting its responses based on what has worked before.

??? example "Defining and extracting episodes"
    
    ??? note "Setup"

        ```python
        from pydantic import BaseModel, Field
        from langmem import create_memory_manager

        class Episode(BaseModel):
            """An episode captures how to handle a specific situation, including the reasoning process
            and what made it successful."""
            
            observation: str = Field(
                ..., 
                description="The situation and relevant context"
            )
            thoughts: str = Field(
                ...,
                description="Key considerations and reasoning process"
            )
            action: str = Field(
                ...,
                description="What was done in response"
            )
            result: str = Field(
                ...,
                description="What happened and why it worked"
            )

        # highlight-next-line
        manager = create_memory_manager(
            "anthropic:claude-3-5-sonnet-latest",
            schemas=[Episode],
            instructions="Extract examples of successful interactions. Include the context, thought process, and why the approach worked.",
            enable_inserts=True,
        )

        # Example conversation
        conversation = [
            {"role": "user", "content": "What's a binary tree? I work with family trees if that helps"},
            {"role": "assistant", "content": "A binary tree is like a family tree, but each parent has at most 2 children. Here's a simple example:\n   Bob\n  /  \\\nAmy  Carl\n\nJust like in family trees, we call Bob the 'parent' and Amy and Carl the 'children'."},
            {"role": "user", "content": "Oh that makes sense! So in a binary search tree, would it be like organizing a family by age?"},
        ]
        ```

    ```python
    # Extract episode(s)
    episodes = manager.invoke({"messages": conversation})
    # Example episode:
    # [
    #     ExtractedMemory(
    #         id="f9194af3-a63f-4d8a-98e9-16c66e649844",
    #         content=Episode(
    #             observation="User struggled debugging a recursive "
    #                         "function for longest path in binary "
    #                         "tree, unclear on logic.",
    #             thoughts="Used explorer in treehouse village "
    #                      "metaphor to explain recursion:\n"
    #                      "- Houses = Nodes\n"
    #                      "- Bridges = Edges\n"
    #                      "- Explorer's path = Traversal",
    #             action="Reframed problem using metaphor, "
    #                    "outlined steps:\n"
    #                    "1. Check left path\n"
    #                    "2. Check right path\n"
    #                    "3. Add 1 for current position\n"
    #                    "Highlighted common bugs",
    #             result="Metaphor helped user understand logic. "
    #                    "Worked because it:\n"
    #                    "1. Made concepts tangible\n"
    #                    "2. Created mental model\n"
    #                    "3. Showed key steps\n"
    #                    "4. Pointed to likely bugs",
    #         ),
    #     )
    # ]
    ```


### Procedural Memory: System Instructions

Procedural memory encodes how an agent should behave and respond. It starts with system prompts that define core behavior, then evolves through feedback and experience. As the agent interacts with users, it refines these instructions, learning which approaches work best for different situations.

![Instructions update process](img/update-instructions.png)

??? example "Optimizing prompts based on feedback"

    ??? note "Setup"

        ```python
        from langmem import create_prompt_optimizer

        # highlight-next-line
        optimizer = create_prompt_optimizer(
            "anthropic:claude-3-5-sonnet-latest",
            kind="metaprompt",
            config={"max_reflection_steps": 3}
        )
        ```
    ```python
    prompt = "You are a helpful assistant."
    trajectory = [
        {"role": "user", "content": "Explain inheritance in Python"},
        {"role": "assistant", "content": "Here's a detailed theoretical explanation..."},
        {"role": "user", "content": "Show me a practical example instead"},
    ]
    optimized = optimizer.invoke({
        "trajectories": [(trajectory, {"user_score": 0})], 
        "prompt": prompt
    })
    print(optimized)
    # You are a helpful assistant with expertise in explaining technical concepts clearly and practically. When explaining programming concepts:

    # 1. Start with a brief, practical explanation supported by a concrete code example
    # 2. If the user requests more theoretical details, provide them after the practical example
    # 3. Always include working code examples for programming-related questions
    # 4. Pay close attention to user preferences - if they ask for a specific approach (like practical examples or theory), adapt your response accordingly
    # 5. Use simple, clear language and break down complex concepts into digestible parts

    # When users ask follow-up questions or request a different approach, immediately adjust your explanation style to match their preferences. If they ask for practical examples, provide them. If they ask for theory, explain the concepts in depth.
    ```



## Writing memories {#writing-memories}

Memories can form in two ways, each suited for different needs. Active formation happens during conversations, enabling immediate updates when critical context emerges. Background formation occurs between interactions, allowing deeper pattern analysis without impacting response time. This dual approach lets you balance responsiveness with thorough learning.

| Formation Type | Latency Impact | Update Speed | Processing Load | Use Case |
|----------------|----------------|--------------|-----------------|-----------|
| Active | Higher | Immediate | During Response | Critical Context Updates |
| Background | None | Delayed | Between/After Calls | Pattern Analysis, Summaries |

![Hot path vs background memory processing](img/hot_path_vs_background.png)

### Conscious Formation

You may want your agent to save memories "in the hot path." This active memory formation happens during the conversation, enabling immediate updates when critical context emerges. This approach is easy to implement and lets the agent itself choose how to store and update its memory. However, it adds perceptible latency to user interactions, and it adds one more obstacle to the agent's ability to satisfy the user's needs.

Check out the ["hot path" quickstart](../hot_path_quickstart.md) for an example of how to use this technique.

### Subconcious Formation

"Subconcious" memory formation refers to the technique of prompting an LLM to reflect on a conversation after it occurs (or after it has been inactive for some period), finding patterns and extracting insights without slowing down the immediate interaction or adding complexity to the agent's tool choice decisions. This approach is perfect for ensuring higher recall of exracted information.

Check out the ["background" quickstart](../background_quickstart.md) for an example of how to use this technique.

## Integration Patterns

LangMem's memory utilities are organized in two layers of integration patterns:

### 1. Core API {#functional-core}

At its heart, LangMem provides functions that transform memory state without side effects. These primitives are the building blocks for memory operations:

- [**Memory Managers**](../reference/memory.md#langmem.create_memory_manager): Extract new memories, update or remove outdated memories, and consolidate and generalize from existing memories based on new conversation information
- [**Prompt Optimizers**](../reference/prompt_optimization.md#langmem.create_prompt_optimizer): Update prompt rules and core behavior based on conversation information (with optional feedback)

These core functions do not depend on any particular database or storage system. You can use them in any application.

### 2. Stateful Integration

The next layer up depends on LangGraph's long-term memory store. These components use the core API above to transform memories that existin in the store and upsert/delete them as needed when new conversation information comesin:

- [**Store Managers**](../reference/memory.md#langmem.create_memory_store_manager): Automatically persist extracted memories
- [**Memory Management Tools**](../reference/tools.md#langmem.create_manage_memory_tool): Give agents direct access to memory operations

Use these if you're using LangGraph Platform or LangGraph OSS, since it's an easy way to add memory capabilities to your agents.


## Storage System {#storage-system}

??? note "Storage is optional"
    
    Remember that LangMem's core functionality is built around that don't require any specific storage layer. The storage features described here are part of LangMem's higher-level integration with LangGraph, useful when you want built-in persistence.


When using LangMem's stateful operators or platform services, the storage system is built on LangGraph's storage primitives, providing a flexible and powerful way to organize and access memories. The storage system is designed around two concepts:

### Memory Namespaces {#memory-namespaces}

Memories are organized into namespaces that allows for natural segmentation of data:

- **Multi-Level Namespaces**: Group memories by organization, user, application, or any other hierarchical structure
- **Contextual Keys**: Identify memories uniquely within their namespace
- **Structured Content**: Store rich, structured data with metadata for better organization

??? example "Organizing memories hierarchically"

    ```python
    # Organize memories by organization -> configurable user -> context
    namespace = ("acme_corp", "{user_id}", "code_assistant")
    ```

Namespaces can include template variables (such as `"{user_id}"`) to be populated at runtime from `configurable` fields in the `RunnableConfig`.
See [how to dynamically configure namespaces](../guides/dynamically_configure_namespaces.md) for an example, or the [NamespaceTemplate](../reference/utils.md#langmem.utils.NamespaceTemplate) reference docs for more details.

### Flexible Retrieval

If you use one of the managed APIs, LangMem will integrate directly with LangGraph's [BaseStore](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore) interface for memory storage and retrieval. The storage system supports multiple ways to retrieve memories:

- [**Direct Access**](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore.get): Get a specific memory by key
- [**Semantic Search**](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore.search): Find memories by semantic similarity
- [**Metadata Filtering**](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore.search): Filter memories by their attributes

For more details on storage capabilities, see the [LangGraph Storage documentation](https://langchain-ai.github.io/langgraph/reference/store/).
---
docs/docs/reference/prompt_optimization.md
---
title: Prompt Optimization API Reference
---

# Prompt Optimization API Reference

::: langmem
    handler: python
    options:
      members:
        - create_prompt_optimizer
        - create_multi_prompt_optimizer

::: langmem.prompts.types
    handler: python
    options:
      members:
        - Prompt
        - OptimizerInput
        - MultiPromptOptimizerInput
        - AnnotatedTrajectory
---
docs/docs/reference/utils.md
---
title: Utilities API Reference
---

# Utilities API Reference

::: langmem.utils
    handler: python
    options:
      members:
        - NamespaceTemplate

::: langmem
    handler: python
    options:
      members:
        - ReflectionExecutor---
docs/docs/reference/memory.md
---
title: Memory API Reference
---

# Memory API Reference

::: langmem
    handler: python
    options:
      members:
        - create_memory_manager
        - create_memory_store_manager
---
docs/docs/reference/index.md
---
title: Reference
description: API reference for LangMem
---

# API Reference

Welcome to the LangMem API reference! The documentation is organized into three main sections:

## [Memory Management](memory.md)

Core memory management utilities:

- [`create_memory_manager`](memory.md#langmem.create_memory_manager) - Stateless memory extraction and updates
- [`create_memory_store_manager`](memory.md#langmem.create_memory_store_manager) - Stateful memory management with BaseStore

## [Memory Tools](tools.md)

Agent tools for memory management:

- [`create_manage_memory_tool`](tools.md#langmem.create_manage_memory_tool) - Tool for storing and updating memories
- [`create_search_memory_tool`](tools.md#langmem.create_search_memory_tool) - Tool for searching stored memories

## [Prompt Optimization](prompt_optimization.md)

Utilities for optimizing prompts:

- [`create_prompt_optimizer`](prompt_optimization.md#langmem.create_prompt_optimizer) - Single prompt optimization
- [`create_multi_prompt_optimizer`](prompt_optimization.md#langmem.create_multi_prompt_optimizer) - Multi-prompt system optimization

## [Utilities](utils.md)

- [`NamespaceTemplate`](utils.md#langmem.utils.NamespaceTemplate) - internal namespace template utility
- [`ReflectionExecutor`](utils.md#langmem.ReflectionExecutor) - Reflection executor to schedule memory management remotely or in a background thread.---
docs/docs/reference/tools.md
---
title: Memory Tools API Reference
---

# Memory Tools API Reference

::: langmem
    handler: python
    options:
      members:
        - create_manage_memory_tool
        - create_search_memory_tool
---
docs/README.md
# Setup

To setup requirements for building docs you can run:

```bash
poetry install --with test
```

## Serving documentation locally

To run the documentation server locally you can run:

```bash
make serve-docs
```

## Execute notebooks

If you would like to automatically execute all of the notebooks, to mimic the "Run notebooks" GHA, you can run:

```bash
python docs/_scripts/prepare_notebooks_for_ci.py
./docs/_scripts/execute_notebooks.sh
```

**Note**: if you want to run the notebooks without `%pip install` cells, you can run:

```bash
python docs/_scripts/prepare_notebooks_for_ci.py --comment-install-cells
./docs/_scripts/execute_notebooks.sh
```

`prepare_notebooks_for_ci.py` script will add VCR cassette context manager for each cell in the notebook, so that:
* when the notebook is run for the first time, cells with network requests will be recorded to a VCR cassette file
* when the notebook is run subsequently, the cells with network requests will be replayed from the cassettes

**Note**: this is currently limited only to the notebooks in `docs/docs/how-tos`

## Adding new notebooks

If you are adding a notebook with API requests, it's **recommended** to record network requests so that they can be subsequently replayed. If this is not done, the notebook runner will make API requests every time the notebook is run, which can be costly and slow.

To record network requests, please make sure to first run `prepare_notebooks_for_ci.py` script.

Then, run

```bash
jupyter execute <path_to_notebook>
```

Once the notebook is executed, you should see the new VCR cassettes recorded in `docs/cassettes` directory and discard the updated notebook.

## Updating existing notebooks

If you are updating an existing notebook, please make sure to remove any existing cassettes for the notebook in `docs/cassettes` directory (each cassette is prefixed with the notebook name), and then run the steps from the "Adding new notebooks" section above.

To delete cassettes for a notebook, you can run:

```bash
rm docs/cassettes/<notebook_name>*
```