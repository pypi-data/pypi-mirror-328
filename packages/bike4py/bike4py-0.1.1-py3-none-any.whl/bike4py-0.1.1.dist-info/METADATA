Metadata-Version: 2.4
Name: bike4py
Version: 0.1.1
Summary: Bike4Mind API client
Project-URL: Homepage, https://bike4mind.com
Project-URL: Repository, https://github.com/MillionOnMars/bike4py
License-Expression: MIT
License-File: LICENSE
Keywords: api,bike4mind,bike4py,chat,client,llm
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Requires-Python: >=3.8
Requires-Dist: aiohttp>=3.9.0
Requires-Dist: click>=8.0.0
Requires-Dist: httpx>=0.24.0
Requires-Dist: pydantic>=2.0
Requires-Dist: websockets>=11.0
Provides-Extra: dev
Requires-Dist: black>=23.0; extra == 'dev'
Requires-Dist: mypy>=1.0; extra == 'dev'
Requires-Dist: pytest-asyncio>=0.21.0; extra == 'dev'
Requires-Dist: pytest>=7.0; extra == 'dev'
Description-Content-Type: text/markdown

# bike4py

Bike4py is a Python client for the Bike4Mind API.

## Installation

```bash
pip install bike4py
```

## Usage

To use the client, you need to create a client with your refresh token:

```python
from bike4py import LLMClient

client = LLMClient(
    refresh_token="your_refresh_token"
)
```

Once you've created a client, you should connect to the websocket - this is
where responses to your prompt will be sent:

```python
# Connect to websocket, needed to see LLM responses
await client.connect()
```

You can then submit a prompt and stream the response.  The notebook ID can be found in the URL of the notebook you want to use.

```python
# Submit a prompt
request = ChatCompletionRequest(
  sessionId="%your_notebook_id%",
  message="Hello, how are you?"
)
response = client.submit_prompt(request)

# Stream the response
async for event in client.stream_events():
    if isinstance(event, StatusEvent):
        print(event.status)
    if isinstance(event, CompletionEvent):
        print(event.success)
    if isinstance(event, ContentEvent):
        print(event.content)
```

The event classes emitted by the `stream_events` method are:

- `StatusEvent`: Status updates from the LLM as Bike4Mind is processing the request.
- `ContentEvent`: While generating, the LLM will stream content in chunks; this event includes the accumulated progress of the response.
- `CompletionEvent`: At completion, this event is emitted with the final response.

The intent is that it's easy to separate the event types and process only the ones you need.  If you want streaming content for UI purposes, process the `ContentEvent` events.  If you want to know when the LLM has completed, process the `CompletionEvent` event.

## Upload a file

```python
file_id = client.upload_file("test.txt", "text/plain")
```

This will return a `file_id` which you can then pass to the `submit_prompt` method to include in a prompt.
```python
...
file_id = client.upload_file("test.txt", "text/plain")
request = ChatCompletionRequest(
  sessionId="%your_notebook_id%",
  message="Can you summarize this file for me?",
  fabFileIds=[file_id]
)
response = client.submit_prompt(request)
...
```