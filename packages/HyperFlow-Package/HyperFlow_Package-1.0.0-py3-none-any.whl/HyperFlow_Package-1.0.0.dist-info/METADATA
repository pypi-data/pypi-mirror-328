Metadata-Version: 2.2
Name: HyperFlow_Package
Version: 1.0.0
Summary: HyperFlow: Next-Generation Computational Framework for Machine Learning & Deep Learning
Home-page: https://github.com/Shyanil/HyperFlow/tree/main/The%20HyperFlow/HyperFlow
Author: Shyanil Mishra
Author-email: shyanilmishra94@gmail.com
License: MIT
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: numpy<2.1,>=1.26
Provides-Extra: dev
Requires-Dist: pytest>=7.0; extra == "dev"
Requires-Dist: twine>=4.0.2; extra == "dev"
Dynamic: author
Dynamic: author-email
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# HyperFlow: Next-Generation Computational Framework for Machine Learning & Deep Learning

![Project Image](https://raw.githubusercontent.com/Shyanil/HyperFlow/refs/heads/main/The%20HyperFlow/HyperFlow_FlowUnit.png)

ğŸ“„ [Read the Full Documentation](https://drive.google.com/file/d/1DfNKxcDb-VEAI13xm1UwnLyZwF5_1_t_/view?usp=sharing)

## ğŸš€ Introduction

HyperFlow is an advanced computational framework designed to enhance machine learning and deep learning research. At its core is the **FlowUnit** class, an optimized and modular implementation supporting:

- ğŸ”¢ **Mathematical operations**
- âš™ï¸ **Activation functions**
- ğŸ“‰ **Optimization techniques**
- ğŸ”„ **Backpropagation**

HyperFlow provides simplicity and power, making it ideal for understanding neural networks, automatic differentiation, and optimization techniques.

## ğŸ¯ Inspiration

Inspired by [Micrograd](https://github.com/karpathy/micrograd) by **Andrej Karpathy**, HyperFlow extends its capabilities by offering:

- ğŸš€ Advanced mathematical operations
- ğŸ¯ Broader activation function support
- âš¡ Optimized backpropagation
- ğŸ” Enhanced modularity

It serves as a lightweight yet powerful alternative to complex deep learning frameworks like PyTorch.

## ğŸ’¡ Why HyperFlow?

âœ… **Lightweight & Transparent** â€“ Focuses on raw Python implementations to help understand ML/DL concepts.  
âš¡ **Efficient & Optimized** â€“ Uses map functions for better performance.  
ğŸ”§ **Flexible & Powerful** â€“ Supports neural networks, including backpropagation.  
ğŸ“‰ **Minimal NumPy Dependency** â€“ Encourages learning without excessive reliance on pre-built libraries.

## ğŸ”‘ Core Functionalities

### ğŸ”¢ Mathematical Operations

- `create2darray`, `convert2darray`, `add`, `sub`, `mul`, `matmul`, `dot`, `pow`

### âš™ï¸ Activation Functions

- `sigmoid`, `tanh`, `ReLU`, `Leaky ReLU`, `softmax`

### ğŸ“‰ Loss Functions

- `categorical_cross_entropy`, `binary_cross_entropy`, `mse_loss`

### ğŸ”„ Optimization & Backpropagation

- `backpropagate`, `gradient_descent`

## ğŸ§  Neural Network Implementation

The **Neuron.py** module simplifies the creation of:

- ğŸ— **Neurons**
- ğŸ”— **Layers**
- ğŸ› **Complete Neural Networks**

This module offers full control over weights, biases, and architecture for in-depth experimentation.

## ğŸ“Œ Examples & Outputs

### âœ… Dot Product Calculation

```python
from hyperflow import FlowUnit

a = FlowUnit([1, 2, 3])
b = FlowUnit([4, 5, 6])

result = a.__dot__(b)
print(f"Dot product result: {result.data}")
```

**Output:**

```
Dot product result: 32
```

### ğŸ”„ Backpropagation Test

```python
def test_backpropagation():
    x = FlowUnit(2.0)
    y = FlowUnit(-3.0)
    z = FlowUnit(1.5)

    a = x.sigmoid()
    b = y.tanh()
    c = z.relu()
    d = x.leaky_relu()

    loss = a + b + c + d
    loss.backpropagate()

    print(f"x.grad: {x.grad}")
    print(f"y.grad: {y.grad}")
    print(f"z.grad: {z.grad}")

test_backpropagation()
```

**Output:**

```
x.grad: 1.1049935854035067
y.grad: 0.00986603716543999
z.grad: 1.0
```

### ğŸ”¢ Activation Function Tests

```python
def test_flow_unit_functions():
    data = np.array([1.0, 2.0, 3.0])
    flow_unit = FlowUnit(data)

    sigmoid_out = flow_unit.sigmoid()
    print("Sigmoid Output:", sigmoid_out.data)

    tanh_out = flow_unit.tanh()
    print("Tanh Output:", tanh_out.data)

    relu_out = flow_unit.relu()
    print("ReLU Output:", relu_out.data)

    leaky_relu_out = flow_unit.leaky_relu(alpha=0.01)
    print("Leaky ReLU Output:", leaky_relu_out.data)

    softmax_out = flow_unit.softmax()
    print("Softmax Output:", softmax_out.data)

test_flow_unit_functions()
```

**Output:**

```
Sigmoid Output: [0.731 0.880 0.952]
Tanh Output: [0.761 0.964 0.995]
ReLU Output: [1.0 2.0 3.0]
Leaky ReLU Output: [1.0 2.0 3.0]
Softmax Output: [0.090 0.244 0.665]
```

### ğŸ“‰ Loss Function Tests

```python
def test_loss_functions():
    logits = FlowUnit(np.array([2.0, 1.0, 0.1]))
    target = [1, 0, 0]

    cce_loss = LossFunctions.categorical_cross_entropy(logits, target)
    print("Categorical Cross-Entropy Loss:", cce_loss.data)

    inputs = FlowUnit(np.array([[1.0, 2.0], [3.0, 4.0]]))
    target = FlowUnit(np.array([1, 0]))
    parameters = (FlowUnit(np.array([0.5, -0.5])), FlowUnit(0.1))

    bce_loss = LossFunctions.binary_cross_entropy_loss(inputs, target, parameters)
    print("Binary Cross-Entropy Loss:", bce_loss.data)

test_loss_functions()
```

**Output:**

```
Categorical Cross-Entropy Loss: 0.417
Binary Cross-Entropy Loss: [0.715]
```

## ğŸ“š Explore More Use Cases

Find more examples and use cases in the [GitHub repository](https://github.com/Shyanil/HyperFlow).
