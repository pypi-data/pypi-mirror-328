"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
from dataclasses_json import Undefined, dataclass_json
from enum import Enum
from honeyhive import utils
from typing import List, Optional


class DatasetType(str, Enum):
    r"""What the dataset is to be used for - \\"evaluation\\" or \\"fine-tuning\\" """
    EVALUATION = 'evaluation'
    FINE_TUNING = 'fine-tuning'


class PipelineType(str, Enum):
    r"""The type of data included in the dataset - \\"event\\" (default) or \\"session\\" """
    EVENT = 'event'
    SESSION = 'session'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class Dataset:
    project: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('project'), 'exclude': lambda f: f is None }})
    r"""UUID of the project associated with this dataset"""
    name: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('name'), 'exclude': lambda f: f is None }})
    r"""Name of the dataset"""
    description: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('description'), 'exclude': lambda f: f is None }})
    r"""A description for the dataset"""
    type: Optional[DatasetType] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('type'), 'exclude': lambda f: f is None }})
    r"""What the dataset is to be used for - \\"evaluation\\" or \\"fine-tuning\\" """
    datapoints: Optional[List[str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('datapoints'), 'exclude': lambda f: f is None }})
    r"""List of unique datapoint ids to be included in this dataset"""
    num_points: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('num_points'), 'exclude': lambda f: f is None }})
    r"""Number of datapoints included in the dataset"""
    linked_evals: Optional[List[str]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('linked_evals'), 'exclude': lambda f: f is None }})
    saved: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('saved'), 'exclude': lambda f: f is None }})
    r"""Whether the dataset has been saved or detected"""
    pipeline_type: Optional[PipelineType] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('pipeline_type'), 'exclude': lambda f: f is None }})
    r"""The type of data included in the dataset - \\"event\\" (default) or \\"session\\" """
    created_at: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('created_at'), 'exclude': lambda f: f is None }})
    r"""Timestamp of when the dataset was created"""
    updated_at: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('updated_at'), 'exclude': lambda f: f is None }})
    r"""Timestamp of when the dataset was last updated"""
    

