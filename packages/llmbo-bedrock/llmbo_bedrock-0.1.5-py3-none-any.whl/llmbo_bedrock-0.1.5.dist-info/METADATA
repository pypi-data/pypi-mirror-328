Metadata-Version: 2.4
Name: llmbo-bedrock
Version: 0.1.5
Summary: Large Language Model Batch Operations
Author-email: David Gillespie <david.gillespie@digital.cabinet-office.gov.uk>
License-File: LICENSE
Requires-Python: >=3.10
Requires-Dist: boto3<2.0,>=1.3
Requires-Dist: pydantic>=2.10
Requires-Dist: python-dotenv>=1.0.1
Description-Content-Type: text/markdown

# LLMbo - Large language model batch operations

A library to make working with batch inference of LLM call in AWS easier. 
Currently support is limited to Anthropic models.

## Prerequisites 

- A `.env` file with an entry for `AWS_PROFILE=`. This profile should have sufficient 
permissions to execute a batch inference job. [find the link]
- A role with the required permissions [find details]
- A s3 bucket to store the input and outputs for the job.   
    - Inputs will be written to `f{s3_bucket}/input/{job_name}.jsonl`
    - Outputs will be written to `f{s3_bucket}/output/{job_id}/{job_name}.jsonl.out` and 
      `f{s3_bucket}/output/{job_id}/manifest.json.out`


## Usage

See:
- `batch_inference_example()`: for an example of free text response
- `structured_batch_inference_example`: for an example of structured response ala instructor


## Developing 

To install the dev and test dependencies:
```
pip install -e ".[test,dev]" 
```

## To Do
- example data folder
- Tests 
- `utils` module for creating the required AWS role etc.