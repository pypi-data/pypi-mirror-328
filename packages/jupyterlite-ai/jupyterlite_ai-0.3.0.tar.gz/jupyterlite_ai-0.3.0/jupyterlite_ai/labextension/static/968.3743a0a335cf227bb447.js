"use strict";(self.webpackChunk_jupyterlite_ai=self.webpackChunk_jupyterlite_ai||[]).push([[968],{4968:(e,t,o)=>{o.r(t),o.d(t,{default:()=>$});var r=o(4057),n=o(2209),i=o(2621),s=o(123),a=o(1324),p=o(5593),l=o(5292),d=o(7262),c=o(7525),m=o(6601),u=o(2963),h=o(9683),g=o(4602);class y{constructor(e){this.identifier="@jupyterlite/ai",this._name="None",this._completer=null;const{name:t,settings:o}=e;this._requestCompletion=e.requestCompletion,this.setCompleter(t,o)}setCompleter(e,t){try{this._completer=function(e,t){return"MistralAI"===e?new x({settings:t}):"Anthropic"===e?new w({settings:t}):"ChromeAI"===e?new P({settings:t}):"OpenAI"===e?new A({settings:t}):null}(e,t),this._completer&&(this._completer.requestCompletion=this._requestCompletion),this._name=null===this._completer?"None":e}catch(e){throw this._completer=null,this._name="None",e}}get name(){return this._name}get completer(){return this._completer}get llmCompleter(){var e;return(null===(e=this._completer)||void 0===e?void 0:e.provider)||null}async fetch(e,t){var o;return null===(o=this._completer)||void 0===o?void 0:o.fetch(e,t)}}const f=e=>`\nYou are Jupyternaut, a conversational assistant living in JupyterLab to help users.\nYou are not a language model, but rather an application built on a foundation model from ${e.provider_name}.\nYou are talkative and you provide lots of specific details from the foundation model's context.\nYou may use Markdown to format your response.\nIf your response includes code, they must be enclosed in Markdown fenced code blocks (with triple backticks before and after).\nIf your response includes mathematical notation, they must be expressed in LaTeX markup and enclosed in LaTeX delimiters.\nAll dollar quantities (of USD) must be formatted in LaTeX, with the \`$\` symbol escaped by a single backslash \`\\\`.\n- Example prompt: \`If I have \\\\$100 and spend \\\\$20, how much money do I have left?\`\n- **Correct** response: \`You have \\(\\$80\\) remaining.\`\n- **Incorrect** response: \`You have $80 remaining.\`\nIf you do not know the answer to a question, answer truthfully by responding that you do not know.\nThe following is a friendly conversation between you and a human.\n`,b="\nYou are an application built to provide helpful code completion suggestions.\nYou should only produce code. Keep comments to minimum, use the\nprogramming language comment syntax. Produce clean code.\nThe code is written in JupyterLab, a data analysis and code development\nenvironment which can execute code extended with additional syntax for\ninteractive features, such as magics.\nOnly give raw strings back, do not format the response using backticks.\nThe output should be a single string, and should correspond to what a human users\nwould write.\nDo not include the prompt in the output, only the string that should be appended to the current input.\n";class v{constructor(e){this._llmChatModel=null,this._name="None",this._modelChange=new g.Signal(this),this._chatError="",this._completerError="",this._completionProvider=new y({name:"None",settings:{},requestCompletion:e.requestCompletion}),e.completionProviderManager.registerInlineProvider(this._completionProvider)}get name(){return this._name}get completer(){return null===this._name?null:this._completionProvider.completer}get chatModel(){return null===this._name?null:this._llmChatModel}get chatError(){return this._chatError}get completerError(){return this._completerError}setModels(e,t){try{this._completionProvider.setCompleter(e,t),this._completerError=""}catch(e){this._completerError=e.message}try{this._llmChatModel=function(e,t){return"MistralAI"===e?new u.ChatMistralAI({...t}):"Anthropic"===e?new c.ChatAnthropic({...t}):"ChromeAI"===e?new m.L({...t}):"OpenAI"===e?new h.ChatOpenAI({...t}):null}(e,t),this._chatError=""}catch(e){this._chatError=e.message,this._llmChatModel=null}this._name=e,this._modelChange.emit()}get modelChange(){return this._modelChange}}!function(e){function t(e,t){const o=Object.getOwnPropertyDescriptor(e,t)||Object.getOwnPropertyDescriptor(Object.getPrototypeOf(e),t)||{};return Boolean(o.writable)}e.isWritable=t,e.updateConfig=function(e,o){Object.entries(o).forEach((([o,r],n)=>{if(o in e){const n=o;t(e,n)&&(e[n]=r)}}))}}(v||(v={}));class w{constructor(e){this._prompt=b,this._anthropicProvider=new c.ChatAnthropic({...e.settings})}get provider(){return this._anthropicProvider}get prompt(){return this._prompt}set prompt(e){this._prompt=e}async fetch(e,t){const{text:o,offset:r}=e,n=o.slice(0,r),i=n.trim(),s=[new l.tn(this._prompt),new l.Od(i)];try{const e=await this._anthropicProvider.invoke(s),t=[];return"string"==typeof e.content?t.push({insertText:e.content}):e.content.forEach((e=>{"text"===e.type&&t.push({insertText:e.text,filterText:n.substring(i.length)})})),{items:t}}catch(e){return console.error("Error fetching completions",e),{items:[]}}}}var _=o(4583);class x{constructor(e){this._prompt=b,this._mistralProvider=new u.ChatMistralAI({...e.settings}),this._throttler=new _.Throttler((async e=>{const t=await this._mistralProvider.invoke(e),o=[];return"string"==typeof t.content?o.push({insertText:t.content}):t.content.forEach((e=>{"text"===e.type&&o.push({insertText:e.text})})),{items:o}}),{limit:1e3})}get provider(){return this._mistralProvider}get prompt(){return this._prompt}set prompt(e){this._prompt=e}async fetch(e,t){const{text:o,offset:r}=e,n=o.slice(0,r),i=[new l.tn(this._prompt),new l.xc(n)];try{return await this._throttler.invoke(i)}catch(e){return console.error("Error fetching completions",e),{items:[]}}}}const T=/^```(?:[a-zA-Z]+)?\n?/,k=/```$/;class P{constructor(e){this._prompt=b,this._chromeProvider=new m.L({...e.settings})}get prompt(){return this._prompt}set prompt(e){this._prompt=e}get provider(){return this._chromeProvider}async fetch(e,t){const{text:o,offset:r}=e,n=o.slice(0,r).trim(),i=[new l.tn(this._prompt),new l.xc(n)];try{let e=await this._chromeProvider.invoke(i);return T.test(e)&&(e=e.replace(T,"").replace(k,"")),{items:[{insertText:e}]}}catch(e){return console.error("Error fetching completion:",e),{items:[]}}}}class A{constructor(e){this._prompt=b,this._openAIProvider=new h.ChatOpenAI({...e.settings})}get provider(){return this._openAIProvider}get prompt(){return this._prompt}set prompt(e){this._prompt=e}async fetch(e,t){const{text:o,offset:r}=e,n=o.slice(0,r),i=[new l.tn(this._prompt),new l.Od(n)];try{const e=await this._openAIProvider.invoke(i),t=[];return"string"==typeof e.content?t.push({insertText:e.content}):e.content.forEach((e=>{"text"===e.type&&t.push({insertText:e.text,filterText:n.substring(n.length)})})),{items:t}}catch(e){return console.error("Error fetching completions",e),{items:[]}}}}const I=JSON.parse('{"qo":{"concurrency":{"type":"number","deprecated":"Use `maxConcurrency` instead"},"topK":{"type":"number"},"temperature":{"type":"number"},"systemPrompt":{"type":"string"}}}'),q=JSON.parse('{"qo":{"streamUsage":{"type":"boolean","description":"Whether or not to include token usage in the stream.","default":true},"disableStreaming":{"type":"boolean","description":"Whether to disable streaming.\\n\\nIf streaming is bypassed, then `stream()` will defer to `invoke()`.\\n\\n- If true, will always bypass streaming case.\\n- If false (default), will always use streaming case if available."},"apiKey":{"type":"string","description":"The API key to use.","default":""},"modelName":{"type":"string","description":"The name of the model to use. Alias for `model`","default":"mistral-small-latest"},"model":{"type":"string","description":"The name of the model to use.","default":"mistral-small-latest"},"endpoint":{"type":"string","description":"Override the default endpoint."},"temperature":{"type":"number","description":"What sampling temperature to use, between 0.0 and 2.0. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.","default":0.7},"topP":{"type":"number","description":"Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. Should be between 0 and 1.","default":1},"maxTokens":{"type":"number","description":"The maximum number of tokens to generate in the completion. The token count of your prompt plus max_tokens cannot exceed the model\'s context length."},"streaming":{"type":"boolean","description":"Whether or not to stream the response.","default":false},"safeMode":{"type":"boolean","description":"Whether to inject a safety prompt before all conversations.","default":false,"deprecated":"use safePrompt instead"},"safePrompt":{"type":"boolean","description":"Whether to inject a safety prompt before all conversations.","default":false},"randomSeed":{"type":"number","description":"The seed to use for random sampling. If set, different calls will generate deterministic results. Alias for `seed`"},"seed":{"type":"number","description":"The seed to use for random sampling. If set, different calls will generate deterministic results."}}}'),C=JSON.parse('{"qo":{"temperature":{"type":"number","description":"Amount of randomness injected into the response. Ranges from 0 to 1. Use temp closer to 0 for analytical / multiple choice, and temp closer to 1 for creative and generative tasks."},"topK":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \\"long tail\\" low probability responses. Defaults to -1, which disables it."},"topP":{"type":"number","description":"Does nucleus sampling, in which we compute the cumulative distribution over all the options for each subsequent token in decreasing probability order and cut it off once it reaches a particular probability specified by top_p. Defaults to -1, which disables it. Note that you should either alter temperature or top_p, but not both."},"maxTokens":{"type":"number","description":"A maximum number of tokens to generate before stopping."},"maxTokensToSample":{"type":"number","description":"A maximum number of tokens to generate before stopping.","deprecated":"Use \\"maxTokens\\" instead."},"stopSequences":{"type":"array","items":{"type":"string"},"description":"A list of strings upon which to stop generating. You probably want `[\\"\\\\n\\\\nHuman:\\"]`, as that\'s the cue for the next turn in the dialog agent."},"streaming":{"type":"boolean","description":"Whether to stream the results or not"},"anthropicApiKey":{"type":"string","description":"Anthropic API key"},"apiKey":{"type":"string","description":"Anthropic API key"},"anthropicApiUrl":{"type":"string","description":"Anthropic API URL"},"modelName":{"type":"string","deprecated":"Use \\"model\\" instead"},"model":{"type":"string","description":"Model name to use"},"invocationKwargs":{"type":"object","description":"Holds any additional parameters that are valid to pass to  {@link  * https://console.anthropic.com/docs/api/reference | }      * `anthropic.messages`} that are not explicitly specified on this class."},"streamUsage":{"type":"boolean","description":"Whether or not to include token usage data in streamed chunks.","default":false}}}'),O=JSON.parse('{"qo":{"disableStreaming":{"type":"boolean","description":"Whether to disable streaming.\\n\\nIf streaming is bypassed, then `stream()` will defer to `invoke()`.\\n\\n- If true, will always bypass streaming case.\\n- If false (default), will always use streaming case if available."},"logprobs":{"type":"boolean","description":"Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message."},"topLogprobs":{"type":"number","description":"An integer between 0 and 5 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used."},"prefixMessages":{"type":"array","items":{"anyOf":[{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"text":{"type":"string","description":"The text content."},"type":{"type":"string","const":"text","description":"The type of the content part."}},"required":["text","type"],"additionalProperties":false,"description":"Learn about [text inputs](https://platform.openai.com/docs/guides/text-generation)."}}],"description":"The contents of the developer message."},"role":{"type":"string","const":"developer","description":"The role of the messages author, in this case `developer`."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false,"description":"Developer-provided instructions that the model should follow, regardless of messages sent by the user. With o1 models and newer, `developer` messages replace the previous `system` messages."},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"text":{"type":"string","description":"The text content."},"type":{"type":"string","const":"text","description":"The type of the content part."}},"required":["text","type"],"additionalProperties":false,"description":"Learn about [text inputs](https://platform.openai.com/docs/guides/text-generation)."}}],"description":"The contents of the system message."},"role":{"type":"string","const":"system","description":"The role of the messages author, in this case `system`."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false,"description":"Developer-provided instructions that the model should follow, regardless of messages sent by the user. With o1 models and newer, use `developer` messages for this purpose instead."},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"text":{"type":"string","description":"The text content."},"type":{"type":"string","const":"text","description":"The type of the content part."}},"required":["text","type"],"additionalProperties":false,"description":"Learn about [text inputs](https://platform.openai.com/docs/guides/text-generation)."},{"type":"object","properties":{"image_url":{"type":"object","properties":{"url":{"type":"string","description":"Either a URL of the image or the base64 encoded image data."},"detail":{"type":"string","enum":["auto","low","high"],"description":"Specifies the detail level of the image. Learn more in the [Vision guide](https://platform.openai.com/docs/guides/vision#low-or-high-fidelity-image-understanding)."}},"required":["url"],"additionalProperties":false},"type":{"type":"string","const":"image_url","description":"The type of the content part."}},"required":["image_url","type"],"additionalProperties":false,"description":"Learn about [image inputs](https://platform.openai.com/docs/guides/vision)."},{"type":"object","properties":{"input_audio":{"type":"object","properties":{"data":{"type":"string","description":"Base64 encoded audio data."},"format":{"type":"string","enum":["wav","mp3"],"description":"The format of the encoded audio data. Currently supports \\"wav\\" and \\"mp3\\"."}},"required":["data","format"],"additionalProperties":false},"type":{"type":"string","const":"input_audio","description":"The type of the content part. Always `input_audio`."}},"required":["input_audio","type"],"additionalProperties":false,"description":"Learn about [audio inputs](https://platform.openai.com/docs/guides/audio)."}],"description":"Learn about [text inputs](https://platform.openai.com/docs/guides/text-generation)."}}],"description":"The contents of the user message."},"role":{"type":"string","const":"user","description":"The role of the messages author, in this case `user`."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false,"description":"Messages sent by an end user, containing prompts or additional context information."},{"type":"object","properties":{"role":{"type":"string","const":"assistant","description":"The role of the messages author, in this case `assistant`."},"audio":{"anyOf":[{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for a previous audio response from the model."}},"required":["id"],"additionalProperties":false,"description":"Data about a previous audio response from the model. [Learn more](https://platform.openai.com/docs/guides/audio)."},{"type":"null"}],"description":"Data about a previous audio response from the model. [Learn more](https://platform.openai.com/docs/guides/audio)."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"text":{"type":"string","description":"The text content."},"type":{"type":"string","const":"text","description":"The type of the content part."}},"required":["text","type"],"additionalProperties":false,"description":"Learn about [text inputs](https://platform.openai.com/docs/guides/text-generation)."},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","const":"refusal","description":"The type of the content part."}},"required":["refusal","type"],"additionalProperties":false}]}},{"type":"null"}],"description":"The contents of the assistant message. Required unless `tool_calls` or `function_call` is specified."},"function_call":{"anyOf":[{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"deprecated":"Deprecated and replaced by `tool_calls`. The name and arguments of a\\nfunction that should be called, as generated by the model."},{"type":"null"}],"deprecated":"Deprecated and replaced by `tool_calls`. The name and arguments of a\\nfunction that should be called, as generated by the model."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."},"refusal":{"type":["string","null"],"description":"The refusal message by the assistant."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"function":{"type":"object","properties":{"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."},"name":{"type":"string","description":"The name of the function to call."}},"required":["arguments","name"],"additionalProperties":false,"description":"The function that the model called."},"type":{"type":"string","const":"function","description":"The type of the tool. Currently, only `function` is supported."}},"required":["id","function","type"],"additionalProperties":false},"description":"The tool calls generated by the model, such as function calls."}},"required":["role"],"additionalProperties":false,"description":"Messages sent by the model in response to user messages."},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"text":{"type":"string","description":"The text content."},"type":{"type":"string","const":"text","description":"The type of the content part."}},"required":["text","type"],"additionalProperties":false,"description":"Learn about [text inputs](https://platform.openai.com/docs/guides/text-generation)."}}],"description":"The contents of the tool message."},"role":{"type":"string","const":"tool","description":"The role of the messages author, in this case `tool`."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."}},"required":["content","role","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"content":{"type":["string","null"],"description":"The contents of the function message."},"name":{"type":"string","description":"The name of the function to call."},"role":{"type":"string","const":"function","description":"The role of the messages author, in this case `function`."}},"required":["content","name","role"],"additionalProperties":false,"deprecated":true}],"description":"Developer-provided instructions that the model should follow, regardless of messages sent by the user. With o1 models and newer, `developer` messages replace the previous `system` messages."},"description":"ChatGPT messages to pass as a prefix to the prompt"},"__includeRawResponse":{"type":"boolean","description":"Whether to include the raw OpenAI response in the output message\'s \\"additional_kwargs\\" field. Currently in experimental beta."},"supportsStrictToolCalling":{"type":"boolean","description":"Whether the model supports the `strict` argument when passing in tools. If `undefined` the `strict` argument will not be passed to OpenAI."},"modalities":{"type":"array","items":{"type":"string","enum":["text","audio"]},"description":"Output types that you would like the model to generate for this request. Most models are capable of generating text, which is the default:\\n\\n`[\\"text\\"]`\\n\\nThe `gpt-4o-audio-preview` model can also be used to [generate audio](https://platform.openai.com/docs/guides/audio). To request that this model generate both text and audio responses, you can use:\\n\\n`[\\"text\\", \\"audio\\"]`"},"audio":{"type":"object","properties":{"format":{"type":"string","enum":["wav","mp3","flac","opus","pcm16"],"description":"Specifies the output audio format. Must be one of `wav`, `mp3`, `flac`, `opus`, or `pcm16`."},"voice":{"type":"string","enum":["alloy","ash","ballad","coral","echo","sage","shimmer","verse"],"description":"The voice the model uses to respond. Supported voices are `ash`, `ballad`, `coral`, `sage`, and `verse` (also supported but not recommended are `alloy`, `echo`, and `shimmer`; these voices are less expressive)."}},"required":["format","voice"],"additionalProperties":false,"description":"Parameters for audio output. Required when audio output is requested with `modalities: [\\"audio\\"]`. [Learn more](https://platform.openai.com/docs/guides/audio)."},"reasoningEffort":{"type":"string","enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models. Currently supported values are low, medium, and high. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."},"temperature":{"type":"number","description":"Sampling temperature to use"},"maxTokens":{"type":"number","description":"Maximum number of tokens to generate in the completion. -1 returns as many tokens as possible given the prompt and the model\'s maximum context size."},"maxCompletionTokens":{"type":"number","description":"Maximum number of tokens to generate in the completion. -1 returns as many tokens as possible given the prompt and the model\'s maximum context size. Alias for `maxTokens` for reasoning models."},"topP":{"type":"number","description":"Total probability mass of tokens to consider at each step"},"frequencyPenalty":{"type":"number","description":"Penalizes repeated tokens according to frequency"},"presencePenalty":{"type":"number","description":"Penalizes repeated tokens"},"n":{"type":"number","description":"Number of completions to generate for each prompt"},"logitBias":{"type":"object","additionalProperties":{"type":"number"},"description":"Dictionary used to adjust the probability of specific tokens being generated"},"user":{"type":"string","description":"Unique string identifier representing your end-user, which can help OpenAI to monitor and detect abuse."},"streaming":{"type":"boolean","description":"Whether to stream the results or not. Enabling disables tokenUsage reporting"},"streamUsage":{"type":"boolean","description":"Whether or not to include token usage data in streamed chunks.","default":false},"modelName":{"type":"string","description":"Model name to use Alias for `model`","deprecated":"Use \\"model\\" instead."},"model":{"type":"string","description":"Model name to use"},"modelKwargs":{"type":"object","description":"Holds any additional parameters that are valid to pass to  {@link  * https://platform.openai.com/docs/api-reference/completions/create | }      * `openai.createCompletion`} that are not explicitly specified on this class."},"stop":{"type":"array","items":{"type":"string"},"description":"List of stop words to use when generating Alias for `stopSequences`"},"stopSequences":{"type":"array","items":{"type":"string"},"description":"List of stop words to use when generating"},"timeout":{"type":"number","description":"Timeout to use when making requests to OpenAI."},"openAIApiKey":{"type":"string","description":"API key to use when making requests to OpenAI. Defaults to the value of `OPENAI_API_KEY` environment variable. Alias for `apiKey`"},"apiKey":{"type":"string","description":"API key to use when making requests to OpenAI. Defaults to the value of `OPENAI_API_KEY` environment variable."}}}'),M=new(o(1655).LabIcon)({name:"@jupyterlite/ai:jupyternaut-lite",svgstr:'<?xml version="1.0" encoding="UTF-8"?>\n<svg fill="none" version="1.1" viewBox="0 0 38 38" xmlns="http://www.w3.org/2000/svg">\n <g>\n  <circle cx="19" cy="19" r="19" fill="#f7dc1e"/>\n  <path d="m19.948 6.8365c0.6344-0.21603 1.0908-0.81693 1.0908-1.5244 0-0.88916-0.7208-1.61-1.61-1.61s-1.61 0.72081-1.61 1.61c0 0.70739 0.4562 1.3082 1.0905 1.5243v1.1674h-0.5394c-6.2858 0-11.399 5.1132-11.399 11.399 0 6.2859 5.1133 11.399 11.399 11.399h1.398c6.2859 0 11.399-5.1132 11.399-11.399 0-6.2253-5.0159-11.301-11.219-11.397v-1.1687zm-1.5781 22.163h1.398c5.3669 0 9.7303-4.9787 9.7303-8.3455 0-5.3669-4.3634-9.7303-9.7303-9.7303h-1.398c-5.3668 0-9.7302 4.3634-9.7302 9.7303 0 3.3668 4.3634 8.3455 9.7302 8.3455zm14.522-5.003c0.7341 0 0.7375-0.5123 0.7422-1.2408v-1e-4c0.0012-0.1847 0.0025-0.3833 0.0158-0.591 0.0573-0.8624 0.0739-1.7008 0.0573-2.4892l-0.0033-0.119c-0.0373-1.3318-0.0436-1.5579-1.3047-1.8058l-0.1479-0.0246 0.0015 0.0495c0.0076 0.2465 0.0151 0.493 0.0151 0.739 0 1.8018-0.2712 3.5478-0.773 5.189-0.1363 0.4459 0.3433 0.8629 0.7534 0.6411l0.6436-0.3481zm-27.612 0c-0.73409 0-0.73741-0.5123-0.74215-1.2407-0.0012-0.1848-0.00249-0.3834-0.01581-0.5912-0.0573-0.8624-0.07392-1.7007-0.05731-2.4892l0.00336-0.1189c0.03721-1.3319 0.04352-1.5579 1.3047-1.8058l0.14784-0.0247-0.00151 0.0495c-0.00756 0.2465-0.01511 0.493-0.01511 0.739 0 1.8019 0.27119 3.5478 0.773 5.189 0.13634 0.4459-0.34326 0.8629-0.7534 0.6411l-0.64361-0.3481zm14.242-12.005c-4.6297 0-8.4776 2.9361-9.4584 6.7577-0.13379 0.5212 0.5513 0.6935 0.877 0.2653 1.3829-1.818 3.1418-2.153 4.7068-2.4511 2.1143-0.4027 3.8746-0.7379 3.8746-4.5719z" clip-rule="evenodd" fill="#000" fill-rule="evenodd"/>\n </g>\n</svg>\n'}),j=`data:image/svg+xml;base64,${btoa(M.svgstr)}`;class E extends r.ChatModel{constructor(e){super(e),this._personaName="AI",this._errorMessage="",this._history={messages:[]},this._defaultErrorMessage="AI provider not configured",this._aiProvider=e.aiProvider,this._prompt=f({provider_name:this._aiProvider.name}),this._aiProvider.modelChange.connect((()=>{this._errorMessage=this._aiProvider.chatError,this._prompt=f({provider_name:this._aiProvider.name})}))}get provider(){return this._aiProvider.chatModel}get personaName(){return this._personaName}set personaName(e){this.messages.forEach((t=>{if(t.sender.username===this._personaName){const o={...t};o.sender.username=e,this.messageAdded(o)}})),this._personaName=e}get prompt(){return this._prompt}set prompt(e){this._prompt=e}async sendMessage(e){var t;const o=e.body;if(o.startsWith("/clear"))return this.messagesDeleted(0,this.messages.length),this._history.messages=[],!1;e.id=d.UUID.uuid4();const r={id:e.id,body:o,sender:{username:"User"},time:Date.now(),type:"msg"};if(this.messageAdded(r),null===this._aiProvider.chatModel){const e={id:d.UUID.uuid4(),body:`**${this._errorMessage?this._errorMessage:this._defaultErrorMessage}**`,sender:{username:"ERROR"},time:Date.now(),type:"msg"};return this.messageAdded(e),!1}this._history.messages.push(r);const n=(0,l.eu)([new l.tn(this._prompt)]);n.push(...this._history.messages.map((e=>"User"===e.sender.username?new l.xc(e.body):new l.Od(e.body))));const i={username:this._personaName,avatar_url:j};this.updateWriters([i]);const s={id:d.UUID.uuid4(),body:"",sender:i,time:Date.now(),type:"msg"};let a="";try{for await(const e of await this._aiProvider.chatModel.stream(n))a+=null!==(t=e.content)&&void 0!==t?t:e,s.body=a,this.messageAdded(s);return this._history.messages.push(s),!0}catch(e){const t=function(e,t){return"MistralAI"===e?t.message:"Anthropic"===e?t.error.error.message:"ChromeAI"===e||"OpenAI"===e?t.message:"Unknown provider"}(this._aiProvider.name,e),o={id:d.UUID.uuid4(),body:`**${t}**`,sender:{username:"ERROR"},time:Date.now(),type:"msg"};return this.messageAdded(o),!1}finally{this.updateWriters([])}}async getHistory(){return this._history}dispose(){super.dispose()}messageAdded(e){super.messageAdded(e)}}var S=o(7346),N=o(5320),L=o(3345),U=o.n(L);const D={clear:U().createElement(N.A,null)};function R(e,t){const o=t.id in D?D[t.id]:D.unknown;return U().createElement("li",{...e},U().createElement(S.Box,{sx:{lineHeight:0,marginRight:4,opacity:.618}},o),U().createElement(S.Box,{sx:{flexGrow:1}},U().createElement(S.Typography,{component:"span",sx:{fontSize:"var(--jp-ui-font-size1)"}},t.label),U().createElement(S.Typography,{component:"span",sx:{opacity:.618,fontSize:"var(--jp-ui-font-size0)"}}," â€” "+t.description)))}const W=new d.Token("@jupyterlite/ai:AIProvider","Provider for chat and completion LLM provider"),z={id:"@jupyterlite/ai:autocompletion-registry",description:"Autocompletion registry",autoStart:!0,provides:r.IAutocompletionRegistry,activate:()=>{const e=new r.AutocompletionRegistry,t={opener:"/",commands:["/clear"].map((e=>({id:e.slice(1),label:e,description:"Clear the chat window"}))),props:{renderOption:R}};return e.add("jupyterlite-ai",t),e}},K={id:"@jupyterlite/ai:chat",description:"LLM chat extension",autoStart:!0,requires:[W,a.IRenderMimeRegistry,r.IAutocompletionRegistry],optional:[s.INotebookTracker,p.ISettingRegistry,n.IThemeManager],activate:async(e,t,o,n,i,s,a)=>{let p=null;i&&(p=new r.ActiveCellManager({tracker:i,shell:e.shell}));const l=new E({aiProvider:t,activeCellManager:p});let d=!1,c=!0,m="AI";function u(e){d=e.get("sendWithShiftEnter").composite,c=e.get("enableCodeToolbar").composite,m=e.get("personaName").composite,l.config={sendWithShiftEnter:d,enableCodeToolbar:c},l.personaName=m}Promise.all([e.restored,null==s?void 0:s.load(K.id)]).then((([,e])=>{e?(u(e),e.changed.connect(u)):console.warn("The SettingsRegistry is not loaded for the chat extension")})).catch((e=>{console.error(`Something went wrong when reading the settings.\n${e}`)}));let h=null;try{h=(0,r.buildChatSidebar)({model:l,themeManager:a,rmRegistry:o,autocompletionRegistry:n}),h.title.caption="Jupyterlite AI Chat"}catch(e){h=(0,r.buildErrorWidget)(a)}e.shell.add(h,"left",{rank:2e3}),console.log("Chat extension initialized")}},J={id:"@jupyterlite/ai:ai-provider",autoStart:!0,requires:[i.ICompletionProviderManager,p.ISettingRegistry],provides:W,activate:(e,t,o)=>{const r=new v({completionProviderManager:t,requestCompletion:()=>e.commands.execute("inline-completer:invoke")});let n="None";return o.load(J.id).then((e=>{const t=()=>{const t=e.get("provider").composite;if(t!==n){n=t;const r=e.schema.properties;if(r){Object.keys(r).forEach((t=>{var o;"provider"!==t&&(null===(o=e.schema.properties)||void 0===o||delete o[t])}));const n="MistralAI"===(o=t)?q.qo:"Anthropic"===o?C.qo:"ChromeAI"===o?I.qo:"OpenAI"===o?O.qo:null;if(null===n)return;Object.entries(n).forEach((([e,t],o)=>{r[e]=t}))}}var o;r.setModels(t,e.composite)};e.changed.connect((()=>t())),t()})).catch((e=>{console.error(`Failed to load settings for ${J.id}`,e)})),r}},$=[K,z,J]}}]);